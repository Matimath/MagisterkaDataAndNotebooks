{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "NormalGraphConv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N9AMK44776g",
        "colab_type": "text"
      },
      "source": [
        "This code is based on: https://github.com/mdeff/cnn_graph and https://nbviewer.jupyter.org/github/mdeff/cnn_graph/blob/outputs/usage.ipynb\n",
        "\n",
        "Reproducing license notice from https://github.com/mdeff/cnn_graph\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2016 Michaël Defferrard\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsbDri31GYjw",
        "colab_type": "code",
        "outputId": "e2e33733-20da-44df-cf55-9722e87c8fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "source": [
        "!pip install tensorflow-gpu==\"1.14\"\n",
        "!pip install numpy matplotlib\n",
        "!pip install keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 46kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.9.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.34.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.27.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.17.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.1.8)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (45.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj6MuhtUOo7b",
        "colab_type": "code",
        "outputId": "7f6dbcc6-51a3-4e30-cd57-b41799399e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "# Download repo\n",
        "!git clone https://github.com/Matimath/cnn_graph\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cnn_graph'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 610 (delta 15), reused 20 (delta 7), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (610/610), 440.58 KiB | 1.42 MiB/s, done.\n",
            "Resolving deltas: 100% (386/386), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g1QoeLarLNY",
        "colab_type": "code",
        "outputId": "d10dfa66-16d8-42d1-8af5-ba9c51f6c3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "# Installing tensorflow and checking version\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh61QD1KOeEo",
        "colab_type": "code",
        "outputId": "acfb3714-251c-4687-cef5-78e610b5f544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# We need tensorflow 1.14 so we change. We also import necessary modules from the github repo we cloned\n",
        "%tensorflow_version 1.14\n",
        "from cnn_graph.lib import models, graph, coarsening, utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "hello world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaDZGhtqOeEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "1935e535-d944-4fd0-98b1-96d8b6ff954d"
      },
      "source": [
        "# Loading dataset\n",
        "import keras\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "    8192/26421880 [..............................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wH51pOPOeEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create underlying graph as 8-NN grid of size 28x28\n",
        "z = graph.grid(28)\n",
        "dist, idx = graph.distance_sklearn_metrics(z, k=8, metric='euclidean')\n",
        "A = graph.adjacency(dist, idx).astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHIWzxwkOeE5",
        "colab_type": "code",
        "outputId": "747bd532-de75-46aa-fee5-74db21d368fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# We prepare coarsened versions of graphs. We also permute data so that we can use 1-D pooling as coarsening pooling as shown by defferard et. al.\n",
        "graphs, perm = coarsening.coarsen(A, levels=3, self_connections=False)\n",
        "\n",
        "X_train = coarsening.perm_data(train_images.reshape((60000, 28*28)), perm)\n",
        "#X_val = coarsening.perm_data(X_val, perm)\n",
        "X_test = coarsening.perm_data(test_images.reshape((10000, 28*28)), perm)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer 0: M_0 = |V| = 912 nodes (128 added),|E| = 3198 edges\n",
            "Layer 1: M_1 = |V| = 456 nodes (48 added),|E| = 1445 edges\n",
            "Layer 2: M_2 = |V| = 228 nodes (14 added),|E| = 691 edges\n",
            "Layer 3: M_3 = |V| = 114 nodes (0 added),|E| = 326 edges\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoChMjA5OeE7",
        "colab_type": "code",
        "outputId": "3b2cfbfd-a0ae-4783-cef7-772176e4f587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "# Computing laplacians for initial graph and coarsened graphs\n",
        "L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
        "graph.plot_spectrum(L)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9oAAAEvCAYAAAC617nEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJaSgLSIiIiIiIlJC/z98bZrN+pcYDwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1224x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPZgZ6yCOeE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = dict()\n",
        "params['dir_name']       = 'demo'\n",
        "params['num_epochs']     = 120\n",
        "params['batch_size']     = 100\n",
        "params['eval_frequency'] = 200\n",
        "\n",
        "# Building blocks.\n",
        "params['filter']         = 'chebyshev5'\n",
        "params['brelu']          = 'b1relu'\n",
        "params['pool']           = 'apool1'\n",
        "\n",
        "# Number of classes.\n",
        "C = train_labels.max() + 1\n",
        "#assert C == np.unique(y).size\n",
        "n_train = 60000\n",
        "# Architecture.\n",
        "params['F']              = [32,32]  # Number of graph convolutional filters.\n",
        "params['K']              = [3,3]  # Polynomial orders.\n",
        "params['p']              = [4, 2]    # Pooling sizes.\n",
        "params['M']              = [512, C]  # Output dimensionality of fully connected layers.\n",
        "\n",
        "# Optimization.\n",
        "params['regularization'] = 0 #5e-5\n",
        "params['dropout']        = 1\n",
        "params['learning_rate']  = 1e-3\n",
        "params['decay_rate']     = 0.95\n",
        "params['momentum']       = 0.9\n",
        "params['decay_steps']    = n_train / params['batch_size']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQvFFEqwOeE_",
        "colab_type": "code",
        "outputId": "143d252b-d5e0-4364-8f88-e1b0228452df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "model = models.cgcnn_original(L, **params)\n",
        "accuracy, loss, train_accuracy, train_loss, t_step = model.fit(X_train, train_labels, X_test, test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN architecture\n",
            "  input: M_0 = 912\n",
            "  layer 1: cgconv1\n",
            "    representation: M_0 * F_1 / p_1 = 912 * 32 / 4 = 7296\n",
            "    weights: F_0 * F_1 * K_1 = 1 * 32 * 3 = 96\n",
            "    biases: F_1 = 32\n",
            "  layer 2: cgconv2\n",
            "    representation: M_1 * F_2 / p_2 = 228 * 32 / 2 = 3648\n",
            "    weights: F_1 * F_2 * K_2 = 32 * 32 * 3 = 3072\n",
            "    biases: F_2 = 32\n",
            "  layer 3: fc1\n",
            "    representation: M_3 = 512\n",
            "    weights: M_2 * M_3 = 3648 * 512 = 1867776\n",
            "    biases: M_3 = 512\n",
            "  layer 4: logits (softmax)\n",
            "    representation: M_4 = 10\n",
            "    weights: M_3 * M_4 = 512 * 10 = 5120\n",
            "    biases: M_4 = 10\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:161: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:833: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:767: The name tf.sparse_reorder is deprecated. Please use tf.sparse.reorder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:776: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:279: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:282: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:816: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:847: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:214: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:222: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:241: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:249: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:176: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:177: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:87: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "step 200 / 72000 (epoch 0.33 / 120):\n",
            "  learning_rate = 1.00e-03, loss_average = 1.33e+00\n",
            "  validation accuracy: 54.94 (5494 / 10000), f1 (weighted): 50.84, loss: 1.31e+00\n",
            "  time: 7s (wall 12s)\n",
            "step 400 / 72000 (epoch 0.67 / 120):\n",
            "  learning_rate = 1.00e-03, loss_average = 8.45e-01\n",
            "  validation accuracy: 68.55 (6855 / 10000), f1 (weighted): 67.97, loss: 8.53e-01\n",
            "  time: 11s (wall 16s)\n",
            "step 600 / 72000 (epoch 1.00 / 120):\n",
            "  learning_rate = 1.00e-03, loss_average = 6.78e-01\n",
            "  validation accuracy: 74.14 (7414 / 10000), f1 (weighted): 73.74, loss: 6.93e-01\n",
            "  time: 15s (wall 19s)\n",
            "step 800 / 72000 (epoch 1.33 / 120):\n",
            "  learning_rate = 9.50e-04, loss_average = 5.81e-01\n",
            "  validation accuracy: 77.25 (7725 / 10000), f1 (weighted): 77.16, loss: 6.31e-01\n",
            "  time: 19s (wall 23s)\n",
            "step 1000 / 72000 (epoch 1.67 / 120):\n",
            "  learning_rate = 9.50e-04, loss_average = 5.46e-01\n",
            "  validation accuracy: 79.03 (7903 / 10000), f1 (weighted): 79.18, loss: 5.76e-01\n",
            "  time: 23s (wall 26s)\n",
            "step 1200 / 72000 (epoch 2.00 / 120):\n",
            "  learning_rate = 9.50e-04, loss_average = 5.70e-01\n",
            "  validation accuracy: 80.33 (8033 / 10000), f1 (weighted): 80.07, loss: 5.41e-01\n",
            "  time: 27s (wall 30s)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "step 1400 / 72000 (epoch 2.33 / 120):\n",
            "  learning_rate = 9.02e-04, loss_average = 4.95e-01\n",
            "  validation accuracy: 80.31 (8031 / 10000), f1 (weighted): 80.30, loss: 5.34e-01\n",
            "  time: 32s (wall 34s)\n",
            "step 1600 / 72000 (epoch 2.67 / 120):\n",
            "  learning_rate = 9.02e-04, loss_average = 5.04e-01\n",
            "  validation accuracy: 81.07 (8107 / 10000), f1 (weighted): 80.97, loss: 5.21e-01\n",
            "  time: 36s (wall 37s)\n",
            "step 1800 / 72000 (epoch 3.00 / 120):\n",
            "  learning_rate = 9.02e-04, loss_average = 4.42e-01\n",
            "  validation accuracy: 81.94 (8194 / 10000), f1 (weighted): 81.58, loss: 4.96e-01\n",
            "  time: 40s (wall 41s)\n",
            "step 2000 / 72000 (epoch 3.33 / 120):\n",
            "  learning_rate = 8.57e-04, loss_average = 4.55e-01\n",
            "  validation accuracy: 82.51 (8251 / 10000), f1 (weighted): 82.36, loss: 4.81e-01\n",
            "  time: 44s (wall 45s)\n",
            "step 2200 / 72000 (epoch 3.67 / 120):\n",
            "  learning_rate = 8.57e-04, loss_average = 4.90e-01\n",
            "  validation accuracy: 82.17 (8217 / 10000), f1 (weighted): 82.03, loss: 4.81e-01\n",
            "  time: 48s (wall 48s)\n",
            "step 2400 / 72000 (epoch 4.00 / 120):\n",
            "  learning_rate = 8.57e-04, loss_average = 4.84e-01\n",
            "  validation accuracy: 82.59 (8259 / 10000), f1 (weighted): 82.57, loss: 4.67e-01\n",
            "  time: 52s (wall 52s)\n",
            "step 2600 / 72000 (epoch 4.33 / 120):\n",
            "  learning_rate = 8.15e-04, loss_average = 4.96e-01\n",
            "  validation accuracy: 83.28 (8328 / 10000), f1 (weighted): 83.11, loss: 4.59e-01\n",
            "  time: 56s (wall 55s)\n",
            "step 2800 / 72000 (epoch 4.67 / 120):\n",
            "  learning_rate = 8.15e-04, loss_average = 3.85e-01\n",
            "  validation accuracy: 83.69 (8369 / 10000), f1 (weighted): 83.32, loss: 4.52e-01\n",
            "  time: 60s (wall 59s)\n",
            "step 3000 / 72000 (epoch 5.00 / 120):\n",
            "  learning_rate = 8.15e-04, loss_average = 3.85e-01\n",
            "  validation accuracy: 83.71 (8371 / 10000), f1 (weighted): 83.78, loss: 4.53e-01\n",
            "  time: 64s (wall 63s)\n",
            "step 3200 / 72000 (epoch 5.33 / 120):\n",
            "  learning_rate = 7.74e-04, loss_average = 3.95e-01\n",
            "  validation accuracy: 83.38 (8338 / 10000), f1 (weighted): 83.26, loss: 4.51e-01\n",
            "  time: 68s (wall 66s)\n",
            "step 3400 / 72000 (epoch 5.67 / 120):\n",
            "  learning_rate = 7.74e-04, loss_average = 4.46e-01\n",
            "  validation accuracy: 83.87 (8387 / 10000), f1 (weighted): 83.90, loss: 4.45e-01\n",
            "  time: 72s (wall 70s)\n",
            "step 3600 / 72000 (epoch 6.00 / 120):\n",
            "  learning_rate = 7.74e-04, loss_average = 3.98e-01\n",
            "  validation accuracy: 83.72 (8372 / 10000), f1 (weighted): 83.66, loss: 4.40e-01\n",
            "  time: 76s (wall 73s)\n",
            "step 3800 / 72000 (epoch 6.33 / 120):\n",
            "  learning_rate = 7.35e-04, loss_average = 4.11e-01\n",
            "  validation accuracy: 84.11 (8411 / 10000), f1 (weighted): 83.91, loss: 4.41e-01\n",
            "  time: 80s (wall 77s)\n",
            "step 4000 / 72000 (epoch 6.67 / 120):\n",
            "  learning_rate = 7.35e-04, loss_average = 4.09e-01\n",
            "  validation accuracy: 83.85 (8385 / 10000), f1 (weighted): 83.42, loss: 4.43e-01\n",
            "  time: 84s (wall 80s)\n",
            "step 4200 / 72000 (epoch 7.00 / 120):\n",
            "  learning_rate = 7.35e-04, loss_average = 4.26e-01\n",
            "  validation accuracy: 84.59 (8459 / 10000), f1 (weighted): 84.55, loss: 4.21e-01\n",
            "  time: 88s (wall 84s)\n",
            "step 4400 / 72000 (epoch 7.33 / 120):\n",
            "  learning_rate = 6.98e-04, loss_average = 3.83e-01\n",
            "  validation accuracy: 84.57 (8457 / 10000), f1 (weighted): 84.31, loss: 4.23e-01\n",
            "  time: 92s (wall 88s)\n",
            "step 4600 / 72000 (epoch 7.67 / 120):\n",
            "  learning_rate = 6.98e-04, loss_average = 4.10e-01\n",
            "  validation accuracy: 84.91 (8491 / 10000), f1 (weighted): 84.57, loss: 4.14e-01\n",
            "  time: 96s (wall 91s)\n",
            "step 4800 / 72000 (epoch 8.00 / 120):\n",
            "  learning_rate = 6.98e-04, loss_average = 3.69e-01\n",
            "  validation accuracy: 84.89 (8489 / 10000), f1 (weighted): 84.84, loss: 4.17e-01\n",
            "  time: 100s (wall 95s)\n",
            "step 5000 / 72000 (epoch 8.33 / 120):\n",
            "  learning_rate = 6.63e-04, loss_average = 4.05e-01\n",
            "  validation accuracy: 84.43 (8443 / 10000), f1 (weighted): 84.24, loss: 4.22e-01\n",
            "  time: 104s (wall 98s)\n",
            "step 5200 / 72000 (epoch 8.67 / 120):\n",
            "  learning_rate = 6.63e-04, loss_average = 3.57e-01\n",
            "  validation accuracy: 84.60 (8460 / 10000), f1 (weighted): 84.10, loss: 4.22e-01\n",
            "  time: 108s (wall 102s)\n",
            "step 5400 / 72000 (epoch 9.00 / 120):\n",
            "  learning_rate = 6.63e-04, loss_average = 3.55e-01\n",
            "  validation accuracy: 84.77 (8477 / 10000), f1 (weighted): 84.61, loss: 4.16e-01\n",
            "  time: 112s (wall 106s)\n",
            "step 5600 / 72000 (epoch 9.33 / 120):\n",
            "  learning_rate = 6.30e-04, loss_average = 3.84e-01\n",
            "  validation accuracy: 85.11 (8511 / 10000), f1 (weighted): 85.01, loss: 4.09e-01\n",
            "  time: 116s (wall 109s)\n",
            "step 5800 / 72000 (epoch 9.67 / 120):\n",
            "  learning_rate = 6.30e-04, loss_average = 4.06e-01\n",
            "  validation accuracy: 84.77 (8477 / 10000), f1 (weighted): 84.70, loss: 4.17e-01\n",
            "  time: 120s (wall 113s)\n",
            "step 6000 / 72000 (epoch 10.00 / 120):\n",
            "  learning_rate = 6.30e-04, loss_average = 3.41e-01\n",
            "  validation accuracy: 85.43 (8543 / 10000), f1 (weighted): 85.29, loss: 4.00e-01\n",
            "  time: 125s (wall 117s)\n",
            "step 6200 / 72000 (epoch 10.33 / 120):\n",
            "  learning_rate = 5.99e-04, loss_average = 3.91e-01\n",
            "  validation accuracy: 85.19 (8519 / 10000), f1 (weighted): 85.08, loss: 4.06e-01\n",
            "  time: 129s (wall 120s)\n",
            "step 6400 / 72000 (epoch 10.67 / 120):\n",
            "  learning_rate = 5.99e-04, loss_average = 3.47e-01\n",
            "  validation accuracy: 85.64 (8564 / 10000), f1 (weighted): 85.45, loss: 3.97e-01\n",
            "  time: 133s (wall 124s)\n",
            "step 6600 / 72000 (epoch 11.00 / 120):\n",
            "  learning_rate = 5.99e-04, loss_average = 3.19e-01\n",
            "  validation accuracy: 85.33 (8533 / 10000), f1 (weighted): 85.49, loss: 4.02e-01\n",
            "  time: 137s (wall 127s)\n",
            "step 6800 / 72000 (epoch 11.33 / 120):\n",
            "  learning_rate = 5.69e-04, loss_average = 3.50e-01\n",
            "  validation accuracy: 85.27 (8527 / 10000), f1 (weighted): 85.32, loss: 3.99e-01\n",
            "  time: 141s (wall 131s)\n",
            "step 7000 / 72000 (epoch 11.67 / 120):\n",
            "  learning_rate = 5.69e-04, loss_average = 3.52e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.54, loss: 3.95e-01\n",
            "  time: 145s (wall 135s)\n",
            "step 7200 / 72000 (epoch 12.00 / 120):\n",
            "  learning_rate = 5.69e-04, loss_average = 3.55e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.70, loss: 3.90e-01\n",
            "  time: 149s (wall 138s)\n",
            "step 7400 / 72000 (epoch 12.33 / 120):\n",
            "  learning_rate = 5.40e-04, loss_average = 3.55e-01\n",
            "  validation accuracy: 85.41 (8541 / 10000), f1 (weighted): 85.56, loss: 3.94e-01\n",
            "  time: 153s (wall 142s)\n",
            "step 7600 / 72000 (epoch 12.67 / 120):\n",
            "  learning_rate = 5.40e-04, loss_average = 3.40e-01\n",
            "  validation accuracy: 85.59 (8559 / 10000), f1 (weighted): 85.57, loss: 3.89e-01\n",
            "  time: 157s (wall 145s)\n",
            "step 7800 / 72000 (epoch 13.00 / 120):\n",
            "  learning_rate = 5.40e-04, loss_average = 3.73e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.74, loss: 3.88e-01\n",
            "  time: 161s (wall 149s)\n",
            "step 8000 / 72000 (epoch 13.33 / 120):\n",
            "  learning_rate = 5.13e-04, loss_average = 3.11e-01\n",
            "  validation accuracy: 85.85 (8585 / 10000), f1 (weighted): 85.73, loss: 3.93e-01\n",
            "  time: 165s (wall 153s)\n",
            "step 8200 / 72000 (epoch 13.67 / 120):\n",
            "  learning_rate = 5.13e-04, loss_average = 3.39e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.86, loss: 3.85e-01\n",
            "  time: 169s (wall 156s)\n",
            "step 8400 / 72000 (epoch 14.00 / 120):\n",
            "  learning_rate = 5.13e-04, loss_average = 3.43e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.79, loss: 3.84e-01\n",
            "  time: 173s (wall 160s)\n",
            "step 8600 / 72000 (epoch 14.33 / 120):\n",
            "  learning_rate = 4.88e-04, loss_average = 3.79e-01\n",
            "  validation accuracy: 85.58 (8558 / 10000), f1 (weighted): 85.51, loss: 3.85e-01\n",
            "  time: 177s (wall 163s)\n",
            "step 8800 / 72000 (epoch 14.67 / 120):\n",
            "  learning_rate = 4.88e-04, loss_average = 3.64e-01\n",
            "  validation accuracy: 85.62 (8562 / 10000), f1 (weighted): 85.52, loss: 3.89e-01\n",
            "  time: 181s (wall 167s)\n",
            "step 9000 / 72000 (epoch 15.00 / 120):\n",
            "  learning_rate = 4.88e-04, loss_average = 3.39e-01\n",
            "  validation accuracy: 86.07 (8607 / 10000), f1 (weighted): 86.05, loss: 3.81e-01\n",
            "  time: 185s (wall 171s)\n",
            "step 9200 / 72000 (epoch 15.33 / 120):\n",
            "  learning_rate = 4.63e-04, loss_average = 3.26e-01\n",
            "  validation accuracy: 86.01 (8601 / 10000), f1 (weighted): 85.96, loss: 3.83e-01\n",
            "  time: 189s (wall 174s)\n",
            "step 9400 / 72000 (epoch 15.67 / 120):\n",
            "  learning_rate = 4.63e-04, loss_average = 3.65e-01\n",
            "  validation accuracy: 85.58 (8558 / 10000), f1 (weighted): 85.47, loss: 3.90e-01\n",
            "  time: 193s (wall 178s)\n",
            "step 9600 / 72000 (epoch 16.00 / 120):\n",
            "  learning_rate = 4.63e-04, loss_average = 3.48e-01\n",
            "  validation accuracy: 86.24 (8624 / 10000), f1 (weighted): 86.19, loss: 3.76e-01\n",
            "  time: 198s (wall 182s)\n",
            "step 9800 / 72000 (epoch 16.33 / 120):\n",
            "  learning_rate = 4.40e-04, loss_average = 3.16e-01\n",
            "  validation accuracy: 85.88 (8588 / 10000), f1 (weighted): 85.63, loss: 3.83e-01\n",
            "  time: 202s (wall 185s)\n",
            "step 10000 / 72000 (epoch 16.67 / 120):\n",
            "  learning_rate = 4.40e-04, loss_average = 3.36e-01\n",
            "  validation accuracy: 86.38 (8638 / 10000), f1 (weighted): 86.35, loss: 3.79e-01\n",
            "  time: 206s (wall 189s)\n",
            "step 10200 / 72000 (epoch 17.00 / 120):\n",
            "  learning_rate = 4.40e-04, loss_average = 3.29e-01\n",
            "  validation accuracy: 85.93 (8593 / 10000), f1 (weighted): 85.91, loss: 3.85e-01\n",
            "  time: 210s (wall 192s)\n",
            "step 10400 / 72000 (epoch 17.33 / 120):\n",
            "  learning_rate = 4.18e-04, loss_average = 3.24e-01\n",
            "  validation accuracy: 86.38 (8638 / 10000), f1 (weighted): 86.27, loss: 3.76e-01\n",
            "  time: 214s (wall 196s)\n",
            "step 10600 / 72000 (epoch 17.67 / 120):\n",
            "  learning_rate = 4.18e-04, loss_average = 3.20e-01\n",
            "  validation accuracy: 86.24 (8624 / 10000), f1 (weighted): 86.15, loss: 3.79e-01\n",
            "  time: 218s (wall 200s)\n",
            "step 10800 / 72000 (epoch 18.00 / 120):\n",
            "  learning_rate = 4.18e-04, loss_average = 3.27e-01\n",
            "  validation accuracy: 85.92 (8592 / 10000), f1 (weighted): 85.88, loss: 3.78e-01\n",
            "  time: 222s (wall 203s)\n",
            "step 11000 / 72000 (epoch 18.33 / 120):\n",
            "  learning_rate = 3.97e-04, loss_average = 3.46e-01\n",
            "  validation accuracy: 86.00 (8600 / 10000), f1 (weighted): 85.81, loss: 3.79e-01\n",
            "  time: 226s (wall 207s)\n",
            "step 11200 / 72000 (epoch 18.67 / 120):\n",
            "  learning_rate = 3.97e-04, loss_average = 3.14e-01\n",
            "  validation accuracy: 85.96 (8596 / 10000), f1 (weighted): 85.74, loss: 3.79e-01\n",
            "  time: 230s (wall 210s)\n",
            "step 11400 / 72000 (epoch 19.00 / 120):\n",
            "  learning_rate = 3.97e-04, loss_average = 3.41e-01\n",
            "  validation accuracy: 86.20 (8620 / 10000), f1 (weighted): 86.18, loss: 3.75e-01\n",
            "  time: 234s (wall 214s)\n",
            "step 11600 / 72000 (epoch 19.33 / 120):\n",
            "  learning_rate = 3.77e-04, loss_average = 2.94e-01\n",
            "  validation accuracy: 86.10 (8610 / 10000), f1 (weighted): 85.90, loss: 3.77e-01\n",
            "  time: 238s (wall 218s)\n",
            "step 11800 / 72000 (epoch 19.67 / 120):\n",
            "  learning_rate = 3.77e-04, loss_average = 3.18e-01\n",
            "  validation accuracy: 86.30 (8630 / 10000), f1 (weighted): 86.23, loss: 3.75e-01\n",
            "  time: 242s (wall 221s)\n",
            "step 12000 / 72000 (epoch 20.00 / 120):\n",
            "  learning_rate = 3.77e-04, loss_average = 3.36e-01\n",
            "  validation accuracy: 86.46 (8646 / 10000), f1 (weighted): 86.36, loss: 3.69e-01\n",
            "  time: 246s (wall 225s)\n",
            "step 12200 / 72000 (epoch 20.33 / 120):\n",
            "  learning_rate = 3.58e-04, loss_average = 3.16e-01\n",
            "  validation accuracy: 86.12 (8612 / 10000), f1 (weighted): 86.14, loss: 3.77e-01\n",
            "  time: 250s (wall 228s)\n",
            "step 12400 / 72000 (epoch 20.67 / 120):\n",
            "  learning_rate = 3.58e-04, loss_average = 3.04e-01\n",
            "  validation accuracy: 86.31 (8631 / 10000), f1 (weighted): 86.23, loss: 3.74e-01\n",
            "  time: 255s (wall 232s)\n",
            "step 12600 / 72000 (epoch 21.00 / 120):\n",
            "  learning_rate = 3.58e-04, loss_average = 3.22e-01\n",
            "  validation accuracy: 86.71 (8671 / 10000), f1 (weighted): 86.64, loss: 3.65e-01\n",
            "  time: 259s (wall 236s)\n",
            "step 12800 / 72000 (epoch 21.33 / 120):\n",
            "  learning_rate = 3.41e-04, loss_average = 3.28e-01\n",
            "  validation accuracy: 86.68 (8668 / 10000), f1 (weighted): 86.55, loss: 3.67e-01\n",
            "  time: 263s (wall 239s)\n",
            "step 13000 / 72000 (epoch 21.67 / 120):\n",
            "  learning_rate = 3.41e-04, loss_average = 3.25e-01\n",
            "  validation accuracy: 86.41 (8641 / 10000), f1 (weighted): 86.16, loss: 3.72e-01\n",
            "  time: 267s (wall 243s)\n",
            "step 13200 / 72000 (epoch 22.00 / 120):\n",
            "  learning_rate = 3.41e-04, loss_average = 2.71e-01\n",
            "  validation accuracy: 86.34 (8634 / 10000), f1 (weighted): 86.23, loss: 3.68e-01\n",
            "  time: 271s (wall 247s)\n",
            "step 13400 / 72000 (epoch 22.33 / 120):\n",
            "  learning_rate = 3.24e-04, loss_average = 2.82e-01\n",
            "  validation accuracy: 86.56 (8656 / 10000), f1 (weighted): 86.62, loss: 3.69e-01\n",
            "  time: 275s (wall 250s)\n",
            "step 13600 / 72000 (epoch 22.67 / 120):\n",
            "  learning_rate = 3.24e-04, loss_average = 3.21e-01\n",
            "  validation accuracy: 86.60 (8660 / 10000), f1 (weighted): 86.58, loss: 3.64e-01\n",
            "  time: 279s (wall 254s)\n",
            "step 13800 / 72000 (epoch 23.00 / 120):\n",
            "  learning_rate = 3.24e-04, loss_average = 3.36e-01\n",
            "  validation accuracy: 86.56 (8656 / 10000), f1 (weighted): 86.55, loss: 3.68e-01\n",
            "  time: 283s (wall 257s)\n",
            "step 14000 / 72000 (epoch 23.33 / 120):\n",
            "  learning_rate = 3.07e-04, loss_average = 2.87e-01\n",
            "  validation accuracy: 86.46 (8646 / 10000), f1 (weighted): 86.42, loss: 3.65e-01\n",
            "  time: 287s (wall 261s)\n",
            "step 14200 / 72000 (epoch 23.67 / 120):\n",
            "  learning_rate = 3.07e-04, loss_average = 3.39e-01\n",
            "  validation accuracy: 86.57 (8657 / 10000), f1 (weighted): 86.54, loss: 3.64e-01\n",
            "  time: 291s (wall 265s)\n",
            "step 14400 / 72000 (epoch 24.00 / 120):\n",
            "  learning_rate = 3.07e-04, loss_average = 2.99e-01\n",
            "  validation accuracy: 86.58 (8658 / 10000), f1 (weighted): 86.43, loss: 3.67e-01\n",
            "  time: 295s (wall 268s)\n",
            "step 14600 / 72000 (epoch 24.33 / 120):\n",
            "  learning_rate = 2.92e-04, loss_average = 3.23e-01\n",
            "  validation accuracy: 86.71 (8671 / 10000), f1 (weighted): 86.63, loss: 3.66e-01\n",
            "  time: 299s (wall 272s)\n",
            "step 14800 / 72000 (epoch 24.67 / 120):\n",
            "  learning_rate = 2.92e-04, loss_average = 3.32e-01\n",
            "  validation accuracy: 86.31 (8631 / 10000), f1 (weighted): 86.33, loss: 3.72e-01\n",
            "  time: 303s (wall 276s)\n",
            "step 15000 / 72000 (epoch 25.00 / 120):\n",
            "  learning_rate = 2.92e-04, loss_average = 3.23e-01\n",
            "  validation accuracy: 86.34 (8634 / 10000), f1 (weighted): 86.44, loss: 3.68e-01\n",
            "  time: 308s (wall 279s)\n",
            "step 15200 / 72000 (epoch 25.33 / 120):\n",
            "  learning_rate = 2.77e-04, loss_average = 3.34e-01\n",
            "  validation accuracy: 86.92 (8692 / 10000), f1 (weighted): 86.84, loss: 3.61e-01\n",
            "  time: 312s (wall 283s)\n",
            "step 15400 / 72000 (epoch 25.67 / 120):\n",
            "  learning_rate = 2.77e-04, loss_average = 2.86e-01\n",
            "  validation accuracy: 86.80 (8680 / 10000), f1 (weighted): 86.69, loss: 3.62e-01\n",
            "  time: 316s (wall 287s)\n",
            "step 15600 / 72000 (epoch 26.00 / 120):\n",
            "  learning_rate = 2.77e-04, loss_average = 2.97e-01\n",
            "  validation accuracy: 86.55 (8655 / 10000), f1 (weighted): 86.47, loss: 3.62e-01\n",
            "  time: 320s (wall 290s)\n",
            "step 15800 / 72000 (epoch 26.33 / 120):\n",
            "  learning_rate = 2.64e-04, loss_average = 3.34e-01\n",
            "  validation accuracy: 86.55 (8655 / 10000), f1 (weighted): 86.48, loss: 3.72e-01\n",
            "  time: 324s (wall 294s)\n",
            "step 16000 / 72000 (epoch 26.67 / 120):\n",
            "  learning_rate = 2.64e-04, loss_average = 2.77e-01\n",
            "  validation accuracy: 86.82 (8682 / 10000), f1 (weighted): 86.70, loss: 3.63e-01\n",
            "  time: 328s (wall 297s)\n",
            "step 16200 / 72000 (epoch 27.00 / 120):\n",
            "  learning_rate = 2.64e-04, loss_average = 2.98e-01\n",
            "  validation accuracy: 86.86 (8686 / 10000), f1 (weighted): 86.83, loss: 3.61e-01\n",
            "  time: 332s (wall 301s)\n",
            "step 16400 / 72000 (epoch 27.33 / 120):\n",
            "  learning_rate = 2.50e-04, loss_average = 3.34e-01\n",
            "  validation accuracy: 86.41 (8641 / 10000), f1 (weighted): 86.52, loss: 3.66e-01\n",
            "  time: 336s (wall 305s)\n",
            "step 16600 / 72000 (epoch 27.67 / 120):\n",
            "  learning_rate = 2.50e-04, loss_average = 2.93e-01\n",
            "  validation accuracy: 86.91 (8691 / 10000), f1 (weighted): 86.80, loss: 3.59e-01\n",
            "  time: 340s (wall 308s)\n",
            "step 16800 / 72000 (epoch 28.00 / 120):\n",
            "  learning_rate = 2.50e-04, loss_average = 3.16e-01\n",
            "  validation accuracy: 86.68 (8668 / 10000), f1 (weighted): 86.61, loss: 3.61e-01\n",
            "  time: 344s (wall 312s)\n",
            "step 17000 / 72000 (epoch 28.33 / 120):\n",
            "  learning_rate = 2.38e-04, loss_average = 2.86e-01\n",
            "  validation accuracy: 86.68 (8668 / 10000), f1 (weighted): 86.44, loss: 3.62e-01\n",
            "  time: 348s (wall 316s)\n",
            "step 17200 / 72000 (epoch 28.67 / 120):\n",
            "  learning_rate = 2.38e-04, loss_average = 3.01e-01\n",
            "  validation accuracy: 86.80 (8680 / 10000), f1 (weighted): 86.82, loss: 3.60e-01\n",
            "  time: 352s (wall 319s)\n",
            "step 17400 / 72000 (epoch 29.00 / 120):\n",
            "  learning_rate = 2.38e-04, loss_average = 2.81e-01\n",
            "  validation accuracy: 86.92 (8692 / 10000), f1 (weighted): 86.81, loss: 3.59e-01\n",
            "  time: 356s (wall 323s)\n",
            "step 17600 / 72000 (epoch 29.33 / 120):\n",
            "  learning_rate = 2.26e-04, loss_average = 3.02e-01\n",
            "  validation accuracy: 86.89 (8689 / 10000), f1 (weighted): 86.85, loss: 3.59e-01\n",
            "  time: 360s (wall 326s)\n",
            "step 17800 / 72000 (epoch 29.67 / 120):\n",
            "  learning_rate = 2.26e-04, loss_average = 3.02e-01\n",
            "  validation accuracy: 86.97 (8697 / 10000), f1 (weighted): 86.92, loss: 3.58e-01\n",
            "  time: 365s (wall 330s)\n",
            "step 18000 / 72000 (epoch 30.00 / 120):\n",
            "  learning_rate = 2.26e-04, loss_average = 3.17e-01\n",
            "  validation accuracy: 86.50 (8650 / 10000), f1 (weighted): 86.49, loss: 3.66e-01\n",
            "  time: 369s (wall 334s)\n",
            "step 18200 / 72000 (epoch 30.33 / 120):\n",
            "  learning_rate = 2.15e-04, loss_average = 3.01e-01\n",
            "  validation accuracy: 86.77 (8677 / 10000), f1 (weighted): 86.79, loss: 3.58e-01\n",
            "  time: 373s (wall 337s)\n",
            "step 18400 / 72000 (epoch 30.67 / 120):\n",
            "  learning_rate = 2.15e-04, loss_average = 3.07e-01\n",
            "  validation accuracy: 86.76 (8676 / 10000), f1 (weighted): 86.55, loss: 3.63e-01\n",
            "  time: 377s (wall 341s)\n",
            "step 18600 / 72000 (epoch 31.00 / 120):\n",
            "  learning_rate = 2.15e-04, loss_average = 2.91e-01\n",
            "  validation accuracy: 86.73 (8673 / 10000), f1 (weighted): 86.82, loss: 3.59e-01\n",
            "  time: 381s (wall 345s)\n",
            "step 18800 / 72000 (epoch 31.33 / 120):\n",
            "  learning_rate = 2.04e-04, loss_average = 3.12e-01\n",
            "  validation accuracy: 86.73 (8673 / 10000), f1 (weighted): 86.66, loss: 3.62e-01\n",
            "  time: 385s (wall 348s)\n",
            "step 19000 / 72000 (epoch 31.67 / 120):\n",
            "  learning_rate = 2.04e-04, loss_average = 2.89e-01\n",
            "  validation accuracy: 86.90 (8690 / 10000), f1 (weighted): 86.89, loss: 3.58e-01\n",
            "  time: 389s (wall 352s)\n",
            "step 19200 / 72000 (epoch 32.00 / 120):\n",
            "  learning_rate = 2.04e-04, loss_average = 2.82e-01\n",
            "  validation accuracy: 86.89 (8689 / 10000), f1 (weighted): 86.80, loss: 3.56e-01\n",
            "  time: 393s (wall 355s)\n",
            "step 19400 / 72000 (epoch 32.33 / 120):\n",
            "  learning_rate = 1.94e-04, loss_average = 2.95e-01\n",
            "  validation accuracy: 87.06 (8706 / 10000), f1 (weighted): 86.92, loss: 3.58e-01\n",
            "  time: 397s (wall 359s)\n",
            "step 19600 / 72000 (epoch 32.67 / 120):\n",
            "  learning_rate = 1.94e-04, loss_average = 2.81e-01\n",
            "  validation accuracy: 86.61 (8661 / 10000), f1 (weighted): 86.41, loss: 3.62e-01\n",
            "  time: 401s (wall 363s)\n",
            "step 19800 / 72000 (epoch 33.00 / 120):\n",
            "  learning_rate = 1.94e-04, loss_average = 3.05e-01\n",
            "  validation accuracy: 86.98 (8698 / 10000), f1 (weighted): 86.93, loss: 3.55e-01\n",
            "  time: 406s (wall 366s)\n",
            "step 20000 / 72000 (epoch 33.33 / 120):\n",
            "  learning_rate = 1.84e-04, loss_average = 2.81e-01\n",
            "  validation accuracy: 86.79 (8679 / 10000), f1 (weighted): 86.73, loss: 3.57e-01\n",
            "  time: 410s (wall 370s)\n",
            "step 20200 / 72000 (epoch 33.67 / 120):\n",
            "  learning_rate = 1.84e-04, loss_average = 3.00e-01\n",
            "  validation accuracy: 86.97 (8697 / 10000), f1 (weighted): 86.88, loss: 3.55e-01\n",
            "  time: 414s (wall 373s)\n",
            "step 20400 / 72000 (epoch 34.00 / 120):\n",
            "  learning_rate = 1.84e-04, loss_average = 2.92e-01\n",
            "  validation accuracy: 86.91 (8691 / 10000), f1 (weighted): 86.89, loss: 3.55e-01\n",
            "  time: 418s (wall 377s)\n",
            "step 20600 / 72000 (epoch 34.33 / 120):\n",
            "  learning_rate = 1.75e-04, loss_average = 3.23e-01\n",
            "  validation accuracy: 86.86 (8686 / 10000), f1 (weighted): 86.82, loss: 3.56e-01\n",
            "  time: 422s (wall 381s)\n",
            "step 20800 / 72000 (epoch 34.67 / 120):\n",
            "  learning_rate = 1.75e-04, loss_average = 2.88e-01\n",
            "  validation accuracy: 87.10 (8710 / 10000), f1 (weighted): 86.96, loss: 3.56e-01\n",
            "  time: 426s (wall 384s)\n",
            "step 21000 / 72000 (epoch 35.00 / 120):\n",
            "  learning_rate = 1.75e-04, loss_average = 2.82e-01\n",
            "  validation accuracy: 86.88 (8688 / 10000), f1 (weighted): 86.90, loss: 3.59e-01\n",
            "  time: 430s (wall 388s)\n",
            "step 21200 / 72000 (epoch 35.33 / 120):\n",
            "  learning_rate = 1.66e-04, loss_average = 3.10e-01\n",
            "  validation accuracy: 86.91 (8691 / 10000), f1 (weighted): 86.77, loss: 3.58e-01\n",
            "  time: 434s (wall 392s)\n",
            "step 21400 / 72000 (epoch 35.67 / 120):\n",
            "  learning_rate = 1.66e-04, loss_average = 2.71e-01\n",
            "  validation accuracy: 86.86 (8686 / 10000), f1 (weighted): 86.84, loss: 3.57e-01\n",
            "  time: 438s (wall 395s)\n",
            "step 21600 / 72000 (epoch 36.00 / 120):\n",
            "  learning_rate = 1.66e-04, loss_average = 2.96e-01\n",
            "  validation accuracy: 86.91 (8691 / 10000), f1 (weighted): 86.90, loss: 3.57e-01\n",
            "  time: 442s (wall 399s)\n",
            "step 21800 / 72000 (epoch 36.33 / 120):\n",
            "  learning_rate = 1.58e-04, loss_average = 2.65e-01\n",
            "  validation accuracy: 87.25 (8725 / 10000), f1 (weighted): 87.15, loss: 3.55e-01\n",
            "  time: 446s (wall 403s)\n",
            "step 22000 / 72000 (epoch 36.67 / 120):\n",
            "  learning_rate = 1.58e-04, loss_average = 2.94e-01\n",
            "  validation accuracy: 86.94 (8694 / 10000), f1 (weighted): 86.91, loss: 3.56e-01\n",
            "  time: 451s (wall 406s)\n",
            "step 22200 / 72000 (epoch 37.00 / 120):\n",
            "  learning_rate = 1.58e-04, loss_average = 2.71e-01\n",
            "  validation accuracy: 86.88 (8688 / 10000), f1 (weighted): 86.72, loss: 3.58e-01\n",
            "  time: 455s (wall 410s)\n",
            "step 22400 / 72000 (epoch 37.33 / 120):\n",
            "  learning_rate = 1.50e-04, loss_average = 3.15e-01\n",
            "  validation accuracy: 87.14 (8714 / 10000), f1 (weighted): 87.09, loss: 3.54e-01\n",
            "  time: 459s (wall 414s)\n",
            "step 22600 / 72000 (epoch 37.67 / 120):\n",
            "  learning_rate = 1.50e-04, loss_average = 2.83e-01\n",
            "  validation accuracy: 86.84 (8684 / 10000), f1 (weighted): 86.57, loss: 3.58e-01\n",
            "  time: 463s (wall 417s)\n",
            "step 22800 / 72000 (epoch 38.00 / 120):\n",
            "  learning_rate = 1.50e-04, loss_average = 2.98e-01\n",
            "  validation accuracy: 87.09 (8709 / 10000), f1 (weighted): 86.99, loss: 3.53e-01\n",
            "  time: 467s (wall 421s)\n",
            "step 23000 / 72000 (epoch 38.33 / 120):\n",
            "  learning_rate = 1.42e-04, loss_average = 2.78e-01\n",
            "  validation accuracy: 87.08 (8708 / 10000), f1 (weighted): 87.06, loss: 3.53e-01\n",
            "  time: 471s (wall 424s)\n",
            "step 23200 / 72000 (epoch 38.67 / 120):\n",
            "  learning_rate = 1.42e-04, loss_average = 3.02e-01\n",
            "  validation accuracy: 87.05 (8705 / 10000), f1 (weighted): 86.99, loss: 3.55e-01\n",
            "  time: 475s (wall 428s)\n",
            "step 23400 / 72000 (epoch 39.00 / 120):\n",
            "  learning_rate = 1.42e-04, loss_average = 2.90e-01\n",
            "  validation accuracy: 86.92 (8692 / 10000), f1 (weighted): 86.93, loss: 3.57e-01\n",
            "  time: 479s (wall 432s)\n",
            "step 23600 / 72000 (epoch 39.33 / 120):\n",
            "  learning_rate = 1.35e-04, loss_average = 2.94e-01\n",
            "  validation accuracy: 87.10 (8710 / 10000), f1 (weighted): 87.01, loss: 3.53e-01\n",
            "  time: 483s (wall 435s)\n",
            "step 23800 / 72000 (epoch 39.67 / 120):\n",
            "  learning_rate = 1.35e-04, loss_average = 2.92e-01\n",
            "  validation accuracy: 87.02 (8702 / 10000), f1 (weighted): 86.91, loss: 3.56e-01\n",
            "  time: 487s (wall 439s)\n",
            "step 24000 / 72000 (epoch 40.00 / 120):\n",
            "  learning_rate = 1.35e-04, loss_average = 2.50e-01\n",
            "  validation accuracy: 86.75 (8675 / 10000), f1 (weighted): 86.85, loss: 3.58e-01\n",
            "  time: 491s (wall 443s)\n",
            "step 24200 / 72000 (epoch 40.33 / 120):\n",
            "  learning_rate = 1.29e-04, loss_average = 2.96e-01\n",
            "  validation accuracy: 86.89 (8689 / 10000), f1 (weighted): 86.87, loss: 3.57e-01\n",
            "  time: 495s (wall 446s)\n",
            "step 24400 / 72000 (epoch 40.67 / 120):\n",
            "  learning_rate = 1.29e-04, loss_average = 2.98e-01\n",
            "  validation accuracy: 86.99 (8699 / 10000), f1 (weighted): 86.89, loss: 3.53e-01\n",
            "  time: 500s (wall 450s)\n",
            "step 24600 / 72000 (epoch 41.00 / 120):\n",
            "  learning_rate = 1.29e-04, loss_average = 3.07e-01\n",
            "  validation accuracy: 87.04 (8704 / 10000), f1 (weighted): 87.01, loss: 3.55e-01\n",
            "  time: 504s (wall 454s)\n",
            "step 24800 / 72000 (epoch 41.33 / 120):\n",
            "  learning_rate = 1.22e-04, loss_average = 3.09e-01\n",
            "  validation accuracy: 87.10 (8710 / 10000), f1 (weighted): 86.99, loss: 3.55e-01\n",
            "  time: 508s (wall 457s)\n",
            "step 25000 / 72000 (epoch 41.67 / 120):\n",
            "  learning_rate = 1.22e-04, loss_average = 2.75e-01\n",
            "  validation accuracy: 87.12 (8712 / 10000), f1 (weighted): 87.02, loss: 3.53e-01\n",
            "  time: 512s (wall 461s)\n",
            "step 25200 / 72000 (epoch 42.00 / 120):\n",
            "  learning_rate = 1.22e-04, loss_average = 2.73e-01\n",
            "  validation accuracy: 86.92 (8692 / 10000), f1 (weighted): 86.98, loss: 3.56e-01\n",
            "  time: 516s (wall 464s)\n",
            "step 25400 / 72000 (epoch 42.33 / 120):\n",
            "  learning_rate = 1.16e-04, loss_average = 2.82e-01\n",
            "  validation accuracy: 87.08 (8708 / 10000), f1 (weighted): 87.01, loss: 3.54e-01\n",
            "  time: 520s (wall 468s)\n",
            "step 25600 / 72000 (epoch 42.67 / 120):\n",
            "  learning_rate = 1.16e-04, loss_average = 2.87e-01\n",
            "  validation accuracy: 87.08 (8708 / 10000), f1 (weighted): 86.99, loss: 3.55e-01\n",
            "  time: 524s (wall 472s)\n",
            "step 25800 / 72000 (epoch 43.00 / 120):\n",
            "  learning_rate = 1.16e-04, loss_average = 2.74e-01\n",
            "  validation accuracy: 87.04 (8704 / 10000), f1 (weighted): 86.89, loss: 3.53e-01\n",
            "  time: 528s (wall 475s)\n",
            "step 26000 / 72000 (epoch 43.33 / 120):\n",
            "  learning_rate = 1.10e-04, loss_average = 2.70e-01\n",
            "  validation accuracy: 87.09 (8709 / 10000), f1 (weighted): 87.02, loss: 3.53e-01\n",
            "  time: 532s (wall 479s)\n",
            "step 26200 / 72000 (epoch 43.67 / 120):\n",
            "  learning_rate = 1.10e-04, loss_average = 2.86e-01\n",
            "  validation accuracy: 86.97 (8697 / 10000), f1 (weighted): 86.96, loss: 3.55e-01\n",
            "  time: 537s (wall 483s)\n",
            "step 26400 / 72000 (epoch 44.00 / 120):\n",
            "  learning_rate = 1.10e-04, loss_average = 3.02e-01\n",
            "  validation accuracy: 87.10 (8710 / 10000), f1 (weighted): 87.07, loss: 3.52e-01\n",
            "  time: 541s (wall 486s)\n",
            "step 26600 / 72000 (epoch 44.33 / 120):\n",
            "  learning_rate = 1.05e-04, loss_average = 2.58e-01\n",
            "  validation accuracy: 87.11 (8711 / 10000), f1 (weighted): 87.01, loss: 3.54e-01\n",
            "  time: 545s (wall 490s)\n",
            "step 26800 / 72000 (epoch 44.67 / 120):\n",
            "  learning_rate = 1.05e-04, loss_average = 2.81e-01\n",
            "  validation accuracy: 87.20 (8720 / 10000), f1 (weighted): 87.06, loss: 3.52e-01\n",
            "  time: 549s (wall 493s)\n",
            "step 27000 / 72000 (epoch 45.00 / 120):\n",
            "  learning_rate = 1.05e-04, loss_average = 2.95e-01\n",
            "  validation accuracy: 87.17 (8717 / 10000), f1 (weighted): 87.17, loss: 3.52e-01\n",
            "  time: 553s (wall 497s)\n",
            "step 27200 / 72000 (epoch 45.33 / 120):\n",
            "  learning_rate = 9.94e-05, loss_average = 2.56e-01\n",
            "  validation accuracy: 87.07 (8707 / 10000), f1 (weighted): 87.00, loss: 3.53e-01\n",
            "  time: 557s (wall 501s)\n",
            "step 27400 / 72000 (epoch 45.67 / 120):\n",
            "  learning_rate = 9.94e-05, loss_average = 2.76e-01\n",
            "  validation accuracy: 86.86 (8686 / 10000), f1 (weighted): 86.87, loss: 3.57e-01\n",
            "  time: 561s (wall 504s)\n",
            "step 27600 / 72000 (epoch 46.00 / 120):\n",
            "  learning_rate = 9.94e-05, loss_average = 2.75e-01\n",
            "  validation accuracy: 87.14 (8714 / 10000), f1 (weighted): 87.10, loss: 3.51e-01\n",
            "  time: 565s (wall 508s)\n",
            "step 27800 / 72000 (epoch 46.33 / 120):\n",
            "  learning_rate = 9.45e-05, loss_average = 2.90e-01\n",
            "  validation accuracy: 87.17 (8717 / 10000), f1 (weighted): 87.10, loss: 3.52e-01\n",
            "  time: 569s (wall 512s)\n",
            "step 28000 / 72000 (epoch 46.67 / 120):\n",
            "  learning_rate = 9.45e-05, loss_average = 2.80e-01\n",
            "  validation accuracy: 87.20 (8720 / 10000), f1 (weighted): 87.12, loss: 3.51e-01\n",
            "  time: 573s (wall 515s)\n",
            "step 28200 / 72000 (epoch 47.00 / 120):\n",
            "  learning_rate = 9.45e-05, loss_average = 2.75e-01\n",
            "  validation accuracy: 87.15 (8715 / 10000), f1 (weighted): 87.12, loss: 3.53e-01\n",
            "  time: 577s (wall 519s)\n",
            "step 28400 / 72000 (epoch 47.33 / 120):\n",
            "  learning_rate = 8.97e-05, loss_average = 2.45e-01\n",
            "  validation accuracy: 87.26 (8726 / 10000), f1 (weighted): 87.21, loss: 3.53e-01\n",
            "  time: 581s (wall 522s)\n",
            "step 28600 / 72000 (epoch 47.67 / 120):\n",
            "  learning_rate = 8.97e-05, loss_average = 2.98e-01\n",
            "  validation accuracy: 87.10 (8710 / 10000), f1 (weighted): 87.10, loss: 3.51e-01\n",
            "  time: 585s (wall 526s)\n",
            "step 28800 / 72000 (epoch 48.00 / 120):\n",
            "  learning_rate = 8.97e-05, loss_average = 3.06e-01\n",
            "  validation accuracy: 87.09 (8709 / 10000), f1 (weighted): 87.09, loss: 3.51e-01\n",
            "  time: 589s (wall 530s)\n",
            "step 29000 / 72000 (epoch 48.33 / 120):\n",
            "  learning_rate = 8.53e-05, loss_average = 2.80e-01\n",
            "  validation accuracy: 87.03 (8703 / 10000), f1 (weighted): 86.97, loss: 3.52e-01\n",
            "  time: 594s (wall 533s)\n",
            "step 29200 / 72000 (epoch 48.67 / 120):\n",
            "  learning_rate = 8.53e-05, loss_average = 2.83e-01\n",
            "  validation accuracy: 86.87 (8687 / 10000), f1 (weighted): 86.83, loss: 3.57e-01\n",
            "  time: 598s (wall 537s)\n",
            "step 29400 / 72000 (epoch 49.00 / 120):\n",
            "  learning_rate = 8.53e-05, loss_average = 2.80e-01\n",
            "  validation accuracy: 86.95 (8695 / 10000), f1 (weighted): 86.93, loss: 3.54e-01\n",
            "  time: 602s (wall 541s)\n",
            "step 29600 / 72000 (epoch 49.33 / 120):\n",
            "  learning_rate = 8.10e-05, loss_average = 2.88e-01\n",
            "  validation accuracy: 87.21 (8721 / 10000), f1 (weighted): 87.12, loss: 3.51e-01\n",
            "  time: 606s (wall 544s)\n",
            "step 29800 / 72000 (epoch 49.67 / 120):\n",
            "  learning_rate = 8.10e-05, loss_average = 2.62e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.19, loss: 3.51e-01\n",
            "  time: 610s (wall 548s)\n",
            "step 30000 / 72000 (epoch 50.00 / 120):\n",
            "  learning_rate = 8.10e-05, loss_average = 3.06e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.22, loss: 3.51e-01\n",
            "  time: 614s (wall 551s)\n",
            "step 30200 / 72000 (epoch 50.33 / 120):\n",
            "  learning_rate = 7.69e-05, loss_average = 2.93e-01\n",
            "  validation accuracy: 87.12 (8712 / 10000), f1 (weighted): 87.08, loss: 3.51e-01\n",
            "  time: 618s (wall 555s)\n",
            "step 30400 / 72000 (epoch 50.67 / 120):\n",
            "  learning_rate = 7.69e-05, loss_average = 2.74e-01\n",
            "  validation accuracy: 87.09 (8709 / 10000), f1 (weighted): 87.03, loss: 3.52e-01\n",
            "  time: 622s (wall 559s)\n",
            "step 30600 / 72000 (epoch 51.00 / 120):\n",
            "  learning_rate = 7.69e-05, loss_average = 2.85e-01\n",
            "  validation accuracy: 87.19 (8719 / 10000), f1 (weighted): 87.10, loss: 3.50e-01\n",
            "  time: 626s (wall 562s)\n",
            "step 30800 / 72000 (epoch 51.33 / 120):\n",
            "  learning_rate = 7.31e-05, loss_average = 3.12e-01\n",
            "  validation accuracy: 87.32 (8732 / 10000), f1 (weighted): 87.24, loss: 3.50e-01\n",
            "  time: 631s (wall 566s)\n",
            "step 31000 / 72000 (epoch 51.67 / 120):\n",
            "  learning_rate = 7.31e-05, loss_average = 2.49e-01\n",
            "  validation accuracy: 87.10 (8710 / 10000), f1 (weighted): 87.04, loss: 3.53e-01\n",
            "  time: 635s (wall 570s)\n",
            "step 31200 / 72000 (epoch 52.00 / 120):\n",
            "  learning_rate = 7.31e-05, loss_average = 2.96e-01\n",
            "  validation accuracy: 87.25 (8725 / 10000), f1 (weighted): 87.17, loss: 3.51e-01\n",
            "  time: 639s (wall 573s)\n",
            "step 31400 / 72000 (epoch 52.33 / 120):\n",
            "  learning_rate = 6.94e-05, loss_average = 2.88e-01\n",
            "  validation accuracy: 87.25 (8725 / 10000), f1 (weighted): 87.15, loss: 3.51e-01\n",
            "  time: 643s (wall 577s)\n",
            "step 31600 / 72000 (epoch 52.67 / 120):\n",
            "  learning_rate = 6.94e-05, loss_average = 2.87e-01\n",
            "  validation accuracy: 87.17 (8717 / 10000), f1 (weighted): 87.14, loss: 3.51e-01\n",
            "  time: 647s (wall 581s)\n",
            "step 31800 / 72000 (epoch 53.00 / 120):\n",
            "  learning_rate = 6.94e-05, loss_average = 2.79e-01\n",
            "  validation accuracy: 87.33 (8733 / 10000), f1 (weighted): 87.29, loss: 3.50e-01\n",
            "  time: 651s (wall 584s)\n",
            "step 32000 / 72000 (epoch 53.33 / 120):\n",
            "  learning_rate = 6.60e-05, loss_average = 2.73e-01\n",
            "  validation accuracy: 87.06 (8706 / 10000), f1 (weighted): 87.06, loss: 3.50e-01\n",
            "  time: 656s (wall 588s)\n",
            "step 32200 / 72000 (epoch 53.67 / 120):\n",
            "  learning_rate = 6.60e-05, loss_average = 2.83e-01\n",
            "  validation accuracy: 87.39 (8739 / 10000), f1 (weighted): 87.35, loss: 3.50e-01\n",
            "  time: 660s (wall 592s)\n",
            "step 32400 / 72000 (epoch 54.00 / 120):\n",
            "  learning_rate = 6.60e-05, loss_average = 2.49e-01\n",
            "  validation accuracy: 87.07 (8707 / 10000), f1 (weighted): 87.02, loss: 3.51e-01\n",
            "  time: 664s (wall 595s)\n",
            "step 32600 / 72000 (epoch 54.33 / 120):\n",
            "  learning_rate = 6.27e-05, loss_average = 2.72e-01\n",
            "  validation accuracy: 87.09 (8709 / 10000), f1 (weighted): 87.00, loss: 3.51e-01\n",
            "  time: 668s (wall 599s)\n",
            "step 32800 / 72000 (epoch 54.67 / 120):\n",
            "  learning_rate = 6.27e-05, loss_average = 2.52e-01\n",
            "  validation accuracy: 87.18 (8718 / 10000), f1 (weighted): 87.06, loss: 3.51e-01\n",
            "  time: 672s (wall 603s)\n",
            "step 33000 / 72000 (epoch 55.00 / 120):\n",
            "  learning_rate = 6.27e-05, loss_average = 2.79e-01\n",
            "  validation accuracy: 87.13 (8713 / 10000), f1 (weighted): 86.98, loss: 3.53e-01\n",
            "  time: 677s (wall 606s)\n",
            "step 33200 / 72000 (epoch 55.33 / 120):\n",
            "  learning_rate = 5.95e-05, loss_average = 2.65e-01\n",
            "  validation accuracy: 87.19 (8719 / 10000), f1 (weighted): 87.20, loss: 3.51e-01\n",
            "  time: 681s (wall 610s)\n",
            "step 33400 / 72000 (epoch 55.67 / 120):\n",
            "  learning_rate = 5.95e-05, loss_average = 2.96e-01\n",
            "  validation accuracy: 87.05 (8705 / 10000), f1 (weighted): 86.96, loss: 3.53e-01\n",
            "  time: 685s (wall 613s)\n",
            "step 33600 / 72000 (epoch 56.00 / 120):\n",
            "  learning_rate = 5.95e-05, loss_average = 2.85e-01\n",
            "  validation accuracy: 87.12 (8712 / 10000), f1 (weighted): 87.12, loss: 3.51e-01\n",
            "  time: 689s (wall 617s)\n",
            "step 33800 / 72000 (epoch 56.33 / 120):\n",
            "  learning_rate = 5.66e-05, loss_average = 2.88e-01\n",
            "  validation accuracy: 87.18 (8718 / 10000), f1 (weighted): 87.14, loss: 3.51e-01\n",
            "  time: 693s (wall 621s)\n",
            "step 34000 / 72000 (epoch 56.67 / 120):\n",
            "  learning_rate = 5.66e-05, loss_average = 3.03e-01\n",
            "  validation accuracy: 87.17 (8717 / 10000), f1 (weighted): 87.11, loss: 3.50e-01\n",
            "  time: 697s (wall 624s)\n",
            "step 34200 / 72000 (epoch 57.00 / 120):\n",
            "  learning_rate = 5.66e-05, loss_average = 2.68e-01\n",
            "  validation accuracy: 87.23 (8723 / 10000), f1 (weighted): 87.18, loss: 3.50e-01\n",
            "  time: 701s (wall 628s)\n",
            "step 34400 / 72000 (epoch 57.33 / 120):\n",
            "  learning_rate = 5.37e-05, loss_average = 2.80e-01\n",
            "  validation accuracy: 87.07 (8707 / 10000), f1 (weighted): 87.07, loss: 3.53e-01\n",
            "  time: 705s (wall 632s)\n",
            "step 34600 / 72000 (epoch 57.67 / 120):\n",
            "  learning_rate = 5.37e-05, loss_average = 2.55e-01\n",
            "  validation accuracy: 87.07 (8707 / 10000), f1 (weighted): 86.98, loss: 3.51e-01\n",
            "  time: 709s (wall 635s)\n",
            "step 34800 / 72000 (epoch 58.00 / 120):\n",
            "  learning_rate = 5.37e-05, loss_average = 2.85e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.26, loss: 3.50e-01\n",
            "  time: 714s (wall 639s)\n",
            "step 35000 / 72000 (epoch 58.33 / 120):\n",
            "  learning_rate = 5.10e-05, loss_average = 2.76e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.30, loss: 3.50e-01\n",
            "  time: 718s (wall 642s)\n",
            "step 35200 / 72000 (epoch 58.67 / 120):\n",
            "  learning_rate = 5.10e-05, loss_average = 2.75e-01\n",
            "  validation accuracy: 87.33 (8733 / 10000), f1 (weighted): 87.20, loss: 3.50e-01\n",
            "  time: 722s (wall 646s)\n",
            "step 35400 / 72000 (epoch 59.00 / 120):\n",
            "  learning_rate = 5.10e-05, loss_average = 2.83e-01\n",
            "  validation accuracy: 87.07 (8707 / 10000), f1 (weighted): 87.01, loss: 3.52e-01\n",
            "  time: 726s (wall 650s)\n",
            "step 35600 / 72000 (epoch 59.33 / 120):\n",
            "  learning_rate = 4.85e-05, loss_average = 2.61e-01\n",
            "  validation accuracy: 87.38 (8738 / 10000), f1 (weighted): 87.25, loss: 3.50e-01\n",
            "  time: 730s (wall 653s)\n",
            "step 35800 / 72000 (epoch 59.67 / 120):\n",
            "  learning_rate = 4.85e-05, loss_average = 2.71e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.27, loss: 3.50e-01\n",
            "  time: 734s (wall 657s)\n",
            "step 36000 / 72000 (epoch 60.00 / 120):\n",
            "  learning_rate = 4.85e-05, loss_average = 2.79e-01\n",
            "  validation accuracy: 87.03 (8703 / 10000), f1 (weighted): 87.06, loss: 3.52e-01\n",
            "  time: 738s (wall 660s)\n",
            "step 36200 / 72000 (epoch 60.33 / 120):\n",
            "  learning_rate = 4.61e-05, loss_average = 2.55e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.24, loss: 3.49e-01\n",
            "  time: 742s (wall 664s)\n",
            "step 36400 / 72000 (epoch 60.67 / 120):\n",
            "  learning_rate = 4.61e-05, loss_average = 2.71e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.25, loss: 3.49e-01\n",
            "  time: 746s (wall 668s)\n",
            "step 36600 / 72000 (epoch 61.00 / 120):\n",
            "  learning_rate = 4.61e-05, loss_average = 2.85e-01\n",
            "  validation accuracy: 87.26 (8726 / 10000), f1 (weighted): 87.24, loss: 3.50e-01\n",
            "  time: 751s (wall 671s)\n",
            "step 36800 / 72000 (epoch 61.33 / 120):\n",
            "  learning_rate = 4.38e-05, loss_average = 2.59e-01\n",
            "  validation accuracy: 87.16 (8716 / 10000), f1 (weighted): 87.13, loss: 3.50e-01\n",
            "  time: 755s (wall 675s)\n",
            "step 37000 / 72000 (epoch 61.67 / 120):\n",
            "  learning_rate = 4.38e-05, loss_average = 2.77e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.28, loss: 3.49e-01\n",
            "  time: 759s (wall 679s)\n",
            "step 37200 / 72000 (epoch 62.00 / 120):\n",
            "  learning_rate = 4.38e-05, loss_average = 2.69e-01\n",
            "  validation accuracy: 87.20 (8720 / 10000), f1 (weighted): 87.10, loss: 3.51e-01\n",
            "  time: 763s (wall 682s)\n",
            "step 37400 / 72000 (epoch 62.33 / 120):\n",
            "  learning_rate = 4.16e-05, loss_average = 2.80e-01\n",
            "  validation accuracy: 87.15 (8715 / 10000), f1 (weighted): 87.15, loss: 3.50e-01\n",
            "  time: 767s (wall 686s)\n",
            "step 37600 / 72000 (epoch 62.67 / 120):\n",
            "  learning_rate = 4.16e-05, loss_average = 2.92e-01\n",
            "  validation accuracy: 87.15 (8715 / 10000), f1 (weighted): 87.11, loss: 3.49e-01\n",
            "  time: 771s (wall 689s)\n",
            "step 37800 / 72000 (epoch 63.00 / 120):\n",
            "  learning_rate = 4.16e-05, loss_average = 2.70e-01\n",
            "  validation accuracy: 87.21 (8721 / 10000), f1 (weighted): 87.15, loss: 3.50e-01\n",
            "  time: 775s (wall 693s)\n",
            "step 38000 / 72000 (epoch 63.33 / 120):\n",
            "  learning_rate = 3.95e-05, loss_average = 2.73e-01\n",
            "  validation accuracy: 87.23 (8723 / 10000), f1 (weighted): 87.21, loss: 3.50e-01\n",
            "  time: 779s (wall 697s)\n",
            "step 38200 / 72000 (epoch 63.67 / 120):\n",
            "  learning_rate = 3.95e-05, loss_average = 2.97e-01\n",
            "  validation accuracy: 87.40 (8740 / 10000), f1 (weighted): 87.38, loss: 3.49e-01\n",
            "  time: 783s (wall 700s)\n",
            "step 38400 / 72000 (epoch 64.00 / 120):\n",
            "  learning_rate = 3.95e-05, loss_average = 2.78e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.21, loss: 3.49e-01\n",
            "  time: 788s (wall 704s)\n",
            "step 38600 / 72000 (epoch 64.33 / 120):\n",
            "  learning_rate = 3.75e-05, loss_average = 3.10e-01\n",
            "  validation accuracy: 87.33 (8733 / 10000), f1 (weighted): 87.20, loss: 3.49e-01\n",
            "  time: 792s (wall 707s)\n",
            "step 38800 / 72000 (epoch 64.67 / 120):\n",
            "  learning_rate = 3.75e-05, loss_average = 3.08e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.21, loss: 3.50e-01\n",
            "  time: 796s (wall 711s)\n",
            "step 39000 / 72000 (epoch 65.00 / 120):\n",
            "  learning_rate = 3.75e-05, loss_average = 2.54e-01\n",
            "  validation accuracy: 87.08 (8708 / 10000), f1 (weighted): 87.04, loss: 3.50e-01\n",
            "  time: 800s (wall 715s)\n",
            "step 39200 / 72000 (epoch 65.33 / 120):\n",
            "  learning_rate = 3.56e-05, loss_average = 2.74e-01\n",
            "  validation accuracy: 87.33 (8733 / 10000), f1 (weighted): 87.25, loss: 3.49e-01\n",
            "  time: 804s (wall 718s)\n",
            "step 39400 / 72000 (epoch 65.67 / 120):\n",
            "  learning_rate = 3.56e-05, loss_average = 2.74e-01\n",
            "  validation accuracy: 87.29 (8729 / 10000), f1 (weighted): 87.29, loss: 3.49e-01\n",
            "  time: 808s (wall 722s)\n",
            "step 39600 / 72000 (epoch 66.00 / 120):\n",
            "  learning_rate = 3.56e-05, loss_average = 2.95e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.24, loss: 3.48e-01\n",
            "  time: 812s (wall 726s)\n",
            "step 39800 / 72000 (epoch 66.33 / 120):\n",
            "  learning_rate = 3.39e-05, loss_average = 2.67e-01\n",
            "  validation accuracy: 87.36 (8736 / 10000), f1 (weighted): 87.28, loss: 3.48e-01\n",
            "  time: 816s (wall 729s)\n",
            "step 40000 / 72000 (epoch 66.67 / 120):\n",
            "  learning_rate = 3.39e-05, loss_average = 2.76e-01\n",
            "  validation accuracy: 87.29 (8729 / 10000), f1 (weighted): 87.23, loss: 3.49e-01\n",
            "  time: 820s (wall 733s)\n",
            "step 40200 / 72000 (epoch 67.00 / 120):\n",
            "  learning_rate = 3.39e-05, loss_average = 2.82e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.22, loss: 3.49e-01\n",
            "  time: 824s (wall 736s)\n",
            "step 40400 / 72000 (epoch 67.33 / 120):\n",
            "  learning_rate = 3.22e-05, loss_average = 2.57e-01\n",
            "  validation accuracy: 87.19 (8719 / 10000), f1 (weighted): 87.15, loss: 3.48e-01\n",
            "  time: 829s (wall 740s)\n",
            "step 40600 / 72000 (epoch 67.67 / 120):\n",
            "  learning_rate = 3.22e-05, loss_average = 2.71e-01\n",
            "  validation accuracy: 87.22 (8722 / 10000), f1 (weighted): 87.19, loss: 3.48e-01\n",
            "  time: 833s (wall 744s)\n",
            "step 40800 / 72000 (epoch 68.00 / 120):\n",
            "  learning_rate = 3.22e-05, loss_average = 2.98e-01\n",
            "  validation accuracy: 87.18 (8718 / 10000), f1 (weighted): 87.09, loss: 3.49e-01\n",
            "  time: 837s (wall 747s)\n",
            "step 41000 / 72000 (epoch 68.33 / 120):\n",
            "  learning_rate = 3.06e-05, loss_average = 2.88e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.22, loss: 3.49e-01\n",
            "  time: 841s (wall 751s)\n",
            "step 41200 / 72000 (epoch 68.67 / 120):\n",
            "  learning_rate = 3.06e-05, loss_average = 2.76e-01\n",
            "  validation accuracy: 87.10 (8710 / 10000), f1 (weighted): 87.03, loss: 3.50e-01\n",
            "  time: 845s (wall 755s)\n",
            "step 41400 / 72000 (epoch 69.00 / 120):\n",
            "  learning_rate = 3.06e-05, loss_average = 2.72e-01\n",
            "  validation accuracy: 87.14 (8714 / 10000), f1 (weighted): 87.12, loss: 3.49e-01\n",
            "  time: 849s (wall 758s)\n",
            "step 41600 / 72000 (epoch 69.33 / 120):\n",
            "  learning_rate = 2.90e-05, loss_average = 2.63e-01\n",
            "  validation accuracy: 87.22 (8722 / 10000), f1 (weighted): 87.14, loss: 3.48e-01\n",
            "  time: 853s (wall 762s)\n",
            "step 41800 / 72000 (epoch 69.67 / 120):\n",
            "  learning_rate = 2.90e-05, loss_average = 2.53e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.22, loss: 3.49e-01\n",
            "  time: 857s (wall 765s)\n",
            "step 42000 / 72000 (epoch 70.00 / 120):\n",
            "  learning_rate = 2.90e-05, loss_average = 2.55e-01\n",
            "  validation accuracy: 87.42 (8742 / 10000), f1 (weighted): 87.40, loss: 3.49e-01\n",
            "  time: 861s (wall 769s)\n",
            "step 42200 / 72000 (epoch 70.33 / 120):\n",
            "  learning_rate = 2.76e-05, loss_average = 2.93e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.20, loss: 3.49e-01\n",
            "  time: 866s (wall 773s)\n",
            "step 42400 / 72000 (epoch 70.67 / 120):\n",
            "  learning_rate = 2.76e-05, loss_average = 2.88e-01\n",
            "  validation accuracy: 87.27 (8727 / 10000), f1 (weighted): 87.22, loss: 3.49e-01\n",
            "  time: 870s (wall 776s)\n",
            "step 42600 / 72000 (epoch 71.00 / 120):\n",
            "  learning_rate = 2.76e-05, loss_average = 2.74e-01\n",
            "  validation accuracy: 87.26 (8726 / 10000), f1 (weighted): 87.21, loss: 3.48e-01\n",
            "  time: 874s (wall 780s)\n",
            "step 42800 / 72000 (epoch 71.33 / 120):\n",
            "  learning_rate = 2.62e-05, loss_average = 2.92e-01\n",
            "  validation accuracy: 87.18 (8718 / 10000), f1 (weighted): 87.16, loss: 3.49e-01\n",
            "  time: 878s (wall 783s)\n",
            "step 43000 / 72000 (epoch 71.67 / 120):\n",
            "  learning_rate = 2.62e-05, loss_average = 2.57e-01\n",
            "  validation accuracy: 87.10 (8710 / 10000), f1 (weighted): 87.05, loss: 3.48e-01\n",
            "  time: 882s (wall 787s)\n",
            "step 43200 / 72000 (epoch 72.00 / 120):\n",
            "  learning_rate = 2.62e-05, loss_average = 2.80e-01\n",
            "  validation accuracy: 87.24 (8724 / 10000), f1 (weighted): 87.14, loss: 3.49e-01\n",
            "  time: 886s (wall 791s)\n",
            "step 43400 / 72000 (epoch 72.33 / 120):\n",
            "  learning_rate = 2.49e-05, loss_average = 2.76e-01\n",
            "  validation accuracy: 87.32 (8732 / 10000), f1 (weighted): 87.25, loss: 3.48e-01\n",
            "  time: 890s (wall 794s)\n",
            "step 43600 / 72000 (epoch 72.67 / 120):\n",
            "  learning_rate = 2.49e-05, loss_average = 2.80e-01\n",
            "  validation accuracy: 87.15 (8715 / 10000), f1 (weighted): 87.09, loss: 3.49e-01\n",
            "  time: 894s (wall 798s)\n",
            "step 43800 / 72000 (epoch 73.00 / 120):\n",
            "  learning_rate = 2.49e-05, loss_average = 2.78e-01\n",
            "  validation accuracy: 87.36 (8736 / 10000), f1 (weighted): 87.29, loss: 3.49e-01\n",
            "  time: 898s (wall 802s)\n",
            "step 44000 / 72000 (epoch 73.33 / 120):\n",
            "  learning_rate = 2.36e-05, loss_average = 2.66e-01\n",
            "  validation accuracy: 87.27 (8727 / 10000), f1 (weighted): 87.25, loss: 3.49e-01\n",
            "  time: 903s (wall 805s)\n",
            "step 44200 / 72000 (epoch 73.67 / 120):\n",
            "  learning_rate = 2.36e-05, loss_average = 2.65e-01\n",
            "  validation accuracy: 87.15 (8715 / 10000), f1 (weighted): 87.11, loss: 3.49e-01\n",
            "  time: 907s (wall 809s)\n",
            "step 44400 / 72000 (epoch 74.00 / 120):\n",
            "  learning_rate = 2.36e-05, loss_average = 2.58e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.26, loss: 3.48e-01\n",
            "  time: 911s (wall 813s)\n",
            "step 44600 / 72000 (epoch 74.33 / 120):\n",
            "  learning_rate = 2.25e-05, loss_average = 2.92e-01\n",
            "  validation accuracy: 87.21 (8721 / 10000), f1 (weighted): 87.19, loss: 3.49e-01\n",
            "  time: 915s (wall 816s)\n",
            "step 44800 / 72000 (epoch 74.67 / 120):\n",
            "  learning_rate = 2.25e-05, loss_average = 2.73e-01\n",
            "  validation accuracy: 87.26 (8726 / 10000), f1 (weighted): 87.19, loss: 3.48e-01\n",
            "  time: 919s (wall 820s)\n",
            "step 45000 / 72000 (epoch 75.00 / 120):\n",
            "  learning_rate = 2.25e-05, loss_average = 2.83e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.30, loss: 3.48e-01\n",
            "  time: 923s (wall 823s)\n",
            "step 45200 / 72000 (epoch 75.33 / 120):\n",
            "  learning_rate = 2.13e-05, loss_average = 2.72e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.25, loss: 3.48e-01\n",
            "  time: 927s (wall 827s)\n",
            "step 45400 / 72000 (epoch 75.67 / 120):\n",
            "  learning_rate = 2.13e-05, loss_average = 3.02e-01\n",
            "  validation accuracy: 87.25 (8725 / 10000), f1 (weighted): 87.22, loss: 3.48e-01\n",
            "  time: 931s (wall 831s)\n",
            "step 45600 / 72000 (epoch 76.00 / 120):\n",
            "  learning_rate = 2.13e-05, loss_average = 2.43e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.23, loss: 3.48e-01\n",
            "  time: 935s (wall 834s)\n",
            "step 45800 / 72000 (epoch 76.33 / 120):\n",
            "  learning_rate = 2.03e-05, loss_average = 2.91e-01\n",
            "  validation accuracy: 87.27 (8727 / 10000), f1 (weighted): 87.27, loss: 3.50e-01\n",
            "  time: 940s (wall 838s)\n",
            "step 46000 / 72000 (epoch 76.67 / 120):\n",
            "  learning_rate = 2.03e-05, loss_average = 2.74e-01\n",
            "  validation accuracy: 87.35 (8735 / 10000), f1 (weighted): 87.28, loss: 3.48e-01\n",
            "  time: 944s (wall 842s)\n",
            "step 46200 / 72000 (epoch 77.00 / 120):\n",
            "  learning_rate = 2.03e-05, loss_average = 2.87e-01\n",
            "  validation accuracy: 87.16 (8716 / 10000), f1 (weighted): 87.12, loss: 3.49e-01\n",
            "  time: 948s (wall 845s)\n",
            "step 46400 / 72000 (epoch 77.33 / 120):\n",
            "  learning_rate = 1.93e-05, loss_average = 3.02e-01\n",
            "  validation accuracy: 87.39 (8739 / 10000), f1 (weighted): 87.36, loss: 3.48e-01\n",
            "  time: 952s (wall 849s)\n",
            "step 46600 / 72000 (epoch 77.67 / 120):\n",
            "  learning_rate = 1.93e-05, loss_average = 2.66e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.24, loss: 3.48e-01\n",
            "  time: 956s (wall 852s)\n",
            "step 46800 / 72000 (epoch 78.00 / 120):\n",
            "  learning_rate = 1.93e-05, loss_average = 2.77e-01\n",
            "  validation accuracy: 87.18 (8718 / 10000), f1 (weighted): 87.14, loss: 3.48e-01\n",
            "  time: 960s (wall 856s)\n",
            "step 47000 / 72000 (epoch 78.33 / 120):\n",
            "  learning_rate = 1.83e-05, loss_average = 2.86e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.26, loss: 3.48e-01\n",
            "  time: 964s (wall 860s)\n",
            "step 47200 / 72000 (epoch 78.67 / 120):\n",
            "  learning_rate = 1.83e-05, loss_average = 2.99e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.28, loss: 3.48e-01\n",
            "  time: 968s (wall 863s)\n",
            "step 47400 / 72000 (epoch 79.00 / 120):\n",
            "  learning_rate = 1.83e-05, loss_average = 2.65e-01\n",
            "  validation accuracy: 87.22 (8722 / 10000), f1 (weighted): 87.17, loss: 3.48e-01\n",
            "  time: 972s (wall 867s)\n",
            "step 47600 / 72000 (epoch 79.33 / 120):\n",
            "  learning_rate = 1.74e-05, loss_average = 2.52e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.24, loss: 3.48e-01\n",
            "  time: 976s (wall 871s)\n",
            "step 47800 / 72000 (epoch 79.67 / 120):\n",
            "  learning_rate = 1.74e-05, loss_average = 2.83e-01\n",
            "  validation accuracy: 87.26 (8726 / 10000), f1 (weighted): 87.20, loss: 3.48e-01\n",
            "  time: 980s (wall 874s)\n",
            "step 48000 / 72000 (epoch 80.00 / 120):\n",
            "  learning_rate = 1.74e-05, loss_average = 2.78e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.20, loss: 3.49e-01\n",
            "  time: 984s (wall 878s)\n",
            "step 48200 / 72000 (epoch 80.33 / 120):\n",
            "  learning_rate = 1.65e-05, loss_average = 2.65e-01\n",
            "  validation accuracy: 87.18 (8718 / 10000), f1 (weighted): 87.11, loss: 3.48e-01\n",
            "  time: 989s (wall 881s)\n",
            "step 48400 / 72000 (epoch 80.67 / 120):\n",
            "  learning_rate = 1.65e-05, loss_average = 2.79e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.29, loss: 3.48e-01\n",
            "  time: 993s (wall 885s)\n",
            "step 48600 / 72000 (epoch 81.00 / 120):\n",
            "  learning_rate = 1.65e-05, loss_average = 2.82e-01\n",
            "  validation accuracy: 87.32 (8732 / 10000), f1 (weighted): 87.30, loss: 3.48e-01\n",
            "  time: 997s (wall 889s)\n",
            "step 48800 / 72000 (epoch 81.33 / 120):\n",
            "  learning_rate = 1.57e-05, loss_average = 2.63e-01\n",
            "  validation accuracy: 87.21 (8721 / 10000), f1 (weighted): 87.13, loss: 3.49e-01\n",
            "  time: 1001s (wall 892s)\n",
            "step 49000 / 72000 (epoch 81.67 / 120):\n",
            "  learning_rate = 1.57e-05, loss_average = 2.89e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.24, loss: 3.48e-01\n",
            "  time: 1005s (wall 896s)\n",
            "step 49200 / 72000 (epoch 82.00 / 120):\n",
            "  learning_rate = 1.57e-05, loss_average = 2.73e-01\n",
            "  validation accuracy: 87.35 (8735 / 10000), f1 (weighted): 87.27, loss: 3.48e-01\n",
            "  time: 1009s (wall 899s)\n",
            "step 49400 / 72000 (epoch 82.33 / 120):\n",
            "  learning_rate = 1.49e-05, loss_average = 2.70e-01\n",
            "  validation accuracy: 87.27 (8727 / 10000), f1 (weighted): 87.23, loss: 3.48e-01\n",
            "  time: 1013s (wall 903s)\n",
            "step 49600 / 72000 (epoch 82.67 / 120):\n",
            "  learning_rate = 1.49e-05, loss_average = 2.56e-01\n",
            "  validation accuracy: 87.36 (8736 / 10000), f1 (weighted): 87.33, loss: 3.48e-01\n",
            "  time: 1017s (wall 907s)\n",
            "step 49800 / 72000 (epoch 83.00 / 120):\n",
            "  learning_rate = 1.49e-05, loss_average = 2.64e-01\n",
            "  validation accuracy: 87.32 (8732 / 10000), f1 (weighted): 87.21, loss: 3.48e-01\n",
            "  time: 1021s (wall 910s)\n",
            "step 50000 / 72000 (epoch 83.33 / 120):\n",
            "  learning_rate = 1.42e-05, loss_average = 2.79e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.22, loss: 3.48e-01\n",
            "  time: 1025s (wall 914s)\n",
            "step 50200 / 72000 (epoch 83.67 / 120):\n",
            "  learning_rate = 1.42e-05, loss_average = 2.76e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.27, loss: 3.48e-01\n",
            "  time: 1030s (wall 918s)\n",
            "step 50400 / 72000 (epoch 84.00 / 120):\n",
            "  learning_rate = 1.42e-05, loss_average = 2.83e-01\n",
            "  validation accuracy: 87.26 (8726 / 10000), f1 (weighted): 87.20, loss: 3.48e-01\n",
            "  time: 1034s (wall 921s)\n",
            "step 50600 / 72000 (epoch 84.33 / 120):\n",
            "  learning_rate = 1.35e-05, loss_average = 2.74e-01\n",
            "  validation accuracy: 87.39 (8739 / 10000), f1 (weighted): 87.37, loss: 3.49e-01\n",
            "  time: 1038s (wall 925s)\n",
            "step 50800 / 72000 (epoch 84.67 / 120):\n",
            "  learning_rate = 1.35e-05, loss_average = 2.82e-01\n",
            "  validation accuracy: 87.38 (8738 / 10000), f1 (weighted): 87.31, loss: 3.48e-01\n",
            "  time: 1042s (wall 928s)\n",
            "step 51000 / 72000 (epoch 85.00 / 120):\n",
            "  learning_rate = 1.35e-05, loss_average = 2.64e-01\n",
            "  validation accuracy: 87.32 (8732 / 10000), f1 (weighted): 87.25, loss: 3.48e-01\n",
            "  time: 1046s (wall 932s)\n",
            "step 51200 / 72000 (epoch 85.33 / 120):\n",
            "  learning_rate = 1.28e-05, loss_average = 2.44e-01\n",
            "  validation accuracy: 87.25 (8725 / 10000), f1 (weighted): 87.18, loss: 3.48e-01\n",
            "  time: 1050s (wall 936s)\n",
            "step 51400 / 72000 (epoch 85.67 / 120):\n",
            "  learning_rate = 1.28e-05, loss_average = 2.67e-01\n",
            "  validation accuracy: 87.21 (8721 / 10000), f1 (weighted): 87.14, loss: 3.48e-01\n",
            "  time: 1054s (wall 939s)\n",
            "step 51600 / 72000 (epoch 86.00 / 120):\n",
            "  learning_rate = 1.28e-05, loss_average = 2.79e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.25, loss: 3.48e-01\n",
            "  time: 1058s (wall 943s)\n",
            "step 51800 / 72000 (epoch 86.33 / 120):\n",
            "  learning_rate = 1.21e-05, loss_average = 2.42e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.25, loss: 3.48e-01\n",
            "  time: 1062s (wall 946s)\n",
            "step 52000 / 72000 (epoch 86.67 / 120):\n",
            "  learning_rate = 1.21e-05, loss_average = 2.68e-01\n",
            "  validation accuracy: 87.35 (8735 / 10000), f1 (weighted): 87.27, loss: 3.48e-01\n",
            "  time: 1066s (wall 950s)\n",
            "step 52200 / 72000 (epoch 87.00 / 120):\n",
            "  learning_rate = 1.21e-05, loss_average = 2.91e-01\n",
            "  validation accuracy: 87.32 (8732 / 10000), f1 (weighted): 87.27, loss: 3.48e-01\n",
            "  time: 1070s (wall 954s)\n",
            "step 52400 / 72000 (epoch 87.33 / 120):\n",
            "  learning_rate = 1.15e-05, loss_average = 2.88e-01\n",
            "  validation accuracy: 87.15 (8715 / 10000), f1 (weighted): 87.09, loss: 3.48e-01\n",
            "  time: 1075s (wall 957s)\n",
            "step 52600 / 72000 (epoch 87.67 / 120):\n",
            "  learning_rate = 1.15e-05, loss_average = 2.76e-01\n",
            "  validation accuracy: 87.29 (8729 / 10000), f1 (weighted): 87.23, loss: 3.48e-01\n",
            "  time: 1079s (wall 961s)\n",
            "step 52800 / 72000 (epoch 88.00 / 120):\n",
            "  learning_rate = 1.15e-05, loss_average = 2.83e-01\n",
            "  validation accuracy: 87.24 (8724 / 10000), f1 (weighted): 87.21, loss: 3.48e-01\n",
            "  time: 1083s (wall 964s)\n",
            "step 53000 / 72000 (epoch 88.33 / 120):\n",
            "  learning_rate = 1.10e-05, loss_average = 2.85e-01\n",
            "  validation accuracy: 87.27 (8727 / 10000), f1 (weighted): 87.21, loss: 3.48e-01\n",
            "  time: 1087s (wall 968s)\n",
            "step 53200 / 72000 (epoch 88.67 / 120):\n",
            "  learning_rate = 1.10e-05, loss_average = 3.00e-01\n",
            "  validation accuracy: 87.35 (8735 / 10000), f1 (weighted): 87.30, loss: 3.48e-01\n",
            "  time: 1091s (wall 972s)\n",
            "step 53400 / 72000 (epoch 89.00 / 120):\n",
            "  learning_rate = 1.10e-05, loss_average = 2.45e-01\n",
            "  validation accuracy: 87.29 (8729 / 10000), f1 (weighted): 87.23, loss: 3.48e-01\n",
            "  time: 1095s (wall 975s)\n",
            "step 53600 / 72000 (epoch 89.33 / 120):\n",
            "  learning_rate = 1.04e-05, loss_average = 2.77e-01\n",
            "  validation accuracy: 87.24 (8724 / 10000), f1 (weighted): 87.20, loss: 3.48e-01\n",
            "  time: 1099s (wall 979s)\n",
            "step 53800 / 72000 (epoch 89.67 / 120):\n",
            "  learning_rate = 1.04e-05, loss_average = 2.45e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.25, loss: 3.48e-01\n",
            "  time: 1103s (wall 982s)\n",
            "step 54000 / 72000 (epoch 90.00 / 120):\n",
            "  learning_rate = 1.04e-05, loss_average = 2.57e-01\n",
            "  validation accuracy: 87.38 (8738 / 10000), f1 (weighted): 87.34, loss: 3.48e-01\n",
            "  time: 1107s (wall 986s)\n",
            "step 54200 / 72000 (epoch 90.33 / 120):\n",
            "  learning_rate = 9.89e-06, loss_average = 2.67e-01\n",
            "  validation accuracy: 87.26 (8726 / 10000), f1 (weighted): 87.22, loss: 3.48e-01\n",
            "  time: 1111s (wall 990s)\n",
            "step 54400 / 72000 (epoch 90.67 / 120):\n",
            "  learning_rate = 9.89e-06, loss_average = 2.86e-01\n",
            "  validation accuracy: 87.32 (8732 / 10000), f1 (weighted): 87.25, loss: 3.48e-01\n",
            "  time: 1115s (wall 993s)\n",
            "step 54600 / 72000 (epoch 91.00 / 120):\n",
            "  learning_rate = 9.89e-06, loss_average = 2.50e-01\n",
            "  validation accuracy: 87.37 (8737 / 10000), f1 (weighted): 87.32, loss: 3.48e-01\n",
            "  time: 1120s (wall 997s)\n",
            "step 54800 / 72000 (epoch 91.33 / 120):\n",
            "  learning_rate = 9.39e-06, loss_average = 3.07e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.22, loss: 3.47e-01\n",
            "  time: 1124s (wall 1001s)\n",
            "step 55000 / 72000 (epoch 91.67 / 120):\n",
            "  learning_rate = 9.39e-06, loss_average = 2.86e-01\n",
            "  validation accuracy: 87.42 (8742 / 10000), f1 (weighted): 87.34, loss: 3.48e-01\n",
            "  time: 1128s (wall 1004s)\n",
            "step 55200 / 72000 (epoch 92.00 / 120):\n",
            "  learning_rate = 9.39e-06, loss_average = 2.67e-01\n",
            "  validation accuracy: 87.21 (8721 / 10000), f1 (weighted): 87.15, loss: 3.48e-01\n",
            "  time: 1132s (wall 1008s)\n",
            "step 55400 / 72000 (epoch 92.33 / 120):\n",
            "  learning_rate = 8.92e-06, loss_average = 3.07e-01\n",
            "  validation accuracy: 87.26 (8726 / 10000), f1 (weighted): 87.21, loss: 3.48e-01\n",
            "  time: 1136s (wall 1011s)\n",
            "step 55600 / 72000 (epoch 92.67 / 120):\n",
            "  learning_rate = 8.92e-06, loss_average = 2.61e-01\n",
            "  validation accuracy: 87.37 (8737 / 10000), f1 (weighted): 87.31, loss: 3.48e-01\n",
            "  time: 1140s (wall 1015s)\n",
            "step 55800 / 72000 (epoch 93.00 / 120):\n",
            "  learning_rate = 8.92e-06, loss_average = 2.46e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.22, loss: 3.48e-01\n",
            "  time: 1144s (wall 1019s)\n",
            "step 56000 / 72000 (epoch 93.33 / 120):\n",
            "  learning_rate = 8.48e-06, loss_average = 2.92e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.24, loss: 3.48e-01\n",
            "  time: 1148s (wall 1022s)\n",
            "step 56200 / 72000 (epoch 93.67 / 120):\n",
            "  learning_rate = 8.48e-06, loss_average = 2.59e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.31, loss: 3.48e-01\n",
            "  time: 1152s (wall 1026s)\n",
            "step 56400 / 72000 (epoch 94.00 / 120):\n",
            "  learning_rate = 8.48e-06, loss_average = 2.55e-01\n",
            "  validation accuracy: 87.41 (8741 / 10000), f1 (weighted): 87.37, loss: 3.48e-01\n",
            "  time: 1156s (wall 1029s)\n",
            "step 56600 / 72000 (epoch 94.33 / 120):\n",
            "  learning_rate = 8.05e-06, loss_average = 2.75e-01\n",
            "  validation accuracy: 87.39 (8739 / 10000), f1 (weighted): 87.32, loss: 3.48e-01\n",
            "  time: 1160s (wall 1033s)\n",
            "step 56800 / 72000 (epoch 94.67 / 120):\n",
            "  learning_rate = 8.05e-06, loss_average = 2.76e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.24, loss: 3.47e-01\n",
            "  time: 1165s (wall 1037s)\n",
            "step 57000 / 72000 (epoch 95.00 / 120):\n",
            "  learning_rate = 8.05e-06, loss_average = 2.69e-01\n",
            "  validation accuracy: 87.28 (8728 / 10000), f1 (weighted): 87.22, loss: 3.48e-01\n",
            "  time: 1169s (wall 1040s)\n",
            "step 57200 / 72000 (epoch 95.33 / 120):\n",
            "  learning_rate = 7.65e-06, loss_average = 2.53e-01\n",
            "  validation accuracy: 87.24 (8724 / 10000), f1 (weighted): 87.18, loss: 3.48e-01\n",
            "  time: 1173s (wall 1044s)\n",
            "step 57400 / 72000 (epoch 95.67 / 120):\n",
            "  learning_rate = 7.65e-06, loss_average = 2.58e-01\n",
            "  validation accuracy: 87.30 (8730 / 10000), f1 (weighted): 87.24, loss: 3.48e-01\n",
            "  time: 1177s (wall 1048s)\n",
            "step 57600 / 72000 (epoch 96.00 / 120):\n",
            "  learning_rate = 7.65e-06, loss_average = 2.60e-01\n",
            "  validation accuracy: 87.25 (8725 / 10000), f1 (weighted): 87.21, loss: 3.48e-01\n",
            "  time: 1181s (wall 1051s)\n",
            "step 57800 / 72000 (epoch 96.33 / 120):\n",
            "  learning_rate = 7.27e-06, loss_average = 2.59e-01\n",
            "  validation accuracy: 87.21 (8721 / 10000), f1 (weighted): 87.18, loss: 3.48e-01\n",
            "  time: 1185s (wall 1055s)\n",
            "step 58000 / 72000 (epoch 96.67 / 120):\n",
            "  learning_rate = 7.27e-06, loss_average = 2.72e-01\n",
            "  validation accuracy: 87.32 (8732 / 10000), f1 (weighted): 87.24, loss: 3.48e-01\n",
            "  time: 1189s (wall 1058s)\n",
            "step 58200 / 72000 (epoch 97.00 / 120):\n",
            "  learning_rate = 7.27e-06, loss_average = 2.51e-01\n",
            "  validation accuracy: 87.29 (8729 / 10000), f1 (weighted): 87.23, loss: 3.48e-01\n",
            "  time: 1193s (wall 1062s)\n",
            "step 58400 / 72000 (epoch 97.33 / 120):\n",
            "  learning_rate = 6.91e-06, loss_average = 2.69e-01\n",
            "  validation accuracy: 87.31 (8731 / 10000), f1 (weighted): 87.26, loss: 3.48e-01\n",
            "  time: 1197s (wall 1066s)\n",
            "step 58600 / 72000 (epoch 97.67 / 120):\n",
            "  learning_rate = 6.91e-06, loss_average = 2.75e-01\n",
            "  validation accuracy: 87.35 (8735 / 10000), f1 (weighted): 87.29, loss: 3.48e-01\n",
            "  time: 1201s (wall 1069s)\n",
            "step 58800 / 72000 (epoch 98.00 / 120):\n",
            "  learning_rate = 6.91e-06, loss_average = 2.85e-01\n",
            "  validation accuracy: 87.27 (8727 / 10000), f1 (weighted): 87.23, loss: 3.48e-01\n",
            "  time: 1206s (wall 1073s)\n",
            "step 59000 / 72000 (epoch 98.33 / 120):\n",
            "  learning_rate = 6.56e-06, loss_average = 2.45e-01\n",
            "  validation accuracy: 87.36 (8736 / 10000), f1 (weighted): 87.30, loss: 3.48e-01\n",
            "  time: 1210s (wall 1076s)\n",
            "step 59200 / 72000 (epoch 98.67 / 120):\n",
            "  learning_rate = 6.56e-06, loss_average = 3.13e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.28, loss: 3.48e-01\n",
            "  time: 1214s (wall 1080s)\n",
            "step 59400 / 72000 (epoch 99.00 / 120):\n",
            "  learning_rate = 6.56e-06, loss_average = 2.84e-01\n",
            "  validation accuracy: 87.27 (8727 / 10000), f1 (weighted): 87.22, loss: 3.48e-01\n",
            "  time: 1218s (wall 1084s)\n",
            "step 59600 / 72000 (epoch 99.33 / 120):\n",
            "  learning_rate = 6.23e-06, loss_average = 2.60e-01\n",
            "  validation accuracy: 87.27 (8727 / 10000), f1 (weighted): 87.22, loss: 3.48e-01\n",
            "  time: 1222s (wall 1087s)\n",
            "step 59800 / 72000 (epoch 99.67 / 120):\n",
            "  learning_rate = 6.23e-06, loss_average = 2.86e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.28, loss: 3.47e-01\n",
            "  time: 1226s (wall 1091s)\n",
            "step 60000 / 72000 (epoch 100.00 / 120):\n",
            "  learning_rate = 6.23e-06, loss_average = 2.85e-01\n",
            "  validation accuracy: 87.34 (8734 / 10000), f1 (weighted): 87.30, loss: 3.48e-01\n",
            "  time: 1230s (wall 1094s)\n",
            "step 60200 / 72000 (epoch 100.33 / 120):\n",
            "  learning_rate = 5.92e-06, loss_average = 2.60e-01\n",
            "  validation accuracy: 87.35 (8735 / 10000), f1 (weighted): 87.31, loss: 3.48e-01\n",
            "  time: 1234s (wall 1098s)\n",
            "step 60400 / 72000 (epoch 100.67 / 120):\n",
            "  learning_rate = 5.92e-06, loss_average = 2.88e-01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po6U2tVXYcj-",
        "colab_type": "code",
        "outputId": "ec43f48c-444d-4f6d-fc22-6d5bd97571ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6677413049340248,\n",
              " 0.5592793336510659,\n",
              " 0.5216266411542893,\n",
              " 0.49911010086536406,\n",
              " 0.46902283251285554,\n",
              " 0.46231012165546415,\n",
              " 0.4494735223054886,\n",
              " 0.45267201513051986,\n",
              " 0.43044895082712176,\n",
              " 0.4466110248863697,\n",
              " 0.4252961525321007,\n",
              " 0.4179118874669075,\n",
              " 0.42758339434862136,\n",
              " 0.3926511287689209,\n",
              " 0.39427189111709593,\n",
              " 0.39514455050230024,\n",
              " 0.3949847684800625,\n",
              " 0.4081530725955963,\n",
              " 0.3833735930919647,\n",
              " 0.3773128740489483,\n",
              " 0.38628690168261526,\n",
              " 0.3719057936966419,\n",
              " 0.37636962831020354,\n",
              " 0.39647179931402204,\n",
              " 0.36696689039468766,\n",
              " 0.3915631654858589,\n",
              " 0.39107815727591516,\n",
              " 0.3962946064770222,\n",
              " 0.3714417789876461,\n",
              " 0.3992862057685852,\n",
              " 0.3542359732091427,\n",
              " 0.36119829714298246,\n",
              " 0.3632568444311619,\n",
              " 0.3562288644909859,\n",
              " 0.3658541126549244,\n",
              " 0.36100170373916624,\n",
              " 0.3559359058737755,\n",
              " 0.35173368602991106,\n",
              " 0.35181689843535424,\n",
              " 0.34434508755803106,\n",
              " 0.3452652661502361,\n",
              " 0.343814787119627,\n",
              " 0.3432680568099022,\n",
              " 0.3416405361890793,\n",
              " 0.3511788584291935,\n",
              " 0.34926760613918306,\n",
              " 0.34264535665512086,\n",
              " 0.35529116064310073,\n",
              " 0.346521292924881,\n",
              " 0.3612274944782257,\n",
              " 0.3424304009974003,\n",
              " 0.34483716532588005,\n",
              " 0.3489234945178032,\n",
              " 0.34214464366436004,\n",
              " 0.3488572981953621,\n",
              " 0.35386348262429235,\n",
              " 0.3473175309598446,\n",
              " 0.33383298680186274,\n",
              " 0.328008231818676,\n",
              " 0.3345072191953659,\n",
              " 0.3389300590753555,\n",
              " 0.35213052704930303,\n",
              " 0.3310342802107334,\n",
              " 0.3387754605710506,\n",
              " 0.3297769249975681,\n",
              " 0.33504108726978304,\n",
              " 0.33675138279795647,\n",
              " 0.32942874908447267,\n",
              " 0.33397351324558255,\n",
              " 0.3269452653825283,\n",
              " 0.3323318935930729,\n",
              " 0.32884614810347557,\n",
              " 0.3312363986670971,\n",
              " 0.3359737578034401,\n",
              " 0.3272377043962479,\n",
              " 0.3248339758813381,\n",
              " 0.3251114931702614,\n",
              " 0.32720946341753004,\n",
              " 0.3250235390663147,\n",
              " 0.3283991239964962,\n",
              " 0.32536417722702027,\n",
              " 0.3239272746443749,\n",
              " 0.32697230264544486,\n",
              " 0.3255325679481029,\n",
              " 0.323710725158453,\n",
              " 0.3250045923888683,\n",
              " 0.3226307274401188,\n",
              " 0.3280275399982929,\n",
              " 0.32431499660015106,\n",
              " 0.3218509757518768,\n",
              " 0.3213166992366314,\n",
              " 0.33103904604911805,\n",
              " 0.32599504113197325,\n",
              " 0.32219584316015243,\n",
              " 0.3247362631559372,\n",
              " 0.3213817098736763,\n",
              " 0.3258545234799385,\n",
              " 0.32480708926916124,\n",
              " 0.3210364206135273,\n",
              " 0.31873338282108304,\n",
              " 0.32391084879636767,\n",
              " 0.3224509280920029,\n",
              " 0.3239316852390766,\n",
              " 0.3287434816360474,\n",
              " 0.3240778587758541,\n",
              " 0.32087339729070663,\n",
              " 0.32397540897130966,\n",
              " 0.3188806876540184,\n",
              " 0.323413522541523,\n",
              " 0.3275664794445038,\n",
              " 0.32545648127794263,\n",
              " 0.31730378195643427,\n",
              " 0.32091725766658785,\n",
              " 0.3272347681224346,\n",
              " 0.3187239845097065,\n",
              " 0.32080851137638094,\n",
              " 0.3194147753715515,\n",
              " 0.32069460034370423,\n",
              " 0.31983705565333365,\n",
              " 0.3173621681332588,\n",
              " 0.31923998817801474,\n",
              " 0.3244585245847702,\n",
              " 0.31723216474056243,\n",
              " 0.31865887746214866,\n",
              " 0.3181255058944225,\n",
              " 0.3157177636027336,\n",
              " 0.32124618381261827,\n",
              " 0.3175565446913242,\n",
              " 0.3204271703958511,\n",
              " 0.3180561648309231,\n",
              " 0.3194453912973404,\n",
              " 0.31957300037145614,\n",
              " 0.32624022021889687,\n",
              " 0.3164959572255611,\n",
              " 0.31864760801196096,\n",
              " 0.3180511365830898,\n",
              " 0.3199095694720745,\n",
              " 0.31696834057569506,\n",
              " 0.32250587522983554,\n",
              " 0.31940741792321203,\n",
              " 0.31833129659295084,\n",
              " 0.3185481508076191,\n",
              " 0.3212520459294319,\n",
              " 0.3194258424639702,\n",
              " 0.31897985622286795,\n",
              " 0.3184378056228161,\n",
              " 0.31941767752170563,\n",
              " 0.3176903769373894,\n",
              " 0.3207012417912483,\n",
              " 0.320326651930809,\n",
              " 0.3195572377741337,\n",
              " 0.3176169210672379,\n",
              " 0.316932772397995,\n",
              " 0.32030768543481825,\n",
              " 0.3179425223171711,\n",
              " 0.32193588197231293,\n",
              " 0.31774224668741224,\n",
              " 0.31794401913881304,\n",
              " 0.3175401674211025,\n",
              " 0.317206159979105,\n",
              " 0.317273835837841,\n",
              " 0.31789703816175463,\n",
              " 0.316767436414957,\n",
              " 0.31557904928922653,\n",
              " 0.31743771463632586,\n",
              " 0.31797428548336026,\n",
              " 0.31812543973326685,\n",
              " 0.31896422669291496,\n",
              " 0.31659582659602165,\n",
              " 0.31606918677687645,\n",
              " 0.31554005086421966,\n",
              " 0.31925874575972557,\n",
              " 0.3171129222214222,\n",
              " 0.31917751938104627,\n",
              " 0.317903523594141,\n",
              " 0.31781838193535805,\n",
              " 0.3180707137286663,\n",
              " 0.3180705125629902,\n",
              " 0.3171991151571274,\n",
              " 0.3186498516798019,\n",
              " 0.32288056805729864,\n",
              " 0.31700526222586634,\n",
              " 0.3157606166601181,\n",
              " 0.31712425097823144,\n",
              " 0.3167171314358711,\n",
              " 0.31758047983050347,\n",
              " 0.31641845852136613,\n",
              " 0.3152483795583248,\n",
              " 0.3168651632964611,\n",
              " 0.3166192039847374,\n",
              " 0.3165085923671722,\n",
              " 0.3180384300649166,\n",
              " 0.317108184248209,\n",
              " 0.3152484187483788,\n",
              " 0.31601644843816756,\n",
              " 0.31572956800460816,\n",
              " 0.3160325174033642,\n",
              " 0.31598536983132364,\n",
              " 0.31819205313920973,\n",
              " 0.3156985571980476,\n",
              " 0.31592940628528593,\n",
              " 0.31584315866231916,\n",
              " 0.3200770451128483,\n",
              " 0.316412698328495,\n",
              " 0.3171404582262039,\n",
              " 0.3164991426467896,\n",
              " 0.3162227369844913,\n",
              " 0.31630935370922086,\n",
              " 0.31560365602374074,\n",
              " 0.31580673322081565,\n",
              " 0.3149034748971462,\n",
              " 0.3166071091592312,\n",
              " 0.31622364550828935,\n",
              " 0.315940580368042,\n",
              " 0.3179753614962101,\n",
              " 0.31655538484454154,\n",
              " 0.31818608045578,\n",
              " 0.31724159345030783,\n",
              " 0.31544607907533645,\n",
              " 0.31633263677358625,\n",
              " 0.3160183252394199,\n",
              " 0.31542314156889917,\n",
              " 0.31744953751564026,\n",
              " 0.3162673057615757,\n",
              " 0.31671360120177267,\n",
              " 0.3160135446488857,\n",
              " 0.3163061860203743,\n",
              " 0.31680514603853227,\n",
              " 0.317674794793129,\n",
              " 0.31612057581543923,\n",
              " 0.3166978149116039,\n",
              " 0.3163696101307869,\n",
              " 0.3167307475209236,\n",
              " 0.3157628445327282,\n",
              " 0.3163168981671333,\n",
              " 0.3162732747197151,\n",
              " 0.3157826691865921,\n",
              " 0.31636147528886793,\n",
              " 0.3167816211283207,\n",
              " 0.3159654924273491,\n",
              " 0.31651472955942156,\n",
              " 0.3172289536893368,\n",
              " 0.31629603534936906,\n",
              " 0.31725443407893184,\n",
              " 0.3162377440929413,\n",
              " 0.3157124584913254,\n",
              " 0.3155784863233566,\n",
              " 0.3158858260512352,\n",
              " 0.316235728263855,\n",
              " 0.3167445023357868,\n",
              " 0.3156630301475525,\n",
              " 0.31544811725616456,\n",
              " 0.3156862008571625,\n",
              " 0.31625246345996855,\n",
              " 0.3155484861135483,\n",
              " 0.3153346920013428,\n",
              " 0.31607952669262884,\n",
              " 0.31579765051603315,\n",
              " 0.3161959607899189,\n",
              " 0.31582213029265405,\n",
              " 0.31746383413672447,\n",
              " 0.31549664452672005,\n",
              " 0.3169053381681442,\n",
              " 0.31571825832128525,\n",
              " 0.31587194427847864,\n",
              " 0.315414113253355,\n",
              " 0.3160060276091099,\n",
              " 0.3156260371208191,\n",
              " 0.3171195834875107,\n",
              " 0.31602740481495856,\n",
              " 0.3156773492693901,\n",
              " 0.3159384998679161,\n",
              " 0.3159009379148483,\n",
              " 0.3156994269788265,\n",
              " 0.31608261555433276,\n",
              " 0.31572348296642305,\n",
              " 0.3161305271089077,\n",
              " 0.3157915785908699,\n",
              " 0.31574600994586943,\n",
              " 0.3164519293606281,\n",
              " 0.3154131127893925,\n",
              " 0.31582660630345344,\n",
              " 0.3155932353436947,\n",
              " 0.3168919529020786,\n",
              " 0.3163174369931221,\n",
              " 0.31567582190036775,\n",
              " 0.315664848536253,\n",
              " 0.3160307517647743,\n",
              " 0.31562866061925887,\n",
              " 0.3159450401365757,\n",
              " 0.315790992975235,\n",
              " 0.31589512079954146,\n",
              " 0.3161246591806412,\n",
              " 0.3160684357583523,\n",
              " 0.31558141246438026,\n",
              " 0.3161477544903755,\n",
              " 0.315816051363945,\n",
              " 0.31609381720423696,\n",
              " 0.31575598642230035,\n",
              " 0.31582351654767993,\n",
              " 0.31622339606285094,\n",
              " 0.31607504799962044,\n",
              " 0.31559774056077006,\n",
              " 0.315769852399826,\n",
              " 0.31578566655516627,\n",
              " 0.3163382011651993,\n",
              " 0.31560839727520945,\n",
              " 0.31544299110770224,\n",
              " 0.3162752646207809,\n",
              " 0.3158289571106434,\n",
              " 0.3158093763887882,\n",
              " 0.31608007282018663,\n",
              " 0.31599058732390406,\n",
              " 0.315603844076395,\n",
              " 0.3158931204676628,\n",
              " 0.315969270914793,\n",
              " 0.3159422156214714,\n",
              " 0.31585192427039144,\n",
              " 0.3158098948001862,\n",
              " 0.3157906509935856,\n",
              " 0.31574087098240855,\n",
              " 0.31603171810507774,\n",
              " 0.31600736364722254,\n",
              " 0.31576501071453095,\n",
              " 0.315739858597517,\n",
              " 0.31587357342243194,\n",
              " 0.3159315471351147,\n",
              " 0.3161323161423206,\n",
              " 0.31557002365589143,\n",
              " 0.315839434415102,\n",
              " 0.3158705852925777,\n",
              " 0.31594301253557205,\n",
              " 0.3156345750391483,\n",
              " 0.31596204206347467,\n",
              " 0.31612220585346223,\n",
              " 0.3159397903084755,\n",
              " 0.3156065425276756,\n",
              " 0.315695920586586,\n",
              " 0.3158353278040886,\n",
              " 0.31599805161356925,\n",
              " 0.3159971599280834,\n",
              " 0.3158991312980652,\n",
              " 0.31597208231687546,\n",
              " 0.31576296672224996,\n",
              " 0.31578290581703183,\n",
              " 0.31595600858330725,\n",
              " 0.31576563611626624,\n",
              " 0.3156756454706192,\n",
              " 0.315660467594862,\n",
              " 0.3158069728314877,\n",
              " 0.3158034789562225,\n",
              " 0.31587908521294594,\n",
              " 0.31599409684538843,\n",
              " 0.31577719151973727,\n",
              " 0.3158045966923237,\n",
              " 0.3159473939239979,\n",
              " 0.3159073616564274,\n",
              " 0.31585307106375693,\n",
              " 0.31560801699757574,\n",
              " 0.31582225516438484]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVRxLW8PRr_1",
        "colab_type": "code",
        "outputId": "38a85b47-8a41-4abe-860c-184ae64666a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "# Ploting code from https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy user Simone\n",
        "plt.plot(train_accuracy[0::3])\n",
        "plt.plot(accuracy[0::3])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(train_loss[0::3])\n",
        "plt.plot(loss[0::3])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb4wxccYCvzHGxJn/D7uEUQk0JU0zAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAhRFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sgqvB8M2ZDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = tf.train.latest_checkpoint(model._get_path('checkpoints'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxKlk6pa2h8K",
        "colab_type": "code",
        "outputId": "2cbe1a26-c8ec-4c11-ab1e-cdf069fd11d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "filename"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/cnn_graph/lib/../checkpoints/demo/model-72000'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSCOx6zf2lwH",
        "colab_type": "code",
        "outputId": "9fbf501a-65c8-4bdb-ee59-d2673ec9c905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "!ls cnn_graph/checkpoints/demo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t\t model-71600.index\n",
            "model-71200.data-00000-of-00001  model-71600.meta\n",
            "model-71200.index\t\t model-71800.data-00000-of-00001\n",
            "model-71200.meta\t\t model-71800.index\n",
            "model-71400.data-00000-of-00001  model-71800.meta\n",
            "model-71400.index\t\t model-72000.data-00000-of-00001\n",
            "model-71400.meta\t\t model-72000.index\n",
            "model-71600.data-00000-of-00001  model-72000.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tkgYrDN5kGZ",
        "colab_type": "code",
        "outputId": "8142b5eb-6cef-4282-b90a-2516eb5d6157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "from google.colab import files\n",
        "#files.download(\"cnn_graph/checkpoints/demo/model-72000.meta\")\n",
        "#files.download(\"cnn_graph/checkpoints/demo/model-72000.index\")\n",
        "files.download(\"cnn_graph/checkpoints/demo/model-72000.data-00000-of-00001\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b12e06de5a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#files.download(\"cnn_graph/checkpoints/demo/model-72000.meta\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#files.download(\"cnn_graph/checkpoints/demo/model-72000.index\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_graph/checkpoints/demo/model-72000.data-00000-of-00001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: NetworkError when attempting to fetch resource."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKhEl7l0FRJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp cnn_graph/checkpoints/demo/model-72000.data-00000-of-00001 ./model-72000.data-00000-of-00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTrHXoJGpW4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create rotated version of datasets\n",
        "rotated_test1 = np.reshape(np.rot90(test_images,1, (1,2)), (10000,28*28))\n",
        "rotated_test2 = np.reshape(np.rot90(test_images,2, (1,2)), (10000,28*28))\n",
        "rotated_test3 = np.reshape(np.rot90(test_images,3, (1,2)), (10000,28*28))\n",
        "#graphs, perm = coarsening.coarsen(A, levels=3, self_connections=False)\n",
        "\n",
        "rotated_test1 = coarsening.perm_data(rotated_test1, perm)\n",
        "rotated_test2 = coarsening.perm_data(rotated_test2, perm)\n",
        "rotated_test3 = coarsening.perm_data(rotated_test3, perm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orfZmcuYpcgw",
        "colab_type": "code",
        "outputId": "8f44df47-0958-46d7-bd90-38dc095fd15e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "# Evaluate on rotated data\n",
        "res = model.evaluate(rotated_test1, test_labels)\n",
        "print(res[0])\n",
        "res = model.evaluate(rotated_test2, test_labels)\n",
        "print(res[0])\n",
        "res = model.evaluate(rotated_test3, test_labels)\n",
        "print(res[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/cnn_graph/lib/../checkpoints/demo/model-72000\n",
            "accuracy: 6.33 (633 / 10000), f1 (weighted): 2.80, loss: 9.14e+00\n",
            "time: 1s (wall 1s)\n",
            "INFO:tensorflow:Restoring parameters from /content/cnn_graph/lib/../checkpoints/demo/model-72000\n",
            "accuracy: 21.63 (2163 / 10000), f1 (weighted): 15.34, loss: 6.66e+00\n",
            "time: 1s (wall 0s)\n",
            "INFO:tensorflow:Restoring parameters from /content/cnn_graph/lib/../checkpoints/demo/model-72000\n",
            "accuracy: 3.83 (383 / 10000), f1 (weighted): 1.42, loss: 1.25e+01\n",
            "time: 0s (wall 0s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP_EzM77DvIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}