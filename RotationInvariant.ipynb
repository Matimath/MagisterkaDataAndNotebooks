{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "py2env",
      "language": "python",
      "name": "py2env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "RotationInvariant.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSl5Xs-48P_8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This code is based on: https://github.com/mdeff/cnn_graph and https://nbviewer.jupyter.org/github/mdeff/cnn_graph/blob/outputs/usage.ipynb\n",
        "\n",
        "Reproducing license notice from https://github.com/mdeff/cnn_graph MIT License\n",
        "\n",
        "Copyright (c) 2016 Michaël Defferrard\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qyzKM5TJw5s",
        "colab_type": "code",
        "outputId": "3c6353d6-3a21-4c80-9847-2b54c8b727ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "source": [
        "!pip install tensorflow-gpu==\"1.14\"\n",
        "!pip install numpy matplotlib\n",
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.17.5)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 66.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.1.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.27.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 70.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (45.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (1.0.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhOJehDUw8EA",
        "colab_type": "code",
        "outputId": "ce04aa15-22fc-456e-b575-35bd78951be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "# Downloading the repo\n",
        "!git clone https://github.com/Matimath/cnn_graph"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cnn_graph'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 610 (delta 15), reused 20 (delta 7), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (610/610), 440.58 KiB | 513.00 KiB/s, done.\n",
            "Resolving deltas: 100% (386/386), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zivtuc9uw4L_",
        "colab_type": "code",
        "outputId": "b154a264-8751-40c6-e7d9-a3b545fbbb98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "# Importing necessary modules from cnn_graph\n",
        "%load_ext autoreload\n",
        "from cnn_graph.lib import models, graph, coarsening, utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "hello world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrz-zfjRw4Md",
        "colab_type": "code",
        "outputId": "3a54c84e-ae6d-470d-83c3-ada398ab2709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "# Load dataset\n",
        "import keras\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 9us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 5s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 2s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJodd-TJw4Ms",
        "colab_type": "code",
        "outputId": "566359f0-58cc-4778-f554-a5ac343493ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create an underlying graph as 28x28 nearest neighbours\n",
        "z = graph.grid(28)\n",
        "dist, idx = graph.distance_sklearn_metrics(z, k=8, metric='euclidean')\n",
        "A = graph.adjacency(dist, idx).astype(np.float32)\n",
        "print('d = |V| = {}, k|V| < |E| = {}'.format(0, A.nnz))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "d = |V| = 0, k|V| < |E| = 6396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXLJu9MHw4M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given grid_size it returns laplacian of 8-NN grid_size x grid_size.\n",
        "def get_laplacian_of_size(grid_size):\n",
        "    z = graph.grid(grid_size)\n",
        "    dist, idx = graph.distance_sklearn_metrics(z, k=8, metric='euclidean')\n",
        "    A = graph.adjacency(dist, idx).astype(np.float32)\n",
        "    return graph.laplacian(A, normalized=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc0oM-oUw4NB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshaping dataset\n",
        "X_train = train_images.reshape((60000, 28*28)) \n",
        "X_test = test_images.reshape((10000, 28*28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIbBfo6Fw4NN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create laplacians for pooled graphs\n",
        "L = [get_laplacian_of_size(s) for s in [28,14,7]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhlxyDP7w4NX",
        "colab_type": "text"
      },
      "source": [
        "# 3 Graph ConvNet\n",
        "\n",
        "Here we apply the graph convolutional neural network to signals lying on graphs. After designing the architecture and setting the hyper-parameters, the model takes as inputs the data matrix $X$, the target $y$ and a list of graph Laplacians $L$, one per coarsening level.\n",
        "\n",
        "The data, architecture and hyper-parameters are absolutely *not engineered to showcase performance*. Its sole purpose is to illustrate usage and functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcOHDxfvw4NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = dict()\n",
        "params['dir_name']       = 'demo'\n",
        "params['num_epochs']     = 120\n",
        "params['batch_size']     = 100\n",
        "params['eval_frequency'] = 200\n",
        "\n",
        "# Building blocks.\n",
        "params['filter']         = 'chebyshev5'\n",
        "params['brelu']          = 'b1relu'\n",
        "params['pool']           = 'apool1'\n",
        "\n",
        "# Number of classes.\n",
        "C = train_labels.max() + 1\n",
        "#assert C == np.unique(y).size\n",
        "n_train = 60000\n",
        "# Architecture.\n",
        "params['F']              = [16,64,64,256]  # Number of graph convolutional filters.\n",
        "params['K']              = [3,3,3,3]  # Polynomial orders.\n",
        "params['p']              = [2,1, 2, 1]    # Pooling sizes.\n",
        "params['M']              = [512, C]  # Output dimensionality of fully connected layers.\n",
        "\n",
        "# Optimization.\n",
        "params['regularization'] = 5e-4\n",
        "params['dropout']        = 1\n",
        "params['learning_rate']  = 1e-3\n",
        "params['decay_rate']     = 0.95\n",
        "params['momentum']       = 0.9\n",
        "params['decay_steps']    = n_train / params['batch_size']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j054MNDqw4Nd",
        "colab_type": "code",
        "outputId": "f0da086e-244d-4d02-deb2-5078b9ea055f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = models.cgcnn(L, **params)\n",
        "accuracy, loss, train_accuracy, train_loss, t_step = model.fit(X_train, train_labels, X_test, test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN architecture\n",
            "  input: M_0 = 784\n",
            "  layer 1: cgconv1\n",
            "    representation: M_0 * F_1 / p_1 = 784 * 16 / 2 = 6272\n",
            "    weights: F_0 * F_1 * K_1 = 1 * 16 * 3 = 48\n",
            "    biases: F_1 = 16\n",
            "  layer 2: cgconv2\n",
            "    representation: M_1 * F_2 / p_2 = 196 * 64 / 1 = 12544\n",
            "    weights: F_1 * F_2 * K_2 = 16 * 64 * 3 = 3072\n",
            "    biases: F_2 = 64\n",
            "  layer 3: cgconv3\n",
            "    representation: M_2 * F_3 / p_3 = 196 * 64 / 2 = 6272\n",
            "    weights: F_2 * F_3 * K_3 = 64 * 64 * 3 = 12288\n",
            "    biases: F_3 = 64\n",
            "  layer 4: cgconv4\n",
            "    representation: M_3 * F_4 / p_4 = 49 * 256 / 1 = 12544\n",
            "    weights: F_3 * F_4 * K_4 = 64 * 256 * 3 = 49152\n",
            "    biases: F_4 = 256\n",
            "  layer 5: fc1\n",
            "    representation: M_5 = 512\n",
            "    weights: M_4 * M_5 = 12544 * 512 = 6422528\n",
            "    biases: M_5 = 512\n",
            "  layer 6: logits (softmax)\n",
            "    representation: M_6 = 10\n",
            "    weights: M_5 * M_6 = 512 * 10 = 5120\n",
            "    biases: M_6 = 10\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:161: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:560: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:486: The name tf.sparse_reorder is deprecated. Please use tf.sparse.reorder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:495: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:279: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:282: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:542: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "hello world\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:576: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:214: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:222: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:241: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:249: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:176: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:177: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_graph/lib/models.py:87: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "step 200 / 72000 (epoch 0.33 / 120):\n",
            "  learning_rate = 1.00e-03, loss_average = 2.31e+00\n",
            "  validation accuracy: 36.26 (3626 / 10000), f1 (weighted): 29.43, loss: 2.30e+00\n",
            "  time: 9s (wall 14s)\n",
            "step 400 / 72000 (epoch 0.67 / 120):\n",
            "  learning_rate = 1.00e-03, loss_average = 1.98e+00\n",
            "  validation accuracy: 46.56 (4656 / 10000), f1 (weighted): 42.59, loss: 1.98e+00\n",
            "  time: 15s (wall 19s)\n",
            "step 600 / 72000 (epoch 1.00 / 120):\n",
            "  learning_rate = 1.00e-03, loss_average = 1.56e+00\n",
            "  validation accuracy: 55.59 (5559 / 10000), f1 (weighted): 53.45, loss: 1.53e+00\n",
            "  time: 20s (wall 25s)\n",
            "step 800 / 72000 (epoch 1.33 / 120):\n",
            "  learning_rate = 9.50e-04, loss_average = 1.39e+00\n",
            "  validation accuracy: 59.75 (5975 / 10000), f1 (weighted): 59.48, loss: 1.35e+00\n",
            "  time: 26s (wall 30s)\n",
            "step 1000 / 72000 (epoch 1.67 / 120):\n",
            "  learning_rate = 9.50e-04, loss_average = 1.27e+00\n",
            "  validation accuracy: 60.66 (6066 / 10000), f1 (weighted): 60.88, loss: 1.30e+00\n",
            "  time: 32s (wall 36s)\n",
            "step 1200 / 72000 (epoch 2.00 / 120):\n",
            "  learning_rate = 9.50e-04, loss_average = 1.19e+00\n",
            "  validation accuracy: 67.05 (6705 / 10000), f1 (weighted): 67.03, loss: 1.18e+00\n",
            "  time: 38s (wall 41s)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "step 1400 / 72000 (epoch 2.33 / 120):\n",
            "  learning_rate = 9.02e-04, loss_average = 1.15e+00\n",
            "  validation accuracy: 68.00 (6800 / 10000), f1 (weighted): 68.33, loss: 1.15e+00\n",
            "  time: 43s (wall 46s)\n",
            "step 1600 / 72000 (epoch 2.67 / 120):\n",
            "  learning_rate = 9.02e-04, loss_average = 1.15e+00\n",
            "  validation accuracy: 68.79 (6879 / 10000), f1 (weighted): 66.97, loss: 1.13e+00\n",
            "  time: 49s (wall 52s)\n",
            "step 1800 / 72000 (epoch 3.00 / 120):\n",
            "  learning_rate = 9.02e-04, loss_average = 1.08e+00\n",
            "  validation accuracy: 67.91 (6791 / 10000), f1 (weighted): 67.71, loss: 1.16e+00\n",
            "  time: 55s (wall 57s)\n",
            "step 2000 / 72000 (epoch 3.33 / 120):\n",
            "  learning_rate = 8.57e-04, loss_average = 1.04e+00\n",
            "  validation accuracy: 71.32 (7132 / 10000), f1 (weighted): 70.94, loss: 1.08e+00\n",
            "  time: 61s (wall 63s)\n",
            "step 2200 / 72000 (epoch 3.67 / 120):\n",
            "  learning_rate = 8.57e-04, loss_average = 1.05e+00\n",
            "  validation accuracy: 71.42 (7142 / 10000), f1 (weighted): 70.07, loss: 1.04e+00\n",
            "  time: 66s (wall 68s)\n",
            "step 2400 / 72000 (epoch 4.00 / 120):\n",
            "  learning_rate = 8.57e-04, loss_average = 1.02e+00\n",
            "  validation accuracy: 72.17 (7217 / 10000), f1 (weighted): 71.92, loss: 1.03e+00\n",
            "  time: 72s (wall 74s)\n",
            "step 2600 / 72000 (epoch 4.33 / 120):\n",
            "  learning_rate = 8.15e-04, loss_average = 1.02e+00\n",
            "  validation accuracy: 74.12 (7412 / 10000), f1 (weighted): 73.69, loss: 9.98e-01\n",
            "  time: 78s (wall 79s)\n",
            "step 2800 / 72000 (epoch 4.67 / 120):\n",
            "  learning_rate = 8.15e-04, loss_average = 9.62e-01\n",
            "  validation accuracy: 73.53 (7353 / 10000), f1 (weighted): 72.95, loss: 9.88e-01\n",
            "  time: 84s (wall 85s)\n",
            "step 3000 / 72000 (epoch 5.00 / 120):\n",
            "  learning_rate = 8.15e-04, loss_average = 9.29e-01\n",
            "  validation accuracy: 74.97 (7497 / 10000), f1 (weighted): 74.33, loss: 9.59e-01\n",
            "  time: 89s (wall 90s)\n",
            "step 3200 / 72000 (epoch 5.33 / 120):\n",
            "  learning_rate = 7.74e-04, loss_average = 9.21e-01\n",
            "  validation accuracy: 75.00 (7500 / 10000), f1 (weighted): 74.75, loss: 9.69e-01\n",
            "  time: 95s (wall 96s)\n",
            "step 3400 / 72000 (epoch 5.67 / 120):\n",
            "  learning_rate = 7.74e-04, loss_average = 9.22e-01\n",
            "  validation accuracy: 77.05 (7705 / 10000), f1 (weighted): 76.44, loss: 9.06e-01\n",
            "  time: 101s (wall 101s)\n",
            "step 3600 / 72000 (epoch 6.00 / 120):\n",
            "  learning_rate = 7.74e-04, loss_average = 9.55e-01\n",
            "  validation accuracy: 75.60 (7560 / 10000), f1 (weighted): 75.46, loss: 9.29e-01\n",
            "  time: 106s (wall 107s)\n",
            "step 3800 / 72000 (epoch 6.33 / 120):\n",
            "  learning_rate = 7.35e-04, loss_average = 8.52e-01\n",
            "  validation accuracy: 78.36 (7836 / 10000), f1 (weighted): 78.31, loss: 8.72e-01\n",
            "  time: 112s (wall 112s)\n",
            "step 4000 / 72000 (epoch 6.67 / 120):\n",
            "  learning_rate = 7.35e-04, loss_average = 8.63e-01\n",
            "  validation accuracy: 78.97 (7897 / 10000), f1 (weighted): 79.14, loss: 8.59e-01\n",
            "  time: 118s (wall 117s)\n",
            "step 4200 / 72000 (epoch 7.00 / 120):\n",
            "  learning_rate = 7.35e-04, loss_average = 8.58e-01\n",
            "  validation accuracy: 77.23 (7723 / 10000), f1 (weighted): 75.69, loss: 8.94e-01\n",
            "  time: 124s (wall 123s)\n",
            "step 4400 / 72000 (epoch 7.33 / 120):\n",
            "  learning_rate = 6.98e-04, loss_average = 8.54e-01\n",
            "  validation accuracy: 77.28 (7728 / 10000), f1 (weighted): 77.48, loss: 8.99e-01\n",
            "  time: 129s (wall 128s)\n",
            "step 4600 / 72000 (epoch 7.67 / 120):\n",
            "  learning_rate = 6.98e-04, loss_average = 8.65e-01\n",
            "  validation accuracy: 79.64 (7964 / 10000), f1 (weighted): 79.66, loss: 8.43e-01\n",
            "  time: 135s (wall 134s)\n",
            "step 4800 / 72000 (epoch 8.00 / 120):\n",
            "  learning_rate = 6.98e-04, loss_average = 8.61e-01\n",
            "  validation accuracy: 77.75 (7775 / 10000), f1 (weighted): 76.90, loss: 8.84e-01\n",
            "  time: 141s (wall 139s)\n",
            "step 5000 / 72000 (epoch 8.33 / 120):\n",
            "  learning_rate = 6.63e-04, loss_average = 8.02e-01\n",
            "  validation accuracy: 78.97 (7897 / 10000), f1 (weighted): 78.49, loss: 8.45e-01\n",
            "  time: 147s (wall 145s)\n",
            "step 5200 / 72000 (epoch 8.67 / 120):\n",
            "  learning_rate = 6.63e-04, loss_average = 8.32e-01\n",
            "  validation accuracy: 79.96 (7996 / 10000), f1 (weighted): 79.57, loss: 8.21e-01\n",
            "  time: 152s (wall 150s)\n",
            "step 5400 / 72000 (epoch 9.00 / 120):\n",
            "  learning_rate = 6.63e-04, loss_average = 7.63e-01\n",
            "  validation accuracy: 78.80 (7880 / 10000), f1 (weighted): 78.50, loss: 8.51e-01\n",
            "  time: 158s (wall 156s)\n",
            "step 5600 / 72000 (epoch 9.33 / 120):\n",
            "  learning_rate = 6.30e-04, loss_average = 7.91e-01\n",
            "  validation accuracy: 79.60 (7960 / 10000), f1 (weighted): 79.16, loss: 8.37e-01\n",
            "  time: 164s (wall 161s)\n",
            "step 5800 / 72000 (epoch 9.67 / 120):\n",
            "  learning_rate = 6.30e-04, loss_average = 7.91e-01\n",
            "  validation accuracy: 80.65 (8065 / 10000), f1 (weighted): 81.13, loss: 8.06e-01\n",
            "  time: 169s (wall 167s)\n",
            "step 6000 / 72000 (epoch 10.00 / 120):\n",
            "  learning_rate = 6.30e-04, loss_average = 7.81e-01\n",
            "  validation accuracy: 80.72 (8072 / 10000), f1 (weighted): 80.68, loss: 8.00e-01\n",
            "  time: 175s (wall 172s)\n",
            "step 6200 / 72000 (epoch 10.33 / 120):\n",
            "  learning_rate = 5.99e-04, loss_average = 7.43e-01\n",
            "  validation accuracy: 80.99 (8099 / 10000), f1 (weighted): 80.68, loss: 7.89e-01\n",
            "  time: 181s (wall 177s)\n",
            "step 6400 / 72000 (epoch 10.67 / 120):\n",
            "  learning_rate = 5.99e-04, loss_average = 7.58e-01\n",
            "  validation accuracy: 81.05 (8105 / 10000), f1 (weighted): 81.12, loss: 7.87e-01\n",
            "  time: 186s (wall 183s)\n",
            "step 6600 / 72000 (epoch 11.00 / 120):\n",
            "  learning_rate = 5.99e-04, loss_average = 7.44e-01\n",
            "  validation accuracy: 82.11 (8211 / 10000), f1 (weighted): 81.77, loss: 7.67e-01\n",
            "  time: 192s (wall 188s)\n",
            "step 6800 / 72000 (epoch 11.33 / 120):\n",
            "  learning_rate = 5.69e-04, loss_average = 7.45e-01\n",
            "  validation accuracy: 81.38 (8138 / 10000), f1 (weighted): 81.59, loss: 7.80e-01\n",
            "  time: 198s (wall 194s)\n",
            "step 7000 / 72000 (epoch 11.67 / 120):\n",
            "  learning_rate = 5.69e-04, loss_average = 7.23e-01\n",
            "  validation accuracy: 81.34 (8134 / 10000), f1 (weighted): 80.92, loss: 7.85e-01\n",
            "  time: 203s (wall 199s)\n",
            "step 7200 / 72000 (epoch 12.00 / 120):\n",
            "  learning_rate = 5.69e-04, loss_average = 7.18e-01\n",
            "  validation accuracy: 81.71 (8171 / 10000), f1 (weighted): 81.75, loss: 7.69e-01\n",
            "  time: 209s (wall 205s)\n",
            "step 7400 / 72000 (epoch 12.33 / 120):\n",
            "  learning_rate = 5.40e-04, loss_average = 7.18e-01\n",
            "  validation accuracy: 82.03 (8203 / 10000), f1 (weighted): 81.77, loss: 7.66e-01\n",
            "  time: 215s (wall 210s)\n",
            "step 7600 / 72000 (epoch 12.67 / 120):\n",
            "  learning_rate = 5.40e-04, loss_average = 7.39e-01\n",
            "  validation accuracy: 82.40 (8240 / 10000), f1 (weighted): 82.50, loss: 7.53e-01\n",
            "  time: 220s (wall 215s)\n",
            "step 7800 / 72000 (epoch 13.00 / 120):\n",
            "  learning_rate = 5.40e-04, loss_average = 7.28e-01\n",
            "  validation accuracy: 81.62 (8162 / 10000), f1 (weighted): 81.26, loss: 7.66e-01\n",
            "  time: 226s (wall 221s)\n",
            "step 8000 / 72000 (epoch 13.33 / 120):\n",
            "  learning_rate = 5.13e-04, loss_average = 7.11e-01\n",
            "  validation accuracy: 82.20 (8220 / 10000), f1 (weighted): 82.29, loss: 7.55e-01\n",
            "  time: 232s (wall 226s)\n",
            "step 8200 / 72000 (epoch 13.67 / 120):\n",
            "  learning_rate = 5.13e-04, loss_average = 7.24e-01\n",
            "  validation accuracy: 81.82 (8182 / 10000), f1 (weighted): 81.81, loss: 7.63e-01\n",
            "  time: 237s (wall 232s)\n",
            "step 8400 / 72000 (epoch 14.00 / 120):\n",
            "  learning_rate = 5.13e-04, loss_average = 7.28e-01\n",
            "  validation accuracy: 82.71 (8271 / 10000), f1 (weighted): 82.51, loss: 7.43e-01\n",
            "  time: 243s (wall 237s)\n",
            "step 8600 / 72000 (epoch 14.33 / 120):\n",
            "  learning_rate = 4.88e-04, loss_average = 6.86e-01\n",
            "  validation accuracy: 83.01 (8301 / 10000), f1 (weighted): 83.15, loss: 7.34e-01\n",
            "  time: 249s (wall 243s)\n",
            "step 8800 / 72000 (epoch 14.67 / 120):\n",
            "  learning_rate = 4.88e-04, loss_average = 7.00e-01\n",
            "  validation accuracy: 81.57 (8157 / 10000), f1 (weighted): 82.05, loss: 7.68e-01\n",
            "  time: 254s (wall 248s)\n",
            "step 9000 / 72000 (epoch 15.00 / 120):\n",
            "  learning_rate = 4.88e-04, loss_average = 7.08e-01\n",
            "  validation accuracy: 82.21 (8221 / 10000), f1 (weighted): 82.18, loss: 7.50e-01\n",
            "  time: 260s (wall 253s)\n",
            "step 9200 / 72000 (epoch 15.33 / 120):\n",
            "  learning_rate = 4.63e-04, loss_average = 7.23e-01\n",
            "  validation accuracy: 83.29 (8329 / 10000), f1 (weighted): 83.33, loss: 7.30e-01\n",
            "  time: 266s (wall 259s)\n",
            "step 9400 / 72000 (epoch 15.67 / 120):\n",
            "  learning_rate = 4.63e-04, loss_average = 7.03e-01\n",
            "  validation accuracy: 83.34 (8334 / 10000), f1 (weighted): 83.54, loss: 7.24e-01\n",
            "  time: 271s (wall 264s)\n",
            "step 9600 / 72000 (epoch 16.00 / 120):\n",
            "  learning_rate = 4.63e-04, loss_average = 6.53e-01\n",
            "  validation accuracy: 83.53 (8353 / 10000), f1 (weighted): 83.49, loss: 7.22e-01\n",
            "  time: 277s (wall 270s)\n",
            "step 9800 / 72000 (epoch 16.33 / 120):\n",
            "  learning_rate = 4.40e-04, loss_average = 6.80e-01\n",
            "  validation accuracy: 82.97 (8297 / 10000), f1 (weighted): 83.02, loss: 7.34e-01\n",
            "  time: 283s (wall 275s)\n",
            "step 10000 / 72000 (epoch 16.67 / 120):\n",
            "  learning_rate = 4.40e-04, loss_average = 6.79e-01\n",
            "  validation accuracy: 82.89 (8289 / 10000), f1 (weighted): 82.65, loss: 7.38e-01\n",
            "  time: 288s (wall 281s)\n",
            "step 10200 / 72000 (epoch 17.00 / 120):\n",
            "  learning_rate = 4.40e-04, loss_average = 7.14e-01\n",
            "  validation accuracy: 82.86 (8286 / 10000), f1 (weighted): 82.52, loss: 7.38e-01\n",
            "  time: 294s (wall 286s)\n",
            "step 10400 / 72000 (epoch 17.33 / 120):\n",
            "  learning_rate = 4.18e-04, loss_average = 6.50e-01\n",
            "  validation accuracy: 83.11 (8311 / 10000), f1 (weighted): 83.35, loss: 7.24e-01\n",
            "  time: 300s (wall 292s)\n",
            "step 10600 / 72000 (epoch 17.67 / 120):\n",
            "  learning_rate = 4.18e-04, loss_average = 6.51e-01\n",
            "  validation accuracy: 82.71 (8271 / 10000), f1 (weighted): 82.78, loss: 7.32e-01\n",
            "  time: 305s (wall 297s)\n",
            "step 10800 / 72000 (epoch 18.00 / 120):\n",
            "  learning_rate = 4.18e-04, loss_average = 6.42e-01\n",
            "  validation accuracy: 82.97 (8297 / 10000), f1 (weighted): 82.98, loss: 7.31e-01\n",
            "  time: 311s (wall 302s)\n",
            "step 11000 / 72000 (epoch 18.33 / 120):\n",
            "  learning_rate = 3.97e-04, loss_average = 6.85e-01\n",
            "  validation accuracy: 82.87 (8287 / 10000), f1 (weighted): 82.28, loss: 7.38e-01\n",
            "  time: 317s (wall 308s)\n",
            "step 11200 / 72000 (epoch 18.67 / 120):\n",
            "  learning_rate = 3.97e-04, loss_average = 6.69e-01\n",
            "  validation accuracy: 84.07 (8407 / 10000), f1 (weighted): 83.88, loss: 7.08e-01\n",
            "  time: 322s (wall 313s)\n",
            "step 11400 / 72000 (epoch 19.00 / 120):\n",
            "  learning_rate = 3.97e-04, loss_average = 6.93e-01\n",
            "  validation accuracy: 82.24 (8224 / 10000), f1 (weighted): 82.46, loss: 7.42e-01\n",
            "  time: 328s (wall 319s)\n",
            "step 11600 / 72000 (epoch 19.33 / 120):\n",
            "  learning_rate = 3.77e-04, loss_average = 6.82e-01\n",
            "  validation accuracy: 83.22 (8322 / 10000), f1 (weighted): 83.22, loss: 7.22e-01\n",
            "  time: 334s (wall 324s)\n",
            "step 11800 / 72000 (epoch 19.67 / 120):\n",
            "  learning_rate = 3.77e-04, loss_average = 6.46e-01\n",
            "  validation accuracy: 82.54 (8254 / 10000), f1 (weighted): 82.37, loss: 7.37e-01\n",
            "  time: 339s (wall 330s)\n",
            "step 12000 / 72000 (epoch 20.00 / 120):\n",
            "  learning_rate = 3.77e-04, loss_average = 6.69e-01\n",
            "  validation accuracy: 83.41 (8341 / 10000), f1 (weighted): 83.57, loss: 7.12e-01\n",
            "  time: 345s (wall 335s)\n",
            "step 12200 / 72000 (epoch 20.33 / 120):\n",
            "  learning_rate = 3.58e-04, loss_average = 6.84e-01\n",
            "  validation accuracy: 83.62 (8362 / 10000), f1 (weighted): 83.71, loss: 7.10e-01\n",
            "  time: 350s (wall 341s)\n",
            "step 12400 / 72000 (epoch 20.67 / 120):\n",
            "  learning_rate = 3.58e-04, loss_average = 6.64e-01\n",
            "  validation accuracy: 82.60 (8260 / 10000), f1 (weighted): 82.31, loss: 7.36e-01\n",
            "  time: 356s (wall 346s)\n",
            "step 12600 / 72000 (epoch 21.00 / 120):\n",
            "  learning_rate = 3.58e-04, loss_average = 6.43e-01\n",
            "  validation accuracy: 84.55 (8455 / 10000), f1 (weighted): 84.50, loss: 6.90e-01\n",
            "  time: 362s (wall 351s)\n",
            "step 12800 / 72000 (epoch 21.33 / 120):\n",
            "  learning_rate = 3.41e-04, loss_average = 6.70e-01\n",
            "  validation accuracy: 82.94 (8294 / 10000), f1 (weighted): 82.90, loss: 7.28e-01\n",
            "  time: 367s (wall 357s)\n",
            "step 13000 / 72000 (epoch 21.67 / 120):\n",
            "  learning_rate = 3.41e-04, loss_average = 6.38e-01\n",
            "  validation accuracy: 83.40 (8340 / 10000), f1 (weighted): 83.06, loss: 7.12e-01\n",
            "  time: 373s (wall 362s)\n",
            "step 13200 / 72000 (epoch 22.00 / 120):\n",
            "  learning_rate = 3.41e-04, loss_average = 6.57e-01\n",
            "  validation accuracy: 83.55 (8355 / 10000), f1 (weighted): 83.48, loss: 7.18e-01\n",
            "  time: 379s (wall 368s)\n",
            "step 13400 / 72000 (epoch 22.33 / 120):\n",
            "  learning_rate = 3.24e-04, loss_average = 6.57e-01\n",
            "  validation accuracy: 84.46 (8446 / 10000), f1 (weighted): 84.29, loss: 6.92e-01\n",
            "  time: 384s (wall 373s)\n",
            "step 13600 / 72000 (epoch 22.67 / 120):\n",
            "  learning_rate = 3.24e-04, loss_average = 6.14e-01\n",
            "  validation accuracy: 84.18 (8418 / 10000), f1 (weighted): 84.05, loss: 7.08e-01\n",
            "  time: 390s (wall 379s)\n",
            "step 13800 / 72000 (epoch 23.00 / 120):\n",
            "  learning_rate = 3.24e-04, loss_average = 6.55e-01\n",
            "  validation accuracy: 83.96 (8396 / 10000), f1 (weighted): 84.01, loss: 6.99e-01\n",
            "  time: 396s (wall 384s)\n",
            "step 14000 / 72000 (epoch 23.33 / 120):\n",
            "  learning_rate = 3.07e-04, loss_average = 6.25e-01\n",
            "  validation accuracy: 84.22 (8422 / 10000), f1 (weighted): 84.23, loss: 6.97e-01\n",
            "  time: 401s (wall 389s)\n",
            "step 14200 / 72000 (epoch 23.67 / 120):\n",
            "  learning_rate = 3.07e-04, loss_average = 6.56e-01\n",
            "  validation accuracy: 84.23 (8423 / 10000), f1 (weighted): 84.19, loss: 6.93e-01\n",
            "  time: 407s (wall 395s)\n",
            "step 14400 / 72000 (epoch 24.00 / 120):\n",
            "  learning_rate = 3.07e-04, loss_average = 6.44e-01\n",
            "  validation accuracy: 84.46 (8446 / 10000), f1 (weighted): 84.46, loss: 6.92e-01\n",
            "  time: 412s (wall 400s)\n",
            "step 14600 / 72000 (epoch 24.33 / 120):\n",
            "  learning_rate = 2.92e-04, loss_average = 6.62e-01\n",
            "  validation accuracy: 84.42 (8442 / 10000), f1 (weighted): 84.53, loss: 6.84e-01\n",
            "  time: 418s (wall 406s)\n",
            "step 14800 / 72000 (epoch 24.67 / 120):\n",
            "  learning_rate = 2.92e-04, loss_average = 6.18e-01\n",
            "  validation accuracy: 84.17 (8417 / 10000), f1 (weighted): 84.32, loss: 6.96e-01\n",
            "  time: 424s (wall 411s)\n",
            "step 15000 / 72000 (epoch 25.00 / 120):\n",
            "  learning_rate = 2.92e-04, loss_average = 6.43e-01\n",
            "  validation accuracy: 84.35 (8435 / 10000), f1 (weighted): 84.21, loss: 6.89e-01\n",
            "  time: 429s (wall 416s)\n",
            "step 15200 / 72000 (epoch 25.33 / 120):\n",
            "  learning_rate = 2.77e-04, loss_average = 6.64e-01\n",
            "  validation accuracy: 84.15 (8415 / 10000), f1 (weighted): 84.00, loss: 6.97e-01\n",
            "  time: 435s (wall 422s)\n",
            "step 15400 / 72000 (epoch 25.67 / 120):\n",
            "  learning_rate = 2.77e-04, loss_average = 6.05e-01\n",
            "  validation accuracy: 84.48 (8448 / 10000), f1 (weighted): 84.60, loss: 6.85e-01\n",
            "  time: 441s (wall 427s)\n",
            "step 15600 / 72000 (epoch 26.00 / 120):\n",
            "  learning_rate = 2.77e-04, loss_average = 6.11e-01\n",
            "  validation accuracy: 84.48 (8448 / 10000), f1 (weighted): 84.18, loss: 6.89e-01\n",
            "  time: 446s (wall 433s)\n",
            "step 15800 / 72000 (epoch 26.33 / 120):\n",
            "  learning_rate = 2.64e-04, loss_average = 6.24e-01\n",
            "  validation accuracy: 84.48 (8448 / 10000), f1 (weighted): 84.53, loss: 6.82e-01\n",
            "  time: 452s (wall 438s)\n",
            "step 16000 / 72000 (epoch 26.67 / 120):\n",
            "  learning_rate = 2.64e-04, loss_average = 6.36e-01\n",
            "  validation accuracy: 84.38 (8438 / 10000), f1 (weighted): 84.29, loss: 6.92e-01\n",
            "  time: 457s (wall 443s)\n",
            "step 16200 / 72000 (epoch 27.00 / 120):\n",
            "  learning_rate = 2.64e-04, loss_average = 6.16e-01\n",
            "  validation accuracy: 83.82 (8382 / 10000), f1 (weighted): 83.98, loss: 7.03e-01\n",
            "  time: 463s (wall 449s)\n",
            "step 16400 / 72000 (epoch 27.33 / 120):\n",
            "  learning_rate = 2.50e-04, loss_average = 6.13e-01\n",
            "  validation accuracy: 84.97 (8497 / 10000), f1 (weighted): 84.89, loss: 6.78e-01\n",
            "  time: 469s (wall 454s)\n",
            "step 16600 / 72000 (epoch 27.67 / 120):\n",
            "  learning_rate = 2.50e-04, loss_average = 6.34e-01\n",
            "  validation accuracy: 84.51 (8451 / 10000), f1 (weighted): 84.12, loss: 6.87e-01\n",
            "  time: 474s (wall 460s)\n",
            "step 16800 / 72000 (epoch 28.00 / 120):\n",
            "  learning_rate = 2.50e-04, loss_average = 6.21e-01\n",
            "  validation accuracy: 84.27 (8427 / 10000), f1 (weighted): 83.91, loss: 6.99e-01\n",
            "  time: 480s (wall 465s)\n",
            "step 17000 / 72000 (epoch 28.33 / 120):\n",
            "  learning_rate = 2.38e-04, loss_average = 6.19e-01\n",
            "  validation accuracy: 84.50 (8450 / 10000), f1 (weighted): 84.54, loss: 6.79e-01\n",
            "  time: 485s (wall 470s)\n",
            "step 17200 / 72000 (epoch 28.67 / 120):\n",
            "  learning_rate = 2.38e-04, loss_average = 6.16e-01\n",
            "  validation accuracy: 84.40 (8440 / 10000), f1 (weighted): 84.58, loss: 6.84e-01\n",
            "  time: 491s (wall 476s)\n",
            "step 17400 / 72000 (epoch 29.00 / 120):\n",
            "  learning_rate = 2.38e-04, loss_average = 5.92e-01\n",
            "  validation accuracy: 84.60 (8460 / 10000), f1 (weighted): 84.51, loss: 6.78e-01\n",
            "  time: 496s (wall 481s)\n",
            "step 17600 / 72000 (epoch 29.33 / 120):\n",
            "  learning_rate = 2.26e-04, loss_average = 6.36e-01\n",
            "  validation accuracy: 84.90 (8490 / 10000), f1 (weighted): 84.78, loss: 6.74e-01\n",
            "  time: 502s (wall 486s)\n",
            "step 17800 / 72000 (epoch 29.67 / 120):\n",
            "  learning_rate = 2.26e-04, loss_average = 6.24e-01\n",
            "  validation accuracy: 84.33 (8433 / 10000), f1 (weighted): 84.38, loss: 6.86e-01\n",
            "  time: 508s (wall 492s)\n",
            "step 18000 / 72000 (epoch 30.00 / 120):\n",
            "  learning_rate = 2.26e-04, loss_average = 6.22e-01\n",
            "  validation accuracy: 84.87 (8487 / 10000), f1 (weighted): 84.74, loss: 6.78e-01\n",
            "  time: 513s (wall 497s)\n",
            "step 18200 / 72000 (epoch 30.33 / 120):\n",
            "  learning_rate = 2.15e-04, loss_average = 6.14e-01\n",
            "  validation accuracy: 85.04 (8504 / 10000), f1 (weighted): 84.93, loss: 6.74e-01\n",
            "  time: 519s (wall 503s)\n",
            "step 18400 / 72000 (epoch 30.67 / 120):\n",
            "  learning_rate = 2.15e-04, loss_average = 5.78e-01\n",
            "  validation accuracy: 84.97 (8497 / 10000), f1 (weighted): 84.77, loss: 6.75e-01\n",
            "  time: 525s (wall 508s)\n",
            "step 18600 / 72000 (epoch 31.00 / 120):\n",
            "  learning_rate = 2.15e-04, loss_average = 5.95e-01\n",
            "  validation accuracy: 85.02 (8502 / 10000), f1 (weighted): 84.82, loss: 6.83e-01\n",
            "  time: 530s (wall 513s)\n",
            "step 18800 / 72000 (epoch 31.33 / 120):\n",
            "  learning_rate = 2.04e-04, loss_average = 6.28e-01\n",
            "  validation accuracy: 84.49 (8449 / 10000), f1 (weighted): 84.27, loss: 6.84e-01\n",
            "  time: 536s (wall 519s)\n",
            "step 19000 / 72000 (epoch 31.67 / 120):\n",
            "  learning_rate = 2.04e-04, loss_average = 5.90e-01\n",
            "  validation accuracy: 84.63 (8463 / 10000), f1 (weighted): 84.52, loss: 6.80e-01\n",
            "  time: 541s (wall 524s)\n",
            "step 19200 / 72000 (epoch 32.00 / 120):\n",
            "  learning_rate = 2.04e-04, loss_average = 6.10e-01\n",
            "  validation accuracy: 84.55 (8455 / 10000), f1 (weighted): 84.74, loss: 6.73e-01\n",
            "  time: 547s (wall 529s)\n",
            "step 19400 / 72000 (epoch 32.33 / 120):\n",
            "  learning_rate = 1.94e-04, loss_average = 5.87e-01\n",
            "  validation accuracy: 84.85 (8485 / 10000), f1 (weighted): 84.95, loss: 6.75e-01\n",
            "  time: 552s (wall 535s)\n",
            "step 19600 / 72000 (epoch 32.67 / 120):\n",
            "  learning_rate = 1.94e-04, loss_average = 5.98e-01\n",
            "  validation accuracy: 84.85 (8485 / 10000), f1 (weighted): 84.97, loss: 6.71e-01\n",
            "  time: 558s (wall 540s)\n",
            "step 19800 / 72000 (epoch 33.00 / 120):\n",
            "  learning_rate = 1.94e-04, loss_average = 6.03e-01\n",
            "  validation accuracy: 84.42 (8442 / 10000), f1 (weighted): 84.56, loss: 6.78e-01\n",
            "  time: 563s (wall 546s)\n",
            "step 20000 / 72000 (epoch 33.33 / 120):\n",
            "  learning_rate = 1.84e-04, loss_average = 6.16e-01\n",
            "  validation accuracy: 84.61 (8461 / 10000), f1 (weighted): 84.73, loss: 6.81e-01\n",
            "  time: 569s (wall 551s)\n",
            "step 20200 / 72000 (epoch 33.67 / 120):\n",
            "  learning_rate = 1.84e-04, loss_average = 5.91e-01\n",
            "  validation accuracy: 84.64 (8464 / 10000), f1 (weighted): 84.54, loss: 6.82e-01\n",
            "  time: 574s (wall 556s)\n",
            "step 20400 / 72000 (epoch 34.00 / 120):\n",
            "  learning_rate = 1.84e-04, loss_average = 6.07e-01\n",
            "  validation accuracy: 85.08 (8508 / 10000), f1 (weighted): 85.11, loss: 6.69e-01\n",
            "  time: 580s (wall 562s)\n",
            "step 20600 / 72000 (epoch 34.33 / 120):\n",
            "  learning_rate = 1.75e-04, loss_average = 6.16e-01\n",
            "  validation accuracy: 84.90 (8490 / 10000), f1 (weighted): 84.94, loss: 6.72e-01\n",
            "  time: 585s (wall 567s)\n",
            "step 20800 / 72000 (epoch 34.67 / 120):\n",
            "  learning_rate = 1.75e-04, loss_average = 6.00e-01\n",
            "  validation accuracy: 85.07 (8507 / 10000), f1 (weighted): 85.09, loss: 6.68e-01\n",
            "  time: 591s (wall 572s)\n",
            "step 21000 / 72000 (epoch 35.00 / 120):\n",
            "  learning_rate = 1.75e-04, loss_average = 5.87e-01\n",
            "  validation accuracy: 85.30 (8530 / 10000), f1 (weighted): 85.37, loss: 6.62e-01\n",
            "  time: 596s (wall 578s)\n",
            "step 21200 / 72000 (epoch 35.33 / 120):\n",
            "  learning_rate = 1.66e-04, loss_average = 5.89e-01\n",
            "  validation accuracy: 85.28 (8528 / 10000), f1 (weighted): 85.23, loss: 6.70e-01\n",
            "  time: 602s (wall 583s)\n",
            "step 21400 / 72000 (epoch 35.67 / 120):\n",
            "  learning_rate = 1.66e-04, loss_average = 6.07e-01\n",
            "  validation accuracy: 84.39 (8439 / 10000), f1 (weighted): 84.06, loss: 6.82e-01\n",
            "  time: 608s (wall 589s)\n",
            "step 21600 / 72000 (epoch 36.00 / 120):\n",
            "  learning_rate = 1.66e-04, loss_average = 6.24e-01\n",
            "  validation accuracy: 84.72 (8472 / 10000), f1 (weighted): 84.72, loss: 6.70e-01\n",
            "  time: 613s (wall 594s)\n",
            "step 21800 / 72000 (epoch 36.33 / 120):\n",
            "  learning_rate = 1.58e-04, loss_average = 5.68e-01\n",
            "  validation accuracy: 85.27 (8527 / 10000), f1 (weighted): 85.11, loss: 6.65e-01\n",
            "  time: 619s (wall 599s)\n",
            "step 22000 / 72000 (epoch 36.67 / 120):\n",
            "  learning_rate = 1.58e-04, loss_average = 5.95e-01\n",
            "  validation accuracy: 84.23 (8423 / 10000), f1 (weighted): 84.07, loss: 6.92e-01\n",
            "  time: 624s (wall 605s)\n",
            "step 22200 / 72000 (epoch 37.00 / 120):\n",
            "  learning_rate = 1.58e-04, loss_average = 5.98e-01\n",
            "  validation accuracy: 85.06 (8506 / 10000), f1 (weighted): 85.02, loss: 6.65e-01\n",
            "  time: 629s (wall 610s)\n",
            "step 22400 / 72000 (epoch 37.33 / 120):\n",
            "  learning_rate = 1.50e-04, loss_average = 5.87e-01\n",
            "  validation accuracy: 85.39 (8539 / 10000), f1 (weighted): 85.32, loss: 6.59e-01\n",
            "  time: 635s (wall 615s)\n",
            "step 22600 / 72000 (epoch 37.67 / 120):\n",
            "  learning_rate = 1.50e-04, loss_average = 5.95e-01\n",
            "  validation accuracy: 85.42 (8542 / 10000), f1 (weighted): 85.25, loss: 6.60e-01\n",
            "  time: 640s (wall 621s)\n",
            "step 22800 / 72000 (epoch 38.00 / 120):\n",
            "  learning_rate = 1.50e-04, loss_average = 5.79e-01\n",
            "  validation accuracy: 85.22 (8522 / 10000), f1 (weighted): 85.13, loss: 6.69e-01\n",
            "  time: 646s (wall 626s)\n",
            "step 23000 / 72000 (epoch 38.33 / 120):\n",
            "  learning_rate = 1.42e-04, loss_average = 5.94e-01\n",
            "  validation accuracy: 85.17 (8517 / 10000), f1 (weighted): 85.06, loss: 6.66e-01\n",
            "  time: 651s (wall 631s)\n",
            "step 23200 / 72000 (epoch 38.67 / 120):\n",
            "  learning_rate = 1.42e-04, loss_average = 5.92e-01\n",
            "  validation accuracy: 85.10 (8510 / 10000), f1 (weighted): 84.99, loss: 6.63e-01\n",
            "  time: 657s (wall 637s)\n",
            "step 23400 / 72000 (epoch 39.00 / 120):\n",
            "  learning_rate = 1.42e-04, loss_average = 5.93e-01\n",
            "  validation accuracy: 85.03 (8503 / 10000), f1 (weighted): 84.70, loss: 6.75e-01\n",
            "  time: 662s (wall 642s)\n",
            "step 23600 / 72000 (epoch 39.33 / 120):\n",
            "  learning_rate = 1.35e-04, loss_average = 5.85e-01\n",
            "  validation accuracy: 85.38 (8538 / 10000), f1 (weighted): 85.27, loss: 6.59e-01\n",
            "  time: 668s (wall 647s)\n",
            "step 23800 / 72000 (epoch 39.67 / 120):\n",
            "  learning_rate = 1.35e-04, loss_average = 6.22e-01\n",
            "  validation accuracy: 85.24 (8524 / 10000), f1 (weighted): 85.05, loss: 6.67e-01\n",
            "  time: 673s (wall 653s)\n",
            "step 24000 / 72000 (epoch 40.00 / 120):\n",
            "  learning_rate = 1.35e-04, loss_average = 5.76e-01\n",
            "  validation accuracy: 85.32 (8532 / 10000), f1 (weighted): 85.34, loss: 6.58e-01\n",
            "  time: 679s (wall 658s)\n",
            "step 24200 / 72000 (epoch 40.33 / 120):\n",
            "  learning_rate = 1.29e-04, loss_average = 5.66e-01\n",
            "  validation accuracy: 85.50 (8550 / 10000), f1 (weighted): 85.34, loss: 6.56e-01\n",
            "  time: 685s (wall 664s)\n",
            "step 24400 / 72000 (epoch 40.67 / 120):\n",
            "  learning_rate = 1.29e-04, loss_average = 5.78e-01\n",
            "  validation accuracy: 85.14 (8514 / 10000), f1 (weighted): 84.94, loss: 6.65e-01\n",
            "  time: 690s (wall 669s)\n",
            "step 24600 / 72000 (epoch 41.00 / 120):\n",
            "  learning_rate = 1.29e-04, loss_average = 5.78e-01\n",
            "  validation accuracy: 85.27 (8527 / 10000), f1 (weighted): 85.38, loss: 6.63e-01\n",
            "  time: 696s (wall 674s)\n",
            "step 24800 / 72000 (epoch 41.33 / 120):\n",
            "  learning_rate = 1.22e-04, loss_average = 5.99e-01\n",
            "  validation accuracy: 85.37 (8537 / 10000), f1 (weighted): 85.39, loss: 6.62e-01\n",
            "  time: 701s (wall 680s)\n",
            "step 25000 / 72000 (epoch 41.67 / 120):\n",
            "  learning_rate = 1.22e-04, loss_average = 5.98e-01\n",
            "  validation accuracy: 85.22 (8522 / 10000), f1 (weighted): 85.04, loss: 6.67e-01\n",
            "  time: 707s (wall 685s)\n",
            "step 25200 / 72000 (epoch 42.00 / 120):\n",
            "  learning_rate = 1.22e-04, loss_average = 5.62e-01\n",
            "  validation accuracy: 85.24 (8524 / 10000), f1 (weighted): 85.21, loss: 6.59e-01\n",
            "  time: 712s (wall 690s)\n",
            "step 25400 / 72000 (epoch 42.33 / 120):\n",
            "  learning_rate = 1.16e-04, loss_average = 6.04e-01\n",
            "  validation accuracy: 85.15 (8515 / 10000), f1 (weighted): 84.95, loss: 6.66e-01\n",
            "  time: 718s (wall 696s)\n",
            "step 25600 / 72000 (epoch 42.67 / 120):\n",
            "  learning_rate = 1.16e-04, loss_average = 5.67e-01\n",
            "  validation accuracy: 85.59 (8559 / 10000), f1 (weighted): 85.54, loss: 6.55e-01\n",
            "  time: 723s (wall 701s)\n",
            "step 25800 / 72000 (epoch 43.00 / 120):\n",
            "  learning_rate = 1.16e-04, loss_average = 5.93e-01\n",
            "  validation accuracy: 84.88 (8488 / 10000), f1 (weighted): 84.78, loss: 6.68e-01\n",
            "  time: 729s (wall 706s)\n",
            "step 26000 / 72000 (epoch 43.33 / 120):\n",
            "  learning_rate = 1.10e-04, loss_average = 5.58e-01\n",
            "  validation accuracy: 85.11 (8511 / 10000), f1 (weighted): 85.06, loss: 6.66e-01\n",
            "  time: 734s (wall 712s)\n",
            "step 26200 / 72000 (epoch 43.67 / 120):\n",
            "  learning_rate = 1.10e-04, loss_average = 5.61e-01\n",
            "  validation accuracy: 85.12 (8512 / 10000), f1 (weighted): 85.10, loss: 6.61e-01\n",
            "  time: 740s (wall 717s)\n",
            "step 26400 / 72000 (epoch 44.00 / 120):\n",
            "  learning_rate = 1.10e-04, loss_average = 5.90e-01\n",
            "  validation accuracy: 85.49 (8549 / 10000), f1 (weighted): 85.45, loss: 6.59e-01\n",
            "  time: 745s (wall 722s)\n",
            "step 26600 / 72000 (epoch 44.33 / 120):\n",
            "  learning_rate = 1.05e-04, loss_average = 6.08e-01\n",
            "  validation accuracy: 85.41 (8541 / 10000), f1 (weighted): 85.35, loss: 6.57e-01\n",
            "  time: 751s (wall 728s)\n",
            "step 26800 / 72000 (epoch 44.67 / 120):\n",
            "  learning_rate = 1.05e-04, loss_average = 5.99e-01\n",
            "  validation accuracy: 85.53 (8553 / 10000), f1 (weighted): 85.50, loss: 6.61e-01\n",
            "  time: 756s (wall 733s)\n",
            "step 27000 / 72000 (epoch 45.00 / 120):\n",
            "  learning_rate = 1.05e-04, loss_average = 5.63e-01\n",
            "  validation accuracy: 85.20 (8520 / 10000), f1 (weighted): 85.15, loss: 6.66e-01\n",
            "  time: 762s (wall 739s)\n",
            "step 27200 / 72000 (epoch 45.33 / 120):\n",
            "  learning_rate = 9.94e-05, loss_average = 5.99e-01\n",
            "  validation accuracy: 85.60 (8560 / 10000), f1 (weighted): 85.58, loss: 6.57e-01\n",
            "  time: 767s (wall 744s)\n",
            "step 27400 / 72000 (epoch 45.67 / 120):\n",
            "  learning_rate = 9.94e-05, loss_average = 5.76e-01\n",
            "  validation accuracy: 85.03 (8503 / 10000), f1 (weighted): 84.96, loss: 6.59e-01\n",
            "  time: 773s (wall 750s)\n",
            "step 27600 / 72000 (epoch 46.00 / 120):\n",
            "  learning_rate = 9.94e-05, loss_average = 6.04e-01\n",
            "  validation accuracy: 85.36 (8536 / 10000), f1 (weighted): 85.33, loss: 6.57e-01\n",
            "  time: 779s (wall 755s)\n",
            "step 27800 / 72000 (epoch 46.33 / 120):\n",
            "  learning_rate = 9.45e-05, loss_average = 5.75e-01\n",
            "  validation accuracy: 85.48 (8548 / 10000), f1 (weighted): 85.45, loss: 6.54e-01\n",
            "  time: 784s (wall 760s)\n",
            "step 28000 / 72000 (epoch 46.67 / 120):\n",
            "  learning_rate = 9.45e-05, loss_average = 5.60e-01\n",
            "  validation accuracy: 85.56 (8556 / 10000), f1 (weighted): 85.50, loss: 6.55e-01\n",
            "  time: 790s (wall 766s)\n",
            "step 28200 / 72000 (epoch 47.00 / 120):\n",
            "  learning_rate = 9.45e-05, loss_average = 5.80e-01\n",
            "  validation accuracy: 85.51 (8551 / 10000), f1 (weighted): 85.43, loss: 6.59e-01\n",
            "  time: 795s (wall 771s)\n",
            "step 28400 / 72000 (epoch 47.33 / 120):\n",
            "  learning_rate = 8.97e-05, loss_average = 5.73e-01\n",
            "  validation accuracy: 85.02 (8502 / 10000), f1 (weighted): 85.00, loss: 6.63e-01\n",
            "  time: 801s (wall 776s)\n",
            "step 28600 / 72000 (epoch 47.67 / 120):\n",
            "  learning_rate = 8.97e-05, loss_average = 5.72e-01\n",
            "  validation accuracy: 85.29 (8529 / 10000), f1 (weighted): 85.33, loss: 6.58e-01\n",
            "  time: 806s (wall 782s)\n",
            "step 28800 / 72000 (epoch 48.00 / 120):\n",
            "  learning_rate = 8.97e-05, loss_average = 5.84e-01\n",
            "  validation accuracy: 85.39 (8539 / 10000), f1 (weighted): 85.27, loss: 6.55e-01\n",
            "  time: 812s (wall 787s)\n",
            "step 29000 / 72000 (epoch 48.33 / 120):\n",
            "  learning_rate = 8.53e-05, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.51 (8551 / 10000), f1 (weighted): 85.52, loss: 6.59e-01\n",
            "  time: 817s (wall 792s)\n",
            "step 29200 / 72000 (epoch 48.67 / 120):\n",
            "  learning_rate = 8.53e-05, loss_average = 5.76e-01\n",
            "  validation accuracy: 85.36 (8536 / 10000), f1 (weighted): 85.38, loss: 6.54e-01\n",
            "  time: 823s (wall 798s)\n",
            "step 29400 / 72000 (epoch 49.00 / 120):\n",
            "  learning_rate = 8.53e-05, loss_average = 5.78e-01\n",
            "  validation accuracy: 85.45 (8545 / 10000), f1 (weighted): 85.46, loss: 6.56e-01\n",
            "  time: 828s (wall 803s)\n",
            "step 29600 / 72000 (epoch 49.33 / 120):\n",
            "  learning_rate = 8.10e-05, loss_average = 5.92e-01\n",
            "  validation accuracy: 85.36 (8536 / 10000), f1 (weighted): 85.30, loss: 6.54e-01\n",
            "  time: 834s (wall 809s)\n",
            "step 29800 / 72000 (epoch 49.67 / 120):\n",
            "  learning_rate = 8.10e-05, loss_average = 5.84e-01\n",
            "  validation accuracy: 85.15 (8515 / 10000), f1 (weighted): 85.24, loss: 6.56e-01\n",
            "  time: 839s (wall 814s)\n",
            "step 30000 / 72000 (epoch 50.00 / 120):\n",
            "  learning_rate = 8.10e-05, loss_average = 5.49e-01\n",
            "  validation accuracy: 85.49 (8549 / 10000), f1 (weighted): 85.16, loss: 6.61e-01\n",
            "  time: 845s (wall 819s)\n",
            "step 30200 / 72000 (epoch 50.33 / 120):\n",
            "  learning_rate = 7.69e-05, loss_average = 5.76e-01\n",
            "  validation accuracy: 85.47 (8547 / 10000), f1 (weighted): 85.60, loss: 6.57e-01\n",
            "  time: 850s (wall 825s)\n",
            "step 30400 / 72000 (epoch 50.67 / 120):\n",
            "  learning_rate = 7.69e-05, loss_average = 5.52e-01\n",
            "  validation accuracy: 85.22 (8522 / 10000), f1 (weighted): 85.16, loss: 6.58e-01\n",
            "  time: 856s (wall 830s)\n",
            "step 30600 / 72000 (epoch 51.00 / 120):\n",
            "  learning_rate = 7.69e-05, loss_average = 5.65e-01\n",
            "  validation accuracy: 85.40 (8540 / 10000), f1 (weighted): 85.39, loss: 6.55e-01\n",
            "  time: 861s (wall 835s)\n",
            "step 30800 / 72000 (epoch 51.33 / 120):\n",
            "  learning_rate = 7.31e-05, loss_average = 5.50e-01\n",
            "  validation accuracy: 85.52 (8552 / 10000), f1 (weighted): 85.42, loss: 6.53e-01\n",
            "  time: 867s (wall 841s)\n",
            "step 31000 / 72000 (epoch 51.67 / 120):\n",
            "  learning_rate = 7.31e-05, loss_average = 5.59e-01\n",
            "  validation accuracy: 85.76 (8576 / 10000), f1 (weighted): 85.66, loss: 6.52e-01\n",
            "  time: 872s (wall 846s)\n",
            "step 31200 / 72000 (epoch 52.00 / 120):\n",
            "  learning_rate = 7.31e-05, loss_average = 5.80e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.61, loss: 6.49e-01\n",
            "  time: 878s (wall 851s)\n",
            "step 31400 / 72000 (epoch 52.33 / 120):\n",
            "  learning_rate = 6.94e-05, loss_average = 5.38e-01\n",
            "  validation accuracy: 85.54 (8554 / 10000), f1 (weighted): 85.47, loss: 6.55e-01\n",
            "  time: 883s (wall 857s)\n",
            "step 31600 / 72000 (epoch 52.67 / 120):\n",
            "  learning_rate = 6.94e-05, loss_average = 5.91e-01\n",
            "  validation accuracy: 85.26 (8526 / 10000), f1 (weighted): 85.44, loss: 6.64e-01\n",
            "  time: 889s (wall 862s)\n",
            "step 31800 / 72000 (epoch 53.00 / 120):\n",
            "  learning_rate = 6.94e-05, loss_average = 5.64e-01\n",
            "  validation accuracy: 85.52 (8552 / 10000), f1 (weighted): 85.54, loss: 6.52e-01\n",
            "  time: 894s (wall 867s)\n",
            "step 32000 / 72000 (epoch 53.33 / 120):\n",
            "  learning_rate = 6.60e-05, loss_average = 5.69e-01\n",
            "  validation accuracy: 85.49 (8549 / 10000), f1 (weighted): 85.49, loss: 6.55e-01\n",
            "  time: 900s (wall 873s)\n",
            "step 32200 / 72000 (epoch 53.67 / 120):\n",
            "  learning_rate = 6.60e-05, loss_average = 5.83e-01\n",
            "  validation accuracy: 85.25 (8525 / 10000), f1 (weighted): 85.26, loss: 6.57e-01\n",
            "  time: 905s (wall 878s)\n",
            "step 32400 / 72000 (epoch 54.00 / 120):\n",
            "  learning_rate = 6.60e-05, loss_average = 5.44e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.51, loss: 6.51e-01\n",
            "  time: 911s (wall 883s)\n",
            "step 32600 / 72000 (epoch 54.33 / 120):\n",
            "  learning_rate = 6.27e-05, loss_average = 5.93e-01\n",
            "  validation accuracy: 85.37 (8537 / 10000), f1 (weighted): 85.22, loss: 6.54e-01\n",
            "  time: 916s (wall 889s)\n",
            "step 32800 / 72000 (epoch 54.67 / 120):\n",
            "  learning_rate = 6.27e-05, loss_average = 5.80e-01\n",
            "  validation accuracy: 85.39 (8539 / 10000), f1 (weighted): 85.12, loss: 6.59e-01\n",
            "  time: 922s (wall 894s)\n",
            "step 33000 / 72000 (epoch 55.00 / 120):\n",
            "  learning_rate = 6.27e-05, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.57, loss: 6.50e-01\n",
            "  time: 927s (wall 900s)\n",
            "step 33200 / 72000 (epoch 55.33 / 120):\n",
            "  learning_rate = 5.95e-05, loss_average = 5.74e-01\n",
            "  validation accuracy: 85.49 (8549 / 10000), f1 (weighted): 85.40, loss: 6.55e-01\n",
            "  time: 933s (wall 905s)\n",
            "step 33400 / 72000 (epoch 55.67 / 120):\n",
            "  learning_rate = 5.95e-05, loss_average = 5.45e-01\n",
            "  validation accuracy: 85.47 (8547 / 10000), f1 (weighted): 85.46, loss: 6.54e-01\n",
            "  time: 938s (wall 910s)\n",
            "step 33600 / 72000 (epoch 56.00 / 120):\n",
            "  learning_rate = 5.95e-05, loss_average = 5.75e-01\n",
            "  validation accuracy: 85.55 (8555 / 10000), f1 (weighted): 85.43, loss: 6.49e-01\n",
            "  time: 944s (wall 916s)\n",
            "step 33800 / 72000 (epoch 56.33 / 120):\n",
            "  learning_rate = 5.66e-05, loss_average = 5.65e-01\n",
            "  validation accuracy: 85.29 (8529 / 10000), f1 (weighted): 85.21, loss: 6.55e-01\n",
            "  time: 949s (wall 921s)\n",
            "step 34000 / 72000 (epoch 56.67 / 120):\n",
            "  learning_rate = 5.66e-05, loss_average = 5.62e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.50, loss: 6.54e-01\n",
            "  time: 955s (wall 926s)\n",
            "step 34200 / 72000 (epoch 57.00 / 120):\n",
            "  learning_rate = 5.66e-05, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.53 (8553 / 10000), f1 (weighted): 85.46, loss: 6.52e-01\n",
            "  time: 960s (wall 931s)\n",
            "step 34400 / 72000 (epoch 57.33 / 120):\n",
            "  learning_rate = 5.37e-05, loss_average = 6.03e-01\n",
            "  validation accuracy: 85.65 (8565 / 10000), f1 (weighted): 85.62, loss: 6.49e-01\n",
            "  time: 966s (wall 937s)\n",
            "step 34600 / 72000 (epoch 57.67 / 120):\n",
            "  learning_rate = 5.37e-05, loss_average = 5.69e-01\n",
            "  validation accuracy: 85.44 (8544 / 10000), f1 (weighted): 85.36, loss: 6.53e-01\n",
            "  time: 971s (wall 942s)\n",
            "step 34800 / 72000 (epoch 58.00 / 120):\n",
            "  learning_rate = 5.37e-05, loss_average = 5.59e-01\n",
            "  validation accuracy: 85.32 (8532 / 10000), f1 (weighted): 85.36, loss: 6.56e-01\n",
            "  time: 977s (wall 948s)\n",
            "step 35000 / 72000 (epoch 58.33 / 120):\n",
            "  learning_rate = 5.10e-05, loss_average = 5.57e-01\n",
            "  validation accuracy: 85.53 (8553 / 10000), f1 (weighted): 85.50, loss: 6.50e-01\n",
            "  time: 982s (wall 953s)\n",
            "step 35200 / 72000 (epoch 58.67 / 120):\n",
            "  learning_rate = 5.10e-05, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.63 (8563 / 10000), f1 (weighted): 85.56, loss: 6.50e-01\n",
            "  time: 988s (wall 958s)\n",
            "step 35400 / 72000 (epoch 59.00 / 120):\n",
            "  learning_rate = 5.10e-05, loss_average = 5.82e-01\n",
            "  validation accuracy: 85.55 (8555 / 10000), f1 (weighted): 85.43, loss: 6.52e-01\n",
            "  time: 993s (wall 964s)\n",
            "step 35600 / 72000 (epoch 59.33 / 120):\n",
            "  learning_rate = 4.85e-05, loss_average = 5.57e-01\n",
            "  validation accuracy: 85.51 (8551 / 10000), f1 (weighted): 85.43, loss: 6.51e-01\n",
            "  time: 999s (wall 969s)\n",
            "step 35800 / 72000 (epoch 59.67 / 120):\n",
            "  learning_rate = 4.85e-05, loss_average = 5.77e-01\n",
            "  validation accuracy: 85.53 (8553 / 10000), f1 (weighted): 85.49, loss: 6.51e-01\n",
            "  time: 1004s (wall 974s)\n",
            "step 36000 / 72000 (epoch 60.00 / 120):\n",
            "  learning_rate = 4.85e-05, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.58 (8558 / 10000), f1 (weighted): 85.46, loss: 6.52e-01\n",
            "  time: 1010s (wall 980s)\n",
            "step 36200 / 72000 (epoch 60.33 / 120):\n",
            "  learning_rate = 4.61e-05, loss_average = 5.88e-01\n",
            "  validation accuracy: 85.62 (8562 / 10000), f1 (weighted): 85.56, loss: 6.50e-01\n",
            "  time: 1015s (wall 985s)\n",
            "step 36400 / 72000 (epoch 60.67 / 120):\n",
            "  learning_rate = 4.61e-05, loss_average = 5.54e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.53, loss: 6.50e-01\n",
            "  time: 1021s (wall 990s)\n",
            "step 36600 / 72000 (epoch 61.00 / 120):\n",
            "  learning_rate = 4.61e-05, loss_average = 5.54e-01\n",
            "  validation accuracy: 85.95 (8595 / 10000), f1 (weighted): 85.79, loss: 6.48e-01\n",
            "  time: 1026s (wall 996s)\n",
            "step 36800 / 72000 (epoch 61.33 / 120):\n",
            "  learning_rate = 4.38e-05, loss_average = 5.51e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.65, loss: 6.50e-01\n",
            "  time: 1032s (wall 1001s)\n",
            "step 37000 / 72000 (epoch 61.67 / 120):\n",
            "  learning_rate = 4.38e-05, loss_average = 5.73e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.74, loss: 6.48e-01\n",
            "  time: 1037s (wall 1006s)\n",
            "step 37200 / 72000 (epoch 62.00 / 120):\n",
            "  learning_rate = 4.38e-05, loss_average = 5.36e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.57, loss: 6.48e-01\n",
            "  time: 1043s (wall 1012s)\n",
            "step 37400 / 72000 (epoch 62.33 / 120):\n",
            "  learning_rate = 4.16e-05, loss_average = 5.32e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.60, loss: 6.49e-01\n",
            "  time: 1048s (wall 1017s)\n",
            "step 37600 / 72000 (epoch 62.67 / 120):\n",
            "  learning_rate = 4.16e-05, loss_average = 5.59e-01\n",
            "  validation accuracy: 85.64 (8564 / 10000), f1 (weighted): 85.52, loss: 6.50e-01\n",
            "  time: 1054s (wall 1022s)\n",
            "step 37800 / 72000 (epoch 63.00 / 120):\n",
            "  learning_rate = 4.16e-05, loss_average = 5.52e-01\n",
            "  validation accuracy: 85.70 (8570 / 10000), f1 (weighted): 85.76, loss: 6.50e-01\n",
            "  time: 1059s (wall 1028s)\n",
            "step 38000 / 72000 (epoch 63.33 / 120):\n",
            "  learning_rate = 3.95e-05, loss_average = 5.54e-01\n",
            "  validation accuracy: 85.45 (8545 / 10000), f1 (weighted): 85.45, loss: 6.52e-01\n",
            "  time: 1064s (wall 1033s)\n",
            "step 38200 / 72000 (epoch 63.67 / 120):\n",
            "  learning_rate = 3.95e-05, loss_average = 5.64e-01\n",
            "  validation accuracy: 85.84 (8584 / 10000), f1 (weighted): 85.81, loss: 6.48e-01\n",
            "  time: 1070s (wall 1038s)\n",
            "step 38400 / 72000 (epoch 64.00 / 120):\n",
            "  learning_rate = 3.95e-05, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.69, loss: 6.50e-01\n",
            "  time: 1076s (wall 1044s)\n",
            "step 38600 / 72000 (epoch 64.33 / 120):\n",
            "  learning_rate = 3.75e-05, loss_average = 5.72e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.65, loss: 6.50e-01\n",
            "  time: 1081s (wall 1049s)\n",
            "step 38800 / 72000 (epoch 64.67 / 120):\n",
            "  learning_rate = 3.75e-05, loss_average = 5.58e-01\n",
            "  validation accuracy: 85.85 (8585 / 10000), f1 (weighted): 85.76, loss: 6.50e-01\n",
            "  time: 1087s (wall 1054s)\n",
            "step 39000 / 72000 (epoch 65.00 / 120):\n",
            "  learning_rate = 3.75e-05, loss_average = 5.67e-01\n",
            "  validation accuracy: 85.56 (8556 / 10000), f1 (weighted): 85.56, loss: 6.50e-01\n",
            "  time: 1092s (wall 1060s)\n",
            "step 39200 / 72000 (epoch 65.33 / 120):\n",
            "  learning_rate = 3.56e-05, loss_average = 5.57e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.61, loss: 6.48e-01\n",
            "  time: 1098s (wall 1065s)\n",
            "step 39400 / 72000 (epoch 65.67 / 120):\n",
            "  learning_rate = 3.56e-05, loss_average = 5.89e-01\n",
            "  validation accuracy: 85.83 (8583 / 10000), f1 (weighted): 85.75, loss: 6.48e-01\n",
            "  time: 1103s (wall 1071s)\n",
            "step 39600 / 72000 (epoch 66.00 / 120):\n",
            "  learning_rate = 3.56e-05, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.67 (8567 / 10000), f1 (weighted): 85.62, loss: 6.50e-01\n",
            "  time: 1109s (wall 1076s)\n",
            "step 39800 / 72000 (epoch 66.33 / 120):\n",
            "  learning_rate = 3.39e-05, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.66 (8566 / 10000), f1 (weighted): 85.55, loss: 6.51e-01\n",
            "  time: 1114s (wall 1081s)\n",
            "step 40000 / 72000 (epoch 66.67 / 120):\n",
            "  learning_rate = 3.39e-05, loss_average = 5.51e-01\n",
            "  validation accuracy: 85.57 (8557 / 10000), f1 (weighted): 85.53, loss: 6.49e-01\n",
            "  time: 1120s (wall 1087s)\n",
            "step 40200 / 72000 (epoch 67.00 / 120):\n",
            "  learning_rate = 3.39e-05, loss_average = 5.44e-01\n",
            "  validation accuracy: 85.61 (8561 / 10000), f1 (weighted): 85.46, loss: 6.49e-01\n",
            "  time: 1125s (wall 1092s)\n",
            "step 40400 / 72000 (epoch 67.33 / 120):\n",
            "  learning_rate = 3.22e-05, loss_average = 5.88e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.68, loss: 6.49e-01\n",
            "  time: 1131s (wall 1097s)\n",
            "step 40600 / 72000 (epoch 67.67 / 120):\n",
            "  learning_rate = 3.22e-05, loss_average = 5.27e-01\n",
            "  validation accuracy: 85.64 (8564 / 10000), f1 (weighted): 85.58, loss: 6.47e-01\n",
            "  time: 1136s (wall 1103s)\n",
            "step 40800 / 72000 (epoch 68.00 / 120):\n",
            "  learning_rate = 3.22e-05, loss_average = 5.22e-01\n",
            "  validation accuracy: 85.64 (8564 / 10000), f1 (weighted): 85.54, loss: 6.50e-01\n",
            "  time: 1142s (wall 1108s)\n",
            "step 41000 / 72000 (epoch 68.33 / 120):\n",
            "  learning_rate = 3.06e-05, loss_average = 5.68e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.66, loss: 6.47e-01\n",
            "  time: 1147s (wall 1113s)\n",
            "step 41200 / 72000 (epoch 68.67 / 120):\n",
            "  learning_rate = 3.06e-05, loss_average = 5.65e-01\n",
            "  validation accuracy: 85.54 (8554 / 10000), f1 (weighted): 85.54, loss: 6.48e-01\n",
            "  time: 1153s (wall 1119s)\n",
            "step 41400 / 72000 (epoch 69.00 / 120):\n",
            "  learning_rate = 3.06e-05, loss_average = 5.81e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.69, loss: 6.46e-01\n",
            "  time: 1158s (wall 1124s)\n",
            "step 41600 / 72000 (epoch 69.33 / 120):\n",
            "  learning_rate = 2.90e-05, loss_average = 5.31e-01\n",
            "  validation accuracy: 85.64 (8564 / 10000), f1 (weighted): 85.56, loss: 6.48e-01\n",
            "  time: 1164s (wall 1129s)\n",
            "step 41800 / 72000 (epoch 69.67 / 120):\n",
            "  learning_rate = 2.90e-05, loss_average = 5.58e-01\n",
            "  validation accuracy: 85.44 (8544 / 10000), f1 (weighted): 85.49, loss: 6.48e-01\n",
            "  time: 1169s (wall 1135s)\n",
            "step 42000 / 72000 (epoch 70.00 / 120):\n",
            "  learning_rate = 2.90e-05, loss_average = 5.57e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.58, loss: 6.48e-01\n",
            "  time: 1175s (wall 1140s)\n",
            "step 42200 / 72000 (epoch 70.33 / 120):\n",
            "  learning_rate = 2.76e-05, loss_average = 5.51e-01\n",
            "  validation accuracy: 85.76 (8576 / 10000), f1 (weighted): 85.69, loss: 6.47e-01\n",
            "  time: 1180s (wall 1145s)\n",
            "step 42400 / 72000 (epoch 70.67 / 120):\n",
            "  learning_rate = 2.76e-05, loss_average = 5.41e-01\n",
            "  validation accuracy: 85.77 (8577 / 10000), f1 (weighted): 85.74, loss: 6.48e-01\n",
            "  time: 1185s (wall 1151s)\n",
            "step 42600 / 72000 (epoch 71.00 / 120):\n",
            "  learning_rate = 2.76e-05, loss_average = 5.76e-01\n",
            "  validation accuracy: 85.52 (8552 / 10000), f1 (weighted): 85.52, loss: 6.47e-01\n",
            "  time: 1191s (wall 1156s)\n",
            "step 42800 / 72000 (epoch 71.33 / 120):\n",
            "  learning_rate = 2.62e-05, loss_average = 5.60e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.67, loss: 6.47e-01\n",
            "  time: 1197s (wall 1161s)\n",
            "step 43000 / 72000 (epoch 71.67 / 120):\n",
            "  learning_rate = 2.62e-05, loss_average = 5.33e-01\n",
            "  validation accuracy: 85.52 (8552 / 10000), f1 (weighted): 85.50, loss: 6.49e-01\n",
            "  time: 1202s (wall 1167s)\n",
            "step 43200 / 72000 (epoch 72.00 / 120):\n",
            "  learning_rate = 2.62e-05, loss_average = 5.30e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.71, loss: 6.46e-01\n",
            "  time: 1207s (wall 1172s)\n",
            "step 43400 / 72000 (epoch 72.33 / 120):\n",
            "  learning_rate = 2.49e-05, loss_average = 5.35e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.77, loss: 6.48e-01\n",
            "  time: 1213s (wall 1177s)\n",
            "step 43600 / 72000 (epoch 72.67 / 120):\n",
            "  learning_rate = 2.49e-05, loss_average = 5.92e-01\n",
            "  validation accuracy: 85.70 (8570 / 10000), f1 (weighted): 85.60, loss: 6.47e-01\n",
            "  time: 1218s (wall 1183s)\n",
            "step 43800 / 72000 (epoch 73.00 / 120):\n",
            "  learning_rate = 2.49e-05, loss_average = 5.52e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.71, loss: 6.47e-01\n",
            "  time: 1224s (wall 1188s)\n",
            "step 44000 / 72000 (epoch 73.33 / 120):\n",
            "  learning_rate = 2.36e-05, loss_average = 5.61e-01\n",
            "  validation accuracy: 85.81 (8581 / 10000), f1 (weighted): 85.78, loss: 6.47e-01\n",
            "  time: 1229s (wall 1193s)\n",
            "step 44200 / 72000 (epoch 73.67 / 120):\n",
            "  learning_rate = 2.36e-05, loss_average = 5.45e-01\n",
            "  validation accuracy: 85.83 (8583 / 10000), f1 (weighted): 85.71, loss: 6.48e-01\n",
            "  time: 1235s (wall 1199s)\n",
            "step 44400 / 72000 (epoch 74.00 / 120):\n",
            "  learning_rate = 2.36e-05, loss_average = 5.54e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.65, loss: 6.46e-01\n",
            "  time: 1240s (wall 1204s)\n",
            "step 44600 / 72000 (epoch 74.33 / 120):\n",
            "  learning_rate = 2.25e-05, loss_average = 5.39e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.61, loss: 6.48e-01\n",
            "  time: 1246s (wall 1210s)\n",
            "step 44800 / 72000 (epoch 74.67 / 120):\n",
            "  learning_rate = 2.25e-05, loss_average = 5.78e-01\n",
            "  validation accuracy: 85.54 (8554 / 10000), f1 (weighted): 85.61, loss: 6.50e-01\n",
            "  time: 1251s (wall 1215s)\n",
            "step 45000 / 72000 (epoch 75.00 / 120):\n",
            "  learning_rate = 2.25e-05, loss_average = 5.37e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.62, loss: 6.47e-01\n",
            "  time: 1257s (wall 1220s)\n",
            "step 45200 / 72000 (epoch 75.33 / 120):\n",
            "  learning_rate = 2.13e-05, loss_average = 5.52e-01\n",
            "  validation accuracy: 85.59 (8559 / 10000), f1 (weighted): 85.58, loss: 6.46e-01\n",
            "  time: 1262s (wall 1226s)\n",
            "step 45400 / 72000 (epoch 75.67 / 120):\n",
            "  learning_rate = 2.13e-05, loss_average = 5.45e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.72, loss: 6.46e-01\n",
            "  time: 1268s (wall 1231s)\n",
            "step 45600 / 72000 (epoch 76.00 / 120):\n",
            "  learning_rate = 2.13e-05, loss_average = 5.49e-01\n",
            "  validation accuracy: 85.53 (8553 / 10000), f1 (weighted): 85.56, loss: 6.47e-01\n",
            "  time: 1274s (wall 1236s)\n",
            "step 45800 / 72000 (epoch 76.33 / 120):\n",
            "  learning_rate = 2.03e-05, loss_average = 5.58e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.68, loss: 6.46e-01\n",
            "  time: 1279s (wall 1242s)\n",
            "step 46000 / 72000 (epoch 76.67 / 120):\n",
            "  learning_rate = 2.03e-05, loss_average = 5.30e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.58, loss: 6.47e-01\n",
            "  time: 1284s (wall 1247s)\n",
            "step 46200 / 72000 (epoch 77.00 / 120):\n",
            "  learning_rate = 2.03e-05, loss_average = 5.48e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.61, loss: 6.45e-01\n",
            "  time: 1290s (wall 1252s)\n",
            "step 46400 / 72000 (epoch 77.33 / 120):\n",
            "  learning_rate = 1.93e-05, loss_average = 5.38e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.62, loss: 6.48e-01\n",
            "  time: 1295s (wall 1258s)\n",
            "step 46600 / 72000 (epoch 77.67 / 120):\n",
            "  learning_rate = 1.93e-05, loss_average = 5.64e-01\n",
            "  validation accuracy: 85.62 (8562 / 10000), f1 (weighted): 85.59, loss: 6.47e-01\n",
            "  time: 1301s (wall 1263s)\n",
            "step 46800 / 72000 (epoch 78.00 / 120):\n",
            "  learning_rate = 1.93e-05, loss_average = 5.32e-01\n",
            "  validation accuracy: 85.86 (8586 / 10000), f1 (weighted): 85.76, loss: 6.46e-01\n",
            "  time: 1306s (wall 1268s)\n",
            "step 47000 / 72000 (epoch 78.33 / 120):\n",
            "  learning_rate = 1.83e-05, loss_average = 5.45e-01\n",
            "  validation accuracy: 85.66 (8566 / 10000), f1 (weighted): 85.64, loss: 6.46e-01\n",
            "  time: 1312s (wall 1274s)\n",
            "step 47200 / 72000 (epoch 78.67 / 120):\n",
            "  learning_rate = 1.83e-05, loss_average = 5.72e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.65, loss: 6.47e-01\n",
            "  time: 1317s (wall 1279s)\n",
            "step 47400 / 72000 (epoch 79.00 / 120):\n",
            "  learning_rate = 1.83e-05, loss_average = 5.78e-01\n",
            "  validation accuracy: 85.49 (8549 / 10000), f1 (weighted): 85.47, loss: 6.47e-01\n",
            "  time: 1323s (wall 1284s)\n",
            "step 47600 / 72000 (epoch 79.33 / 120):\n",
            "  learning_rate = 1.74e-05, loss_average = 5.41e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.74, loss: 6.47e-01\n",
            "  time: 1328s (wall 1290s)\n",
            "step 47800 / 72000 (epoch 79.67 / 120):\n",
            "  learning_rate = 1.74e-05, loss_average = 5.46e-01\n",
            "  validation accuracy: 85.78 (8578 / 10000), f1 (weighted): 85.69, loss: 6.44e-01\n",
            "  time: 1334s (wall 1295s)\n",
            "step 48000 / 72000 (epoch 80.00 / 120):\n",
            "  learning_rate = 1.74e-05, loss_average = 5.64e-01\n",
            "  validation accuracy: 85.52 (8552 / 10000), f1 (weighted): 85.51, loss: 6.47e-01\n",
            "  time: 1339s (wall 1300s)\n",
            "step 48200 / 72000 (epoch 80.33 / 120):\n",
            "  learning_rate = 1.65e-05, loss_average = 5.60e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.61, loss: 6.46e-01\n",
            "  time: 1345s (wall 1306s)\n",
            "step 48400 / 72000 (epoch 80.67 / 120):\n",
            "  learning_rate = 1.65e-05, loss_average = 5.61e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.62, loss: 6.49e-01\n",
            "  time: 1350s (wall 1311s)\n",
            "step 48600 / 72000 (epoch 81.00 / 120):\n",
            "  learning_rate = 1.65e-05, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.77 (8577 / 10000), f1 (weighted): 85.72, loss: 6.46e-01\n",
            "  time: 1356s (wall 1316s)\n",
            "step 48800 / 72000 (epoch 81.33 / 120):\n",
            "  learning_rate = 1.57e-05, loss_average = 5.57e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.60, loss: 6.47e-01\n",
            "  time: 1361s (wall 1322s)\n",
            "step 49000 / 72000 (epoch 81.67 / 120):\n",
            "  learning_rate = 1.57e-05, loss_average = 5.33e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.69, loss: 6.47e-01\n",
            "  time: 1367s (wall 1327s)\n",
            "step 49200 / 72000 (epoch 82.00 / 120):\n",
            "  learning_rate = 1.57e-05, loss_average = 5.66e-01\n",
            "  validation accuracy: 85.62 (8562 / 10000), f1 (weighted): 85.54, loss: 6.46e-01\n",
            "  time: 1372s (wall 1332s)\n",
            "step 49400 / 72000 (epoch 82.33 / 120):\n",
            "  learning_rate = 1.49e-05, loss_average = 5.52e-01\n",
            "  validation accuracy: 85.63 (8563 / 10000), f1 (weighted): 85.57, loss: 6.45e-01\n",
            "  time: 1378s (wall 1338s)\n",
            "step 49600 / 72000 (epoch 82.67 / 120):\n",
            "  learning_rate = 1.49e-05, loss_average = 5.51e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.66, loss: 6.47e-01\n",
            "  time: 1383s (wall 1343s)\n",
            "step 49800 / 72000 (epoch 83.00 / 120):\n",
            "  learning_rate = 1.49e-05, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.67, loss: 6.45e-01\n",
            "  time: 1389s (wall 1349s)\n",
            "step 50000 / 72000 (epoch 83.33 / 120):\n",
            "  learning_rate = 1.42e-05, loss_average = 5.58e-01\n",
            "  validation accuracy: 85.66 (8566 / 10000), f1 (weighted): 85.64, loss: 6.45e-01\n",
            "  time: 1394s (wall 1354s)\n",
            "step 50200 / 72000 (epoch 83.67 / 120):\n",
            "  learning_rate = 1.42e-05, loss_average = 5.22e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.71, loss: 6.46e-01\n",
            "  time: 1400s (wall 1359s)\n",
            "step 50400 / 72000 (epoch 84.00 / 120):\n",
            "  learning_rate = 1.42e-05, loss_average = 5.61e-01\n",
            "  validation accuracy: 85.57 (8557 / 10000), f1 (weighted): 85.53, loss: 6.45e-01\n",
            "  time: 1405s (wall 1365s)\n",
            "step 50600 / 72000 (epoch 84.33 / 120):\n",
            "  learning_rate = 1.35e-05, loss_average = 5.37e-01\n",
            "  validation accuracy: 85.62 (8562 / 10000), f1 (weighted): 85.56, loss: 6.45e-01\n",
            "  time: 1410s (wall 1370s)\n",
            "step 50800 / 72000 (epoch 84.67 / 120):\n",
            "  learning_rate = 1.35e-05, loss_average = 5.55e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.67, loss: 6.45e-01\n",
            "  time: 1416s (wall 1375s)\n",
            "step 51000 / 72000 (epoch 85.00 / 120):\n",
            "  learning_rate = 1.35e-05, loss_average = 5.40e-01\n",
            "  validation accuracy: 85.67 (8567 / 10000), f1 (weighted): 85.62, loss: 6.46e-01\n",
            "  time: 1421s (wall 1381s)\n",
            "step 51200 / 72000 (epoch 85.33 / 120):\n",
            "  learning_rate = 1.28e-05, loss_average = 5.62e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.61, loss: 6.45e-01\n",
            "  time: 1427s (wall 1386s)\n",
            "step 51400 / 72000 (epoch 85.67 / 120):\n",
            "  learning_rate = 1.28e-05, loss_average = 5.42e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.62, loss: 6.45e-01\n",
            "  time: 1433s (wall 1391s)\n",
            "step 51600 / 72000 (epoch 86.00 / 120):\n",
            "  learning_rate = 1.28e-05, loss_average = 5.41e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.64, loss: 6.45e-01\n",
            "  time: 1438s (wall 1397s)\n",
            "step 51800 / 72000 (epoch 86.33 / 120):\n",
            "  learning_rate = 1.21e-05, loss_average = 5.42e-01\n",
            "  validation accuracy: 85.63 (8563 / 10000), f1 (weighted): 85.60, loss: 6.45e-01\n",
            "  time: 1444s (wall 1402s)\n",
            "step 52000 / 72000 (epoch 86.67 / 120):\n",
            "  learning_rate = 1.21e-05, loss_average = 5.25e-01\n",
            "  validation accuracy: 85.76 (8576 / 10000), f1 (weighted): 85.66, loss: 6.46e-01\n",
            "  time: 1449s (wall 1407s)\n",
            "step 52200 / 72000 (epoch 87.00 / 120):\n",
            "  learning_rate = 1.21e-05, loss_average = 5.51e-01\n",
            "  validation accuracy: 85.81 (8581 / 10000), f1 (weighted): 85.74, loss: 6.45e-01\n",
            "  time: 1455s (wall 1413s)\n",
            "step 52400 / 72000 (epoch 87.33 / 120):\n",
            "  learning_rate = 1.15e-05, loss_average = 5.50e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.64, loss: 6.45e-01\n",
            "  time: 1460s (wall 1418s)\n",
            "step 52600 / 72000 (epoch 87.67 / 120):\n",
            "  learning_rate = 1.15e-05, loss_average = 5.51e-01\n",
            "  validation accuracy: 85.67 (8567 / 10000), f1 (weighted): 85.62, loss: 6.45e-01\n",
            "  time: 1465s (wall 1423s)\n",
            "step 52800 / 72000 (epoch 88.00 / 120):\n",
            "  learning_rate = 1.15e-05, loss_average = 5.45e-01\n",
            "  validation accuracy: 85.65 (8565 / 10000), f1 (weighted): 85.53, loss: 6.45e-01\n",
            "  time: 1471s (wall 1429s)\n",
            "step 53000 / 72000 (epoch 88.33 / 120):\n",
            "  learning_rate = 1.10e-05, loss_average = 5.49e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.61, loss: 6.45e-01\n",
            "  time: 1476s (wall 1434s)\n",
            "step 53200 / 72000 (epoch 88.67 / 120):\n",
            "  learning_rate = 1.10e-05, loss_average = 5.37e-01\n",
            "  validation accuracy: 85.65 (8565 / 10000), f1 (weighted): 85.55, loss: 6.46e-01\n",
            "  time: 1482s (wall 1439s)\n",
            "step 53400 / 72000 (epoch 89.00 / 120):\n",
            "  learning_rate = 1.10e-05, loss_average = 5.47e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.64, loss: 6.44e-01\n",
            "  time: 1487s (wall 1445s)\n",
            "step 53600 / 72000 (epoch 89.33 / 120):\n",
            "  learning_rate = 1.04e-05, loss_average = 5.75e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.65, loss: 6.45e-01\n",
            "  time: 1493s (wall 1450s)\n",
            "step 53800 / 72000 (epoch 89.67 / 120):\n",
            "  learning_rate = 1.04e-05, loss_average = 5.47e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.75, loss: 6.45e-01\n",
            "  time: 1499s (wall 1456s)\n",
            "step 54000 / 72000 (epoch 90.00 / 120):\n",
            "  learning_rate = 1.04e-05, loss_average = 5.41e-01\n",
            "  validation accuracy: 85.83 (8583 / 10000), f1 (weighted): 85.78, loss: 6.45e-01\n",
            "  time: 1504s (wall 1461s)\n",
            "step 54200 / 72000 (epoch 90.33 / 120):\n",
            "  learning_rate = 9.89e-06, loss_average = 5.42e-01\n",
            "  validation accuracy: 85.77 (8577 / 10000), f1 (weighted): 85.69, loss: 6.45e-01\n",
            "  time: 1510s (wall 1466s)\n",
            "step 54400 / 72000 (epoch 90.67 / 120):\n",
            "  learning_rate = 9.89e-06, loss_average = 5.65e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.70, loss: 6.45e-01\n",
            "  time: 1515s (wall 1472s)\n",
            "step 54600 / 72000 (epoch 91.00 / 120):\n",
            "  learning_rate = 9.89e-06, loss_average = 5.90e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.63, loss: 6.46e-01\n",
            "  time: 1521s (wall 1477s)\n",
            "step 54800 / 72000 (epoch 91.33 / 120):\n",
            "  learning_rate = 9.39e-06, loss_average = 5.66e-01\n",
            "  validation accuracy: 85.62 (8562 / 10000), f1 (weighted): 85.56, loss: 6.45e-01\n",
            "  time: 1527s (wall 1483s)\n",
            "step 55000 / 72000 (epoch 91.67 / 120):\n",
            "  learning_rate = 9.39e-06, loss_average = 5.25e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.72, loss: 6.45e-01\n",
            "  time: 1532s (wall 1488s)\n",
            "step 55200 / 72000 (epoch 92.00 / 120):\n",
            "  learning_rate = 9.39e-06, loss_average = 5.53e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.74, loss: 6.45e-01\n",
            "  time: 1538s (wall 1493s)\n",
            "step 55400 / 72000 (epoch 92.33 / 120):\n",
            "  learning_rate = 8.92e-06, loss_average = 5.22e-01\n",
            "  validation accuracy: 85.77 (8577 / 10000), f1 (weighted): 85.68, loss: 6.45e-01\n",
            "  time: 1543s (wall 1499s)\n",
            "step 55600 / 72000 (epoch 92.67 / 120):\n",
            "  learning_rate = 8.92e-06, loss_average = 5.44e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.66, loss: 6.45e-01\n",
            "  time: 1549s (wall 1504s)\n",
            "step 55800 / 72000 (epoch 93.00 / 120):\n",
            "  learning_rate = 8.92e-06, loss_average = 5.48e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.61, loss: 6.46e-01\n",
            "  time: 1554s (wall 1510s)\n",
            "step 56000 / 72000 (epoch 93.33 / 120):\n",
            "  learning_rate = 8.48e-06, loss_average = 5.71e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.63, loss: 6.44e-01\n",
            "  time: 1560s (wall 1515s)\n",
            "step 56200 / 72000 (epoch 93.67 / 120):\n",
            "  learning_rate = 8.48e-06, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.82 (8582 / 10000), f1 (weighted): 85.81, loss: 6.45e-01\n",
            "  time: 1566s (wall 1520s)\n",
            "step 56400 / 72000 (epoch 94.00 / 120):\n",
            "  learning_rate = 8.48e-06, loss_average = 5.69e-01\n",
            "  validation accuracy: 85.57 (8557 / 10000), f1 (weighted): 85.50, loss: 6.45e-01\n",
            "  time: 1571s (wall 1526s)\n",
            "step 56600 / 72000 (epoch 94.33 / 120):\n",
            "  learning_rate = 8.05e-06, loss_average = 5.40e-01\n",
            "  validation accuracy: 85.82 (8582 / 10000), f1 (weighted): 85.72, loss: 6.45e-01\n",
            "  time: 1577s (wall 1531s)\n",
            "step 56800 / 72000 (epoch 94.67 / 120):\n",
            "  learning_rate = 8.05e-06, loss_average = 5.80e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.62, loss: 6.45e-01\n",
            "  time: 1583s (wall 1537s)\n",
            "step 57000 / 72000 (epoch 95.00 / 120):\n",
            "  learning_rate = 8.05e-06, loss_average = 5.49e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.64, loss: 6.45e-01\n",
            "  time: 1588s (wall 1542s)\n",
            "step 57200 / 72000 (epoch 95.33 / 120):\n",
            "  learning_rate = 7.65e-06, loss_average = 5.73e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.62, loss: 6.45e-01\n",
            "  time: 1594s (wall 1548s)\n",
            "step 57400 / 72000 (epoch 95.67 / 120):\n",
            "  learning_rate = 7.65e-06, loss_average = 5.62e-01\n",
            "  validation accuracy: 85.81 (8581 / 10000), f1 (weighted): 85.72, loss: 6.44e-01\n",
            "  time: 1600s (wall 1553s)\n",
            "step 57600 / 72000 (epoch 96.00 / 120):\n",
            "  learning_rate = 7.65e-06, loss_average = 5.39e-01\n",
            "  validation accuracy: 85.81 (8581 / 10000), f1 (weighted): 85.74, loss: 6.45e-01\n",
            "  time: 1605s (wall 1558s)\n",
            "step 57800 / 72000 (epoch 96.33 / 120):\n",
            "  learning_rate = 7.27e-06, loss_average = 5.75e-01\n",
            "  validation accuracy: 85.83 (8583 / 10000), f1 (weighted): 85.76, loss: 6.44e-01\n",
            "  time: 1611s (wall 1564s)\n",
            "step 58000 / 72000 (epoch 96.67 / 120):\n",
            "  learning_rate = 7.27e-06, loss_average = 5.58e-01\n",
            "  validation accuracy: 85.81 (8581 / 10000), f1 (weighted): 85.70, loss: 6.45e-01\n",
            "  time: 1616s (wall 1569s)\n",
            "step 58200 / 72000 (epoch 97.00 / 120):\n",
            "  learning_rate = 7.27e-06, loss_average = 5.55e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.63, loss: 6.45e-01\n",
            "  time: 1622s (wall 1575s)\n",
            "step 58400 / 72000 (epoch 97.33 / 120):\n",
            "  learning_rate = 6.91e-06, loss_average = 5.59e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.74, loss: 6.45e-01\n",
            "  time: 1628s (wall 1580s)\n",
            "step 58600 / 72000 (epoch 97.67 / 120):\n",
            "  learning_rate = 6.91e-06, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.65, loss: 6.45e-01\n",
            "  time: 1633s (wall 1586s)\n",
            "step 58800 / 72000 (epoch 98.00 / 120):\n",
            "  learning_rate = 6.91e-06, loss_average = 5.52e-01\n",
            "  validation accuracy: 85.67 (8567 / 10000), f1 (weighted): 85.63, loss: 6.44e-01\n",
            "  time: 1639s (wall 1591s)\n",
            "step 59000 / 72000 (epoch 98.33 / 120):\n",
            "  learning_rate = 6.56e-06, loss_average = 5.42e-01\n",
            "  validation accuracy: 85.63 (8563 / 10000), f1 (weighted): 85.59, loss: 6.45e-01\n",
            "  time: 1645s (wall 1596s)\n",
            "step 59200 / 72000 (epoch 98.67 / 120):\n",
            "  learning_rate = 6.56e-06, loss_average = 5.53e-01\n",
            "  validation accuracy: 85.81 (8581 / 10000), f1 (weighted): 85.73, loss: 6.45e-01\n",
            "  time: 1650s (wall 1602s)\n",
            "step 59400 / 72000 (epoch 99.00 / 120):\n",
            "  learning_rate = 6.56e-06, loss_average = 5.58e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.71, loss: 6.44e-01\n",
            "  time: 1656s (wall 1607s)\n",
            "step 59600 / 72000 (epoch 99.33 / 120):\n",
            "  learning_rate = 6.23e-06, loss_average = 5.23e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.62, loss: 6.45e-01\n",
            "  time: 1661s (wall 1613s)\n",
            "step 59800 / 72000 (epoch 99.67 / 120):\n",
            "  learning_rate = 6.23e-06, loss_average = 5.74e-01\n",
            "  validation accuracy: 85.66 (8566 / 10000), f1 (weighted): 85.64, loss: 6.45e-01\n",
            "  time: 1667s (wall 1618s)\n",
            "step 60000 / 72000 (epoch 100.00 / 120):\n",
            "  learning_rate = 6.23e-06, loss_average = 5.55e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.65, loss: 6.44e-01\n",
            "  time: 1673s (wall 1623s)\n",
            "step 60200 / 72000 (epoch 100.33 / 120):\n",
            "  learning_rate = 5.92e-06, loss_average = 5.66e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.72, loss: 6.45e-01\n",
            "  time: 1678s (wall 1629s)\n",
            "step 60400 / 72000 (epoch 100.67 / 120):\n",
            "  learning_rate = 5.92e-06, loss_average = 5.68e-01\n",
            "  validation accuracy: 85.67 (8567 / 10000), f1 (weighted): 85.62, loss: 6.45e-01\n",
            "  time: 1684s (wall 1634s)\n",
            "step 60600 / 72000 (epoch 101.00 / 120):\n",
            "  learning_rate = 5.92e-06, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.78 (8578 / 10000), f1 (weighted): 85.68, loss: 6.45e-01\n",
            "  time: 1689s (wall 1640s)\n",
            "step 60800 / 72000 (epoch 101.33 / 120):\n",
            "  learning_rate = 5.62e-06, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.63 (8563 / 10000), f1 (weighted): 85.60, loss: 6.45e-01\n",
            "  time: 1695s (wall 1645s)\n",
            "step 61000 / 72000 (epoch 101.67 / 120):\n",
            "  learning_rate = 5.62e-06, loss_average = 5.75e-01\n",
            "  validation accuracy: 85.76 (8576 / 10000), f1 (weighted): 85.70, loss: 6.45e-01\n",
            "  time: 1700s (wall 1650s)\n",
            "step 61200 / 72000 (epoch 102.00 / 120):\n",
            "  learning_rate = 5.62e-06, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.81 (8581 / 10000), f1 (weighted): 85.72, loss: 6.45e-01\n",
            "  time: 1706s (wall 1656s)\n",
            "step 61400 / 72000 (epoch 102.33 / 120):\n",
            "  learning_rate = 5.34e-06, loss_average = 5.60e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.67, loss: 6.44e-01\n",
            "  time: 1712s (wall 1661s)\n",
            "step 61600 / 72000 (epoch 102.67 / 120):\n",
            "  learning_rate = 5.34e-06, loss_average = 5.48e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.66, loss: 6.44e-01\n",
            "  time: 1717s (wall 1667s)\n",
            "step 61800 / 72000 (epoch 103.00 / 120):\n",
            "  learning_rate = 5.34e-06, loss_average = 5.35e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.70, loss: 6.45e-01\n",
            "  time: 1723s (wall 1672s)\n",
            "step 62000 / 72000 (epoch 103.33 / 120):\n",
            "  learning_rate = 5.08e-06, loss_average = 5.53e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.67, loss: 6.44e-01\n",
            "  time: 1728s (wall 1677s)\n",
            "step 62200 / 72000 (epoch 103.67 / 120):\n",
            "  learning_rate = 5.08e-06, loss_average = 5.48e-01\n",
            "  validation accuracy: 85.76 (8576 / 10000), f1 (weighted): 85.72, loss: 6.45e-01\n",
            "  time: 1734s (wall 1683s)\n",
            "step 62400 / 72000 (epoch 104.00 / 120):\n",
            "  learning_rate = 5.08e-06, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.76 (8576 / 10000), f1 (weighted): 85.69, loss: 6.44e-01\n",
            "  time: 1740s (wall 1688s)\n",
            "step 62600 / 72000 (epoch 104.33 / 120):\n",
            "  learning_rate = 4.82e-06, loss_average = 5.78e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.65, loss: 6.44e-01\n",
            "  time: 1745s (wall 1694s)\n",
            "step 62800 / 72000 (epoch 104.67 / 120):\n",
            "  learning_rate = 4.82e-06, loss_average = 5.11e-01\n",
            "  validation accuracy: 85.78 (8578 / 10000), f1 (weighted): 85.72, loss: 6.45e-01\n",
            "  time: 1751s (wall 1699s)\n",
            "step 63000 / 72000 (epoch 105.00 / 120):\n",
            "  learning_rate = 4.82e-06, loss_average = 5.50e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.64, loss: 6.45e-01\n",
            "  time: 1756s (wall 1705s)\n",
            "step 63200 / 72000 (epoch 105.33 / 120):\n",
            "  learning_rate = 4.58e-06, loss_average = 5.81e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.65, loss: 6.45e-01\n",
            "  time: 1762s (wall 1710s)\n",
            "step 63400 / 72000 (epoch 105.67 / 120):\n",
            "  learning_rate = 4.58e-06, loss_average = 5.55e-01\n",
            "  validation accuracy: 85.84 (8584 / 10000), f1 (weighted): 85.77, loss: 6.44e-01\n",
            "  time: 1768s (wall 1715s)\n",
            "step 63600 / 72000 (epoch 106.00 / 120):\n",
            "  learning_rate = 4.58e-06, loss_average = 5.84e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.63, loss: 6.44e-01\n",
            "  time: 1773s (wall 1721s)\n",
            "step 63800 / 72000 (epoch 106.33 / 120):\n",
            "  learning_rate = 4.35e-06, loss_average = 5.61e-01\n",
            "  validation accuracy: 85.70 (8570 / 10000), f1 (weighted): 85.60, loss: 6.45e-01\n",
            "  time: 1779s (wall 1726s)\n",
            "step 64000 / 72000 (epoch 106.67 / 120):\n",
            "  learning_rate = 4.35e-06, loss_average = 5.52e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.72, loss: 6.44e-01\n",
            "  time: 1785s (wall 1732s)\n",
            "step 64200 / 72000 (epoch 107.00 / 120):\n",
            "  learning_rate = 4.35e-06, loss_average = 5.39e-01\n",
            "  validation accuracy: 85.82 (8582 / 10000), f1 (weighted): 85.77, loss: 6.44e-01\n",
            "  time: 1790s (wall 1737s)\n",
            "step 64400 / 72000 (epoch 107.33 / 120):\n",
            "  learning_rate = 4.13e-06, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.67, loss: 6.44e-01\n",
            "  time: 1796s (wall 1742s)\n",
            "step 64600 / 72000 (epoch 107.67 / 120):\n",
            "  learning_rate = 4.13e-06, loss_average = 5.49e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.66, loss: 6.44e-01\n",
            "  time: 1801s (wall 1748s)\n",
            "step 64800 / 72000 (epoch 108.00 / 120):\n",
            "  learning_rate = 4.13e-06, loss_average = 5.59e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.67, loss: 6.45e-01\n",
            "  time: 1807s (wall 1753s)\n",
            "step 65000 / 72000 (epoch 108.33 / 120):\n",
            "  learning_rate = 3.93e-06, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.61, loss: 6.44e-01\n",
            "  time: 1812s (wall 1759s)\n",
            "step 65200 / 72000 (epoch 108.67 / 120):\n",
            "  learning_rate = 3.93e-06, loss_average = 5.35e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.65, loss: 6.44e-01\n",
            "  time: 1818s (wall 1764s)\n",
            "step 65400 / 72000 (epoch 109.00 / 120):\n",
            "  learning_rate = 3.93e-06, loss_average = 5.50e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.71, loss: 6.44e-01\n",
            "  time: 1824s (wall 1769s)\n",
            "step 65600 / 72000 (epoch 109.33 / 120):\n",
            "  learning_rate = 3.73e-06, loss_average = 5.80e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.68, loss: 6.44e-01\n",
            "  time: 1829s (wall 1775s)\n",
            "step 65800 / 72000 (epoch 109.67 / 120):\n",
            "  learning_rate = 3.73e-06, loss_average = 5.49e-01\n",
            "  validation accuracy: 85.64 (8564 / 10000), f1 (weighted): 85.58, loss: 6.45e-01\n",
            "  time: 1835s (wall 1780s)\n",
            "step 66000 / 72000 (epoch 110.00 / 120):\n",
            "  learning_rate = 3.73e-06, loss_average = 5.49e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.69, loss: 6.45e-01\n",
            "  time: 1840s (wall 1785s)\n",
            "step 66200 / 72000 (epoch 110.33 / 120):\n",
            "  learning_rate = 3.54e-06, loss_average = 5.76e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.67, loss: 6.45e-01\n",
            "  time: 1846s (wall 1791s)\n",
            "step 66400 / 72000 (epoch 110.67 / 120):\n",
            "  learning_rate = 3.54e-06, loss_average = 5.56e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.62, loss: 6.44e-01\n",
            "  time: 1852s (wall 1796s)\n",
            "step 66600 / 72000 (epoch 111.00 / 120):\n",
            "  learning_rate = 3.54e-06, loss_average = 5.36e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.74, loss: 6.44e-01\n",
            "  time: 1857s (wall 1802s)\n",
            "step 66800 / 72000 (epoch 111.33 / 120):\n",
            "  learning_rate = 3.37e-06, loss_average = 5.48e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.69, loss: 6.44e-01\n",
            "  time: 1863s (wall 1807s)\n",
            "step 67000 / 72000 (epoch 111.67 / 120):\n",
            "  learning_rate = 3.37e-06, loss_average = 5.36e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.63, loss: 6.44e-01\n",
            "  time: 1868s (wall 1813s)\n",
            "step 67200 / 72000 (epoch 112.00 / 120):\n",
            "  learning_rate = 3.37e-06, loss_average = 5.34e-01\n",
            "  validation accuracy: 85.79 (8579 / 10000), f1 (weighted): 85.72, loss: 6.44e-01\n",
            "  time: 1874s (wall 1818s)\n",
            "step 67400 / 72000 (epoch 112.33 / 120):\n",
            "  learning_rate = 3.20e-06, loss_average = 5.44e-01\n",
            "  validation accuracy: 85.77 (8577 / 10000), f1 (weighted): 85.70, loss: 6.44e-01\n",
            "  time: 1880s (wall 1823s)\n",
            "step 67600 / 72000 (epoch 112.67 / 120):\n",
            "  learning_rate = 3.20e-06, loss_average = 5.38e-01\n",
            "  validation accuracy: 85.70 (8570 / 10000), f1 (weighted): 85.65, loss: 6.44e-01\n",
            "  time: 1885s (wall 1829s)\n",
            "step 67800 / 72000 (epoch 113.00 / 120):\n",
            "  learning_rate = 3.20e-06, loss_average = 5.36e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.67, loss: 6.44e-01\n",
            "  time: 1891s (wall 1834s)\n",
            "step 68000 / 72000 (epoch 113.33 / 120):\n",
            "  learning_rate = 3.04e-06, loss_average = 5.53e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.63, loss: 6.44e-01\n",
            "  time: 1896s (wall 1839s)\n",
            "step 68200 / 72000 (epoch 113.67 / 120):\n",
            "  learning_rate = 3.04e-06, loss_average = 5.15e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.64, loss: 6.44e-01\n",
            "  time: 1902s (wall 1845s)\n",
            "step 68400 / 72000 (epoch 114.00 / 120):\n",
            "  learning_rate = 3.04e-06, loss_average = 5.60e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.69, loss: 6.44e-01\n",
            "  time: 1907s (wall 1850s)\n",
            "step 68600 / 72000 (epoch 114.33 / 120):\n",
            "  learning_rate = 2.89e-06, loss_average = 5.86e-01\n",
            "  validation accuracy: 85.71 (8571 / 10000), f1 (weighted): 85.65, loss: 6.44e-01\n",
            "  time: 1913s (wall 1856s)\n",
            "step 68800 / 72000 (epoch 114.67 / 120):\n",
            "  learning_rate = 2.89e-06, loss_average = 5.47e-01\n",
            "  validation accuracy: 85.76 (8576 / 10000), f1 (weighted): 85.68, loss: 6.44e-01\n",
            "  time: 1919s (wall 1861s)\n",
            "step 69000 / 72000 (epoch 115.00 / 120):\n",
            "  learning_rate = 2.89e-06, loss_average = 5.28e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.67, loss: 6.44e-01\n",
            "  time: 1924s (wall 1866s)\n",
            "step 69200 / 72000 (epoch 115.33 / 120):\n",
            "  learning_rate = 2.74e-06, loss_average = 5.31e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.72, loss: 6.44e-01\n",
            "  time: 1930s (wall 1872s)\n",
            "step 69400 / 72000 (epoch 115.67 / 120):\n",
            "  learning_rate = 2.74e-06, loss_average = 5.60e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.67, loss: 6.44e-01\n",
            "  time: 1935s (wall 1877s)\n",
            "step 69600 / 72000 (epoch 116.00 / 120):\n",
            "  learning_rate = 2.74e-06, loss_average = 5.30e-01\n",
            "  validation accuracy: 85.77 (8577 / 10000), f1 (weighted): 85.72, loss: 6.44e-01\n",
            "  time: 1941s (wall 1883s)\n",
            "step 69800 / 72000 (epoch 116.33 / 120):\n",
            "  learning_rate = 2.61e-06, loss_average = 5.24e-01\n",
            "  validation accuracy: 85.69 (8569 / 10000), f1 (weighted): 85.62, loss: 6.44e-01\n",
            "  time: 1946s (wall 1888s)\n",
            "step 70000 / 72000 (epoch 116.67 / 120):\n",
            "  learning_rate = 2.61e-06, loss_average = 5.59e-01\n",
            "  validation accuracy: 85.73 (8573 / 10000), f1 (weighted): 85.67, loss: 6.45e-01\n",
            "  time: 1952s (wall 1893s)\n",
            "step 70200 / 72000 (epoch 117.00 / 120):\n",
            "  learning_rate = 2.61e-06, loss_average = 5.75e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.69, loss: 6.44e-01\n",
            "  time: 1957s (wall 1899s)\n",
            "step 70400 / 72000 (epoch 117.33 / 120):\n",
            "  learning_rate = 2.48e-06, loss_average = 5.36e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.68, loss: 6.44e-01\n",
            "  time: 1963s (wall 1904s)\n",
            "step 70600 / 72000 (epoch 117.67 / 120):\n",
            "  learning_rate = 2.48e-06, loss_average = 5.62e-01\n",
            "  validation accuracy: 85.80 (8580 / 10000), f1 (weighted): 85.75, loss: 6.44e-01\n",
            "  time: 1969s (wall 1910s)\n",
            "step 70800 / 72000 (epoch 118.00 / 120):\n",
            "  learning_rate = 2.48e-06, loss_average = 5.71e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.67, loss: 6.44e-01\n",
            "  time: 1974s (wall 1915s)\n",
            "step 71000 / 72000 (epoch 118.33 / 120):\n",
            "  learning_rate = 2.35e-06, loss_average = 5.55e-01\n",
            "  validation accuracy: 85.70 (8570 / 10000), f1 (weighted): 85.64, loss: 6.44e-01\n",
            "  time: 1980s (wall 1920s)\n",
            "step 71200 / 72000 (epoch 118.67 / 120):\n",
            "  learning_rate = 2.35e-06, loss_average = 5.46e-01\n",
            "  validation accuracy: 85.74 (8574 / 10000), f1 (weighted): 85.67, loss: 6.44e-01\n",
            "  time: 1985s (wall 1926s)\n",
            "step 71400 / 72000 (epoch 119.00 / 120):\n",
            "  learning_rate = 2.35e-06, loss_average = 5.70e-01\n",
            "  validation accuracy: 85.68 (8568 / 10000), f1 (weighted): 85.61, loss: 6.44e-01\n",
            "  time: 1991s (wall 1931s)\n",
            "step 71600 / 72000 (epoch 119.33 / 120):\n",
            "  learning_rate = 2.23e-06, loss_average = 5.21e-01\n",
            "  validation accuracy: 85.75 (8575 / 10000), f1 (weighted): 85.68, loss: 6.44e-01\n",
            "  time: 1997s (wall 1937s)\n",
            "step 71800 / 72000 (epoch 119.67 / 120):\n",
            "  learning_rate = 2.23e-06, loss_average = 5.22e-01\n",
            "  validation accuracy: 85.72 (8572 / 10000), f1 (weighted): 85.64, loss: 6.44e-01\n",
            "  time: 2002s (wall 1942s)\n",
            "step 72000 / 72000 (epoch 120.00 / 120):\n",
            "  learning_rate = 2.23e-06, loss_average = 5.36e-01\n",
            "  validation accuracy: 85.70 (8570 / 10000), f1 (weighted): 85.65, loss: 6.44e-01\n",
            "  time: 2008s (wall 1947s)\n",
            "validation accuracy: peak = 85.95, mean = 85.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPQp8ZdofXN5",
        "colab_type": "code",
        "outputId": "8c7cab6c-41f2-483a-fcb0-d1d4bd1b652e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "# Ploting code from https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy user Simone\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_accuracy[0::3])\n",
        "plt.plot(accuracy[0::3])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(train_loss[0::3])\n",
        "plt.plot(loss[0::3])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6/8Dzrq0x4lwdPkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voJUi1Itx_DH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rotated_test1 = np.reshape(np.rot90(np.reshape(X_test, (10000,28,28)),1, (1,2)), (10000,28*28))\n",
        "rotated_test2 = np.reshape(np.rot90(np.reshape(X_test, (10000,28,28)),2, (1,2)), (10000,28*28))\n",
        "rotated_test3 = np.reshape(np.rot90(np.reshape(X_test, (10000,28,28)),3, (1,2)), (10000,28*28))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ITWVPqw4Ni",
        "colab_type": "code",
        "outputId": "cb58a89f-8940-4802-9821-7f37cc4e7881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "res = model.evaluate(rotated_test1, test_labels)\n",
        "print(res[0])\n",
        "res = model.evaluate(rotated_test2, test_labels)\n",
        "print(res[0])\n",
        "res = model.evaluate(rotated_test3, test_labels)\n",
        "print(res[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/cnn_graph/lib/../checkpoints/demo/model-72000\n",
            "accuracy: 85.41 (8541 / 10000), f1 (weighted): 85.41, loss: 6.55e-01\n",
            "time: 1s (wall 1s)\n",
            "INFO:tensorflow:Restoring parameters from /content/cnn_graph/lib/../checkpoints/demo/model-72000\n",
            "accuracy: 85.38 (8538 / 10000), f1 (weighted): 85.39, loss: 6.59e-01\n",
            "time: 1s (wall 1s)\n",
            "INFO:tensorflow:Restoring parameters from /content/cnn_graph/lib/../checkpoints/demo/model-72000\n",
            "accuracy: 85.55 (8555 / 10000), f1 (weighted): 85.52, loss: 6.60e-01\n",
            "time: 1s (wall 1s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDw7LwF4w4No",
        "colab_type": "code",
        "outputId": "6482cfe7-2968-48a4-ba74-77840c11c558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "res = model.evaluate(X_test, test_labels)\n",
        "print(res[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/cnn_graph/lib/../checkpoints/demo/model-72000\n",
            "accuracy: 85.70 (8570 / 10000), f1 (weighted): 85.65, loss: 6.44e-01\n",
            "time: 1s (wall 1s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMxSBPXXr2Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"cnn_graph/checkpoints/demo/model-72000.meta\")\n",
        "files.download(\"cnn_graph/checkpoints/demo/model-72000.index\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgfLjoWMU8vP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp cnn_graph/checkpoints/demo/model-72000.data-00000-of-00001 ./model-72000.data-00000-of-00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_clZrqfVAQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}